{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textcnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb6znk-DDmnu",
        "colab_type": "code",
        "outputId": "e9299b23-d5ca-4f42-b0b1-3fcc2c5a665f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip install torch torchtext numpy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.38.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tscyOkDXESk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import data,datasets\n",
        "\n",
        "TEXT = data.Field(lower=True,batch_first=True)\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "# make splits for data\n",
        "train, val, test = datasets.SST.splits(TEXT, LABEL, 'data/',fine_grained=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNES61KHEinz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEXT.build_vocab(train, vectors=\"fasttext.en.300d\")\n",
        "TEXT.build_vocab(train, vectors=\"glove.6B.300d\")\n",
        "LABEL.build_vocab(train,val,test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsiDQBmDWj1N",
        "colab_type": "code",
        "outputId": "fccd8a56-2273-4df6-a18f-1677abe0421a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
        "print(LABEL.vocab.itos)\n",
        "print(LABEL.vocab.stoi)\n",
        "print('len(LABEL.vocab)', len(LABEL.vocab))   # vocab include '<unk>'\n",
        "print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(TEXT.vocab) 16581\n",
            "['negative', 'positive', 'neutral', 'very positive', 'very negative']\n",
            "defaultdict(<function _default_unk_index at 0x7f0e4bff70d0>, {'negative': 0, 'positive': 1, 'neutral': 2, 'very positive': 3, 'very negative': 4})\n",
            "len(LABEL.vocab) 5\n",
            "TEXT.vocab.vectors.size() torch.Size([16581, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0ZHHoI3HtIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "_DEBUG=False\n",
        "\n",
        "def ilog(*args,**kwargs):\n",
        "    if _DEBUG:\n",
        "        print(*args,**kwargs)\n",
        "    \n",
        "class textCNN(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super().__init__()\n",
        "        dim = args['dim']\n",
        "        n_class = args['n_class']\n",
        "        embedding_matrix=args['embedding_matrix']\n",
        "        kernels=[3,4,5]\n",
        "        kernel_number=[100,100,100]\n",
        "        self.embeding = nn.Embedding.from_pretrained(embedding_matrix)\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, number, (size, dim),padding=(size-1,0)) for (size,number) in zip(kernels,kernel_number)])\n",
        "        self.dropout=nn.Dropout()\n",
        "        self.out = nn.Linear(sum(kernel_number), n_class)\n",
        " \n",
        "    def forward(self, x):\n",
        "        ilog('ori input',x.size())\n",
        "        x = self.embeding(x)\n",
        "        ilog('after embeding',x.size())\n",
        "        x = x.unsqueeze(1)\n",
        "        ilog('unsqueeze',x.size())\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "        ilog(x[0].size())\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "    \n",
        "class textCNNMulti(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super().__init__()\n",
        "        dim = args['dim']\n",
        "        n_class = args['n_class']\n",
        "        embedding_matrix=args['embedding_matrix']\n",
        "        kernels=[3,4,5]\n",
        "        kernel_number=[100,100,100]\n",
        "        self.static_embed = nn.Embedding.from_pretrained(embedding_matrix)\n",
        "        self.non_static_embed = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(2, number, (size, dim),padding=(size-1,0)) for (size,number) in zip(kernels,kernel_number)])\n",
        "        self.dropout=nn.Dropout()\n",
        "        self.out = nn.Linear(sum(kernel_number), n_class)\n",
        " \n",
        "    def forward(self, x):\n",
        "        ilog('ori input',x.size())\n",
        "        non_static_input = self.non_static_embed(x)\n",
        "        static_input = self.static_embed(x)\n",
        "        x = torch.stack([non_static_input, static_input], dim=1)\n",
        "        ilog('after embeding',x.size())\n",
        "        ilog('unsqueeze',x.size())\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "        ilog(x[0].size())\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class textCNNNonStatic(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super().__init__()\n",
        "        dim = args['dim']\n",
        "        n_class = args['n_class']\n",
        "        embedding_matrix=args['embedding_matrix']\n",
        "        kernels=[3,4,5]\n",
        "        kernel_number=[100,100,100]\n",
        "        self.embeding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, number, (size, dim),padding=(size-1,0)) for (size,number) in zip(kernels,kernel_number)])\n",
        "        self.dropout=nn.Dropout()\n",
        "        self.out = nn.Linear(sum(kernel_number), n_class)\n",
        " \n",
        "    def forward(self, x):\n",
        "        ilog('ori input',x.size())\n",
        "        x = self.embeding(x)\n",
        "        ilog('after embeding',x.size())\n",
        "        x = x.unsqueeze(1)\n",
        "        ilog('unsqueeze',x.size())\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "        ilog(x[0].size())\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtMDFmnqqVFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train, val, test), batch_sizes=(32, 256, 256),shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwZ494LkqwVc",
        "colab_type": "code",
        "outputId": "afbd149f-61e0-4071-e645-83c86654d03a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "args={}\n",
        "args['vocb_size']=len(TEXT.vocab)\n",
        "args['dim']=300\n",
        "args['n_class']=len(LABEL.vocab)\n",
        "args['embedding_matrix']=TEXT.vocab.vectors\n",
        "args['lr']=1e-5\n",
        "args['epochs']=400\n",
        "args['log_interval']=20\n",
        "args['test_interval']=100\n",
        "args['save_dir']='./'\n",
        "\n",
        "print(args['vocb_size'])\n",
        "print(args['n_class'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16581\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19sOr41FrGWT",
        "colab_type": "code",
        "outputId": "26eb068e-a58b-4d64-a9d9-91b497348ce0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0e4c4c53b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4H0NFX3c1rm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a9ecae6-7467-4905-eb1f-0981b5ce1bf0"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR-sHoABrMg3",
        "colab_type": "code",
        "outputId": "0435b6f9-3ebf-4ce8-ecb7-006a3192fe64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "def save(model, save_dir, save_prefix, steps):\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    save_prefix = os.path.join(save_dir, save_prefix)\n",
        "    save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "\n",
        "\n",
        "\n",
        "model=textCNNMulti(args)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "best_acc = 0\n",
        "last_step = 0\n",
        "model.train()\n",
        "steps=0\n",
        "\n",
        "\n",
        "def create_early_stopping(patience):\n",
        "    recent_metric = deque(maxlen=patience)\n",
        "    best_metric = None\n",
        "\n",
        "    def check(metric, model):\n",
        "        nonlocal best_metric\n",
        "        is_stop = False\n",
        "        if not best_metric or metric > best_metric:\n",
        "            print('save best_model.pt, metric: {}'.format(metric))\n",
        "            best_metric = metric\n",
        "            torch.save(model, 'best_model.pt')\n",
        "\n",
        "        recent_metric.append(metric)\n",
        "\n",
        "        if all([i < best_metric for i in recent_metric]):\n",
        "            is_stop = True\n",
        "        return is_stop\n",
        "\n",
        "    return check\n",
        "\n",
        "\n",
        "def eval(data_iter, model, args):\n",
        "    model.eval()\n",
        "    corrects, avg_loss = 0, 0\n",
        "    for i,data in enumerate(data_iter):\n",
        "        x, target = data.text, data.label\n",
        "        x=x.to(device)\n",
        "\n",
        "        target=target.to(device)\n",
        "\n",
        "        logit = model(x)\n",
        "        loss = F.cross_entropy(logit, target, reduction='sum')\n",
        "\n",
        "        avg_loss += loss.item()\n",
        "        corrects += (torch.max(logit, 1)\n",
        "                     [1].view(target.size()).data == target.data).sum()\n",
        "\n",
        "    size = len(data_iter.dataset)\n",
        "    avg_loss /= size\n",
        "    accuracy = 100.0 * int(corrects)/size\n",
        "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, \n",
        "                                                                       accuracy, \n",
        "                                                                       corrects, \n",
        "                                                                       size))\n",
        "    model.train()\n",
        "    return accuracy\n",
        "\n",
        "early_stop = create_early_stopping(150)\n",
        "\n",
        "for epoch in range(1, args['epochs']+1):\n",
        "    for i,data in enumerate(train_iter):\n",
        "        steps+=1\n",
        "\n",
        "        x, target = data.text, data.label\n",
        "        x=x.to(device)\n",
        "        target=target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if steps % args['log_interval'] == 0:\n",
        "            corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            accuracy = 100.0 * int(corrects)/data.batch_size\n",
        "            print(\n",
        "                'Epoch [{}] Batch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(epoch,\n",
        "                                                                         steps, \n",
        "                                                                         loss.item(), \n",
        "                                                                         accuracy,\n",
        "                                                                         corrects,\n",
        "                                                                         data.batch_size))\n",
        "        if steps % args['test_interval'] == 0:\n",
        "            val_acc = eval(val_iter, model, args)\n",
        "            is_stop = early_stop(val_acc, model)\n",
        "            if is_stop:\n",
        "                raise RuntimeError('early stop')\n",
        "\n",
        "        model.train()\n",
        "print('final_result')\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1] Batch[20] - loss: 1.548777  acc: 28.1250%(9/32)\n",
            "Epoch [1] Batch[40] - loss: 1.583792  acc: 21.8750%(7/32)\n",
            "Epoch [1] Batch[60] - loss: 1.607926  acc: 28.1250%(9/32)\n",
            "Epoch [1] Batch[80] - loss: 1.596044  acc: 34.3750%(11/32)\n",
            "Epoch [1] Batch[100] - loss: 1.547954  acc: 34.3750%(11/32)\n",
            "\n",
            "Evaluation - loss: 1.572960  acc: 26.3397%(290/1101) \n",
            "\n",
            "save best_model.pt, metric: 26.33969118982743\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type textCNNMulti. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1] Batch[120] - loss: 1.546566  acc: 25.0000%(8/32)\n",
            "Epoch [1] Batch[140] - loss: 1.559595  acc: 25.0000%(8/32)\n",
            "Epoch [1] Batch[160] - loss: 1.658371  acc: 15.6250%(5/32)\n",
            "Epoch [1] Batch[180] - loss: 1.504653  acc: 34.3750%(11/32)\n",
            "Epoch [1] Batch[200] - loss: 1.631015  acc: 28.1250%(9/32)\n",
            "\n",
            "Evaluation - loss: 1.564610  acc: 27.1571%(299/1101) \n",
            "\n",
            "save best_model.pt, metric: 27.157129881925524\n",
            "Epoch [1] Batch[220] - loss: 1.515379  acc: 34.3750%(11/32)\n",
            "Epoch [1] Batch[240] - loss: 1.652367  acc: 25.0000%(8/32)\n",
            "Epoch [1] Batch[260] - loss: 1.584523  acc: 21.8750%(7/32)\n",
            "Epoch [2] Batch[280] - loss: 1.604073  acc: 25.0000%(8/32)\n",
            "Epoch [2] Batch[300] - loss: 1.540003  acc: 34.3750%(11/32)\n",
            "\n",
            "Evaluation - loss: 1.560351  acc: 28.9737%(319/1101) \n",
            "\n",
            "save best_model.pt, metric: 28.97366030881017\n",
            "Epoch [2] Batch[320] - loss: 1.518684  acc: 34.3750%(11/32)\n",
            "Epoch [2] Batch[340] - loss: 1.571397  acc: 34.3750%(11/32)\n",
            "Epoch [2] Batch[360] - loss: 1.610183  acc: 18.7500%(6/32)\n",
            "Epoch [2] Batch[380] - loss: 1.472001  acc: 37.5000%(12/32)\n",
            "Epoch [2] Batch[400] - loss: 1.547848  acc: 28.1250%(9/32)\n",
            "\n",
            "Evaluation - loss: 1.555340  acc: 29.1553%(321/1101) \n",
            "\n",
            "save best_model.pt, metric: 29.155313351498638\n",
            "Epoch [2] Batch[420] - loss: 1.599001  acc: 21.8750%(7/32)\n",
            "Epoch [2] Batch[440] - loss: 1.541501  acc: 25.0000%(8/32)\n",
            "Epoch [2] Batch[460] - loss: 1.504786  acc: 43.7500%(14/32)\n",
            "Epoch [2] Batch[480] - loss: 1.708901  acc: 15.6250%(5/32)\n",
            "Epoch [2] Batch[500] - loss: 1.434611  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.551491  acc: 28.7920%(317/1101) \n",
            "\n",
            "Epoch [2] Batch[520] - loss: 1.669953  acc: 18.7500%(6/32)\n",
            "Epoch [3] Batch[540] - loss: 1.518669  acc: 28.1250%(9/32)\n",
            "Epoch [3] Batch[560] - loss: 1.603179  acc: 21.8750%(7/32)\n",
            "Epoch [3] Batch[580] - loss: 1.525541  acc: 37.5000%(12/32)\n",
            "Epoch [3] Batch[600] - loss: 1.502586  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.547394  acc: 30.4269%(335/1101) \n",
            "\n",
            "save best_model.pt, metric: 30.42688465031789\n",
            "Epoch [3] Batch[620] - loss: 1.566071  acc: 28.1250%(9/32)\n",
            "Epoch [3] Batch[640] - loss: 1.536434  acc: 25.0000%(8/32)\n",
            "Epoch [3] Batch[660] - loss: 1.600375  acc: 18.7500%(6/32)\n",
            "Epoch [3] Batch[680] - loss: 1.608164  acc: 21.8750%(7/32)\n",
            "Epoch [3] Batch[700] - loss: 1.471105  acc: 37.5000%(12/32)\n",
            "\n",
            "Evaluation - loss: 1.542427  acc: 31.9709%(352/1101) \n",
            "\n",
            "save best_model.pt, metric: 31.970935513169845\n",
            "Epoch [3] Batch[720] - loss: 1.476423  acc: 37.5000%(12/32)\n",
            "Epoch [3] Batch[740] - loss: 1.618059  acc: 28.1250%(9/32)\n",
            "Epoch [3] Batch[760] - loss: 1.608983  acc: 18.7500%(6/32)\n",
            "Epoch [3] Batch[780] - loss: 1.560923  acc: 34.3750%(11/32)\n",
            "Epoch [3] Batch[800] - loss: 1.560745  acc: 28.1250%(9/32)\n",
            "\n",
            "Evaluation - loss: 1.539187  acc: 31.2443%(344/1101) \n",
            "\n",
            "Epoch [4] Batch[820] - loss: 1.568168  acc: 25.0000%(8/32)\n",
            "Epoch [4] Batch[840] - loss: 1.548557  acc: 40.6250%(13/32)\n",
            "Epoch [4] Batch[860] - loss: 1.505231  acc: 37.5000%(12/32)\n",
            "Epoch [4] Batch[880] - loss: 1.519097  acc: 40.6250%(13/32)\n",
            "Epoch [4] Batch[900] - loss: 1.438726  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.534823  acc: 31.5168%(347/1101) \n",
            "\n",
            "Epoch [4] Batch[920] - loss: 1.489580  acc: 21.8750%(7/32)\n",
            "Epoch [4] Batch[940] - loss: 1.512770  acc: 34.3750%(11/32)\n",
            "Epoch [4] Batch[960] - loss: 1.522384  acc: 34.3750%(11/32)\n",
            "Epoch [4] Batch[980] - loss: 1.455446  acc: 43.7500%(14/32)\n",
            "Epoch [4] Batch[1000] - loss: 1.514686  acc: 34.3750%(11/32)\n",
            "\n",
            "Evaluation - loss: 1.530635  acc: 33.0609%(364/1101) \n",
            "\n",
            "save best_model.pt, metric: 33.060853769300635\n",
            "Epoch [4] Batch[1020] - loss: 1.443801  acc: 53.1250%(17/32)\n",
            "Epoch [4] Batch[1040] - loss: 1.469011  acc: 28.1250%(9/32)\n",
            "Epoch [4] Batch[1060] - loss: 1.449806  acc: 43.7500%(14/32)\n",
            "Epoch [5] Batch[1080] - loss: 1.484592  acc: 46.8750%(15/32)\n",
            "Epoch [5] Batch[1100] - loss: 1.499310  acc: 37.5000%(12/32)\n",
            "\n",
            "Evaluation - loss: 1.527853  acc: 32.2434%(355/1101) \n",
            "\n",
            "Epoch [5] Batch[1120] - loss: 1.596875  acc: 18.7500%(6/32)\n",
            "Epoch [5] Batch[1140] - loss: 1.461407  acc: 37.5000%(12/32)\n",
            "Epoch [5] Batch[1160] - loss: 1.496619  acc: 34.3750%(11/32)\n",
            "Epoch [5] Batch[1180] - loss: 1.512382  acc: 46.8750%(15/32)\n",
            "Epoch [5] Batch[1200] - loss: 1.467190  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.523180  acc: 33.6966%(371/1101) \n",
            "\n",
            "save best_model.pt, metric: 33.69663941871026\n",
            "Epoch [5] Batch[1220] - loss: 1.462295  acc: 43.7500%(14/32)\n",
            "Epoch [5] Batch[1240] - loss: 1.431033  acc: 46.8750%(15/32)\n",
            "Epoch [5] Batch[1260] - loss: 1.480816  acc: 31.2500%(10/32)\n",
            "Epoch [5] Batch[1280] - loss: 1.538534  acc: 50.0000%(16/32)\n",
            "Epoch [5] Batch[1300] - loss: 1.506956  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.518770  acc: 34.3324%(378/1101) \n",
            "\n",
            "save best_model.pt, metric: 34.33242506811989\n",
            "Epoch [5] Batch[1320] - loss: 1.480380  acc: 37.5000%(12/32)\n",
            "Epoch [6] Batch[1340] - loss: 1.402573  acc: 43.7500%(14/32)\n",
            "Epoch [6] Batch[1360] - loss: 1.462966  acc: 40.6250%(13/32)\n",
            "Epoch [6] Batch[1380] - loss: 1.534705  acc: 25.0000%(8/32)\n",
            "Epoch [6] Batch[1400] - loss: 1.417631  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.515253  acc: 34.4233%(379/1101) \n",
            "\n",
            "save best_model.pt, metric: 34.42325158946412\n",
            "Epoch [6] Batch[1420] - loss: 1.542216  acc: 34.3750%(11/32)\n",
            "Epoch [6] Batch[1440] - loss: 1.561013  acc: 25.0000%(8/32)\n",
            "Epoch [6] Batch[1460] - loss: 1.408588  acc: 46.8750%(15/32)\n",
            "Epoch [6] Batch[1480] - loss: 1.413699  acc: 43.7500%(14/32)\n",
            "Epoch [6] Batch[1500] - loss: 1.435308  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.511198  acc: 34.2416%(377/1101) \n",
            "\n",
            "Epoch [6] Batch[1520] - loss: 1.483005  acc: 31.2500%(10/32)\n",
            "Epoch [6] Batch[1540] - loss: 1.445162  acc: 43.7500%(14/32)\n",
            "Epoch [6] Batch[1560] - loss: 1.485083  acc: 25.0000%(8/32)\n",
            "Epoch [6] Batch[1580] - loss: 1.349487  acc: 50.0000%(16/32)\n",
            "Epoch [6] Batch[1600] - loss: 1.523381  acc: 28.1250%(9/32)\n",
            "\n",
            "Evaluation - loss: 1.506236  acc: 35.2407%(388/1101) \n",
            "\n",
            "save best_model.pt, metric: 35.240690281562216\n",
            "Epoch [7] Batch[1620] - loss: 1.382743  acc: 59.3750%(19/32)\n",
            "Epoch [7] Batch[1640] - loss: 1.437860  acc: 43.7500%(14/32)\n",
            "Epoch [7] Batch[1660] - loss: 1.500551  acc: 31.2500%(10/32)\n",
            "Epoch [7] Batch[1680] - loss: 1.474713  acc: 43.7500%(14/32)\n",
            "Epoch [7] Batch[1700] - loss: 1.493485  acc: 31.2500%(10/32)\n",
            "\n",
            "Evaluation - loss: 1.502183  acc: 34.8774%(384/1101) \n",
            "\n",
            "Epoch [7] Batch[1720] - loss: 1.438006  acc: 37.5000%(12/32)\n",
            "Epoch [7] Batch[1740] - loss: 1.461731  acc: 43.7500%(14/32)\n",
            "Epoch [7] Batch[1760] - loss: 1.492705  acc: 40.6250%(13/32)\n",
            "Epoch [7] Batch[1780] - loss: 1.425893  acc: 46.8750%(15/32)\n",
            "Epoch [7] Batch[1800] - loss: 1.439595  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.499397  acc: 34.6049%(381/1101) \n",
            "\n",
            "Epoch [7] Batch[1820] - loss: 1.514219  acc: 34.3750%(11/32)\n",
            "Epoch [7] Batch[1840] - loss: 1.470924  acc: 34.3750%(11/32)\n",
            "Epoch [7] Batch[1860] - loss: 1.503715  acc: 40.6250%(13/32)\n",
            "Epoch [8] Batch[1880] - loss: 1.460840  acc: 40.6250%(13/32)\n",
            "Epoch [8] Batch[1900] - loss: 1.467584  acc: 25.0000%(8/32)\n",
            "\n",
            "Evaluation - loss: 1.495472  acc: 34.9682%(385/1101) \n",
            "\n",
            "Epoch [8] Batch[1920] - loss: 1.446282  acc: 43.7500%(14/32)\n",
            "Epoch [8] Batch[1940] - loss: 1.464359  acc: 31.2500%(10/32)\n",
            "Epoch [8] Batch[1960] - loss: 1.455261  acc: 40.6250%(13/32)\n",
            "Epoch [8] Batch[1980] - loss: 1.572383  acc: 34.3750%(11/32)\n",
            "Epoch [8] Batch[2000] - loss: 1.470390  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.491761  acc: 34.8774%(384/1101) \n",
            "\n",
            "Epoch [8] Batch[2020] - loss: 1.385717  acc: 37.5000%(12/32)\n",
            "Epoch [8] Batch[2040] - loss: 1.439502  acc: 46.8750%(15/32)\n",
            "Epoch [8] Batch[2060] - loss: 1.322735  acc: 50.0000%(16/32)\n",
            "Epoch [8] Batch[2080] - loss: 1.419385  acc: 37.5000%(12/32)\n",
            "Epoch [8] Batch[2100] - loss: 1.449610  acc: 31.2500%(10/32)\n",
            "\n",
            "Evaluation - loss: 1.487010  acc: 35.2407%(388/1101) \n",
            "\n",
            "Epoch [8] Batch[2120] - loss: 1.383548  acc: 46.8750%(15/32)\n",
            "Epoch [9] Batch[2140] - loss: 1.455753  acc: 34.3750%(11/32)\n",
            "Epoch [9] Batch[2160] - loss: 1.359998  acc: 40.6250%(13/32)\n",
            "Epoch [9] Batch[2180] - loss: 1.391536  acc: 43.7500%(14/32)\n",
            "Epoch [9] Batch[2200] - loss: 1.458457  acc: 34.3750%(11/32)\n",
            "\n",
            "Evaluation - loss: 1.482643  acc: 36.5123%(402/1101) \n",
            "\n",
            "save best_model.pt, metric: 36.51226158038147\n",
            "Epoch [9] Batch[2220] - loss: 1.427240  acc: 50.0000%(16/32)\n",
            "Epoch [9] Batch[2240] - loss: 1.372951  acc: 56.2500%(18/32)\n",
            "Epoch [9] Batch[2260] - loss: 1.382416  acc: 40.6250%(13/32)\n",
            "Epoch [9] Batch[2280] - loss: 1.436081  acc: 31.2500%(10/32)\n",
            "Epoch [9] Batch[2300] - loss: 1.464283  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.479867  acc: 35.6040%(392/1101) \n",
            "\n",
            "Epoch [9] Batch[2320] - loss: 1.264503  acc: 56.2500%(18/32)\n",
            "Epoch [9] Batch[2340] - loss: 1.426115  acc: 37.5000%(12/32)\n",
            "Epoch [9] Batch[2360] - loss: 1.338650  acc: 53.1250%(17/32)\n",
            "Epoch [9] Batch[2380] - loss: 1.499772  acc: 43.7500%(14/32)\n",
            "Epoch [9] Batch[2400] - loss: 1.539448  acc: 31.2500%(10/32)\n",
            "\n",
            "Evaluation - loss: 1.475996  acc: 35.8765%(395/1101) \n",
            "\n",
            "Epoch [10] Batch[2420] - loss: 1.438867  acc: 43.7500%(14/32)\n",
            "Epoch [10] Batch[2440] - loss: 1.412574  acc: 43.7500%(14/32)\n",
            "Epoch [10] Batch[2460] - loss: 1.340739  acc: 46.8750%(15/32)\n",
            "Epoch [10] Batch[2480] - loss: 1.465383  acc: 31.2500%(10/32)\n",
            "Epoch [10] Batch[2500] - loss: 1.409837  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.472392  acc: 35.9673%(396/1101) \n",
            "\n",
            "Epoch [10] Batch[2520] - loss: 1.481634  acc: 31.2500%(10/32)\n",
            "Epoch [10] Batch[2540] - loss: 1.496484  acc: 37.5000%(12/32)\n",
            "Epoch [10] Batch[2560] - loss: 1.347041  acc: 59.3750%(19/32)\n",
            "Epoch [10] Batch[2580] - loss: 1.481034  acc: 37.5000%(12/32)\n",
            "Epoch [10] Batch[2600] - loss: 1.432974  acc: 37.5000%(12/32)\n",
            "\n",
            "Evaluation - loss: 1.467370  acc: 36.9664%(407/1101) \n",
            "\n",
            "save best_model.pt, metric: 36.96639418710264\n",
            "Epoch [10] Batch[2620] - loss: 1.413279  acc: 37.5000%(12/32)\n",
            "Epoch [10] Batch[2640] - loss: 1.432064  acc: 40.6250%(13/32)\n",
            "Epoch [10] Batch[2660] - loss: 1.508355  acc: 28.1250%(9/32)\n",
            "Epoch [11] Batch[2680] - loss: 1.478732  acc: 46.8750%(15/32)\n",
            "Epoch [11] Batch[2700] - loss: 1.348266  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.463393  acc: 36.5123%(402/1101) \n",
            "\n",
            "Epoch [11] Batch[2720] - loss: 1.417826  acc: 43.7500%(14/32)\n",
            "Epoch [11] Batch[2740] - loss: 1.364048  acc: 50.0000%(16/32)\n",
            "Epoch [11] Batch[2760] - loss: 1.447087  acc: 34.3750%(11/32)\n",
            "Epoch [11] Batch[2780] - loss: 1.448192  acc: 31.2500%(10/32)\n",
            "Epoch [11] Batch[2800] - loss: 1.484238  acc: 34.3750%(11/32)\n",
            "\n",
            "Evaluation - loss: 1.461137  acc: 36.1490%(398/1101) \n",
            "\n",
            "Epoch [11] Batch[2820] - loss: 1.355288  acc: 53.1250%(17/32)\n",
            "Epoch [11] Batch[2840] - loss: 1.533663  acc: 37.5000%(12/32)\n",
            "Epoch [11] Batch[2860] - loss: 1.406135  acc: 37.5000%(12/32)\n",
            "Epoch [11] Batch[2880] - loss: 1.454829  acc: 31.2500%(10/32)\n",
            "Epoch [11] Batch[2900] - loss: 1.326995  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.456284  acc: 37.0572%(408/1101) \n",
            "\n",
            "save best_model.pt, metric: 37.05722070844686\n",
            "Epoch [11] Batch[2920] - loss: 1.405081  acc: 53.1250%(17/32)\n",
            "Epoch [12] Batch[2940] - loss: 1.392806  acc: 46.8750%(15/32)\n",
            "Epoch [12] Batch[2960] - loss: 1.353933  acc: 56.2500%(18/32)\n",
            "Epoch [12] Batch[2980] - loss: 1.330288  acc: 50.0000%(16/32)\n",
            "Epoch [12] Batch[3000] - loss: 1.388690  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.453370  acc: 36.5123%(402/1101) \n",
            "\n",
            "Epoch [12] Batch[3020] - loss: 1.375666  acc: 46.8750%(15/32)\n",
            "Epoch [12] Batch[3040] - loss: 1.338680  acc: 50.0000%(16/32)\n",
            "Epoch [12] Batch[3060] - loss: 1.399861  acc: 37.5000%(12/32)\n",
            "Epoch [12] Batch[3080] - loss: 1.406953  acc: 40.6250%(13/32)\n",
            "Epoch [12] Batch[3100] - loss: 1.428410  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.448676  acc: 37.5114%(413/1101) \n",
            "\n",
            "save best_model.pt, metric: 37.51135331516803\n",
            "Epoch [12] Batch[3120] - loss: 1.302086  acc: 50.0000%(16/32)\n",
            "Epoch [12] Batch[3140] - loss: 1.370167  acc: 56.2500%(18/32)\n",
            "Epoch [12] Batch[3160] - loss: 1.289363  acc: 50.0000%(16/32)\n",
            "Epoch [12] Batch[3180] - loss: 1.369651  acc: 40.6250%(13/32)\n",
            "Epoch [12] Batch[3200] - loss: 1.552161  acc: 18.7500%(6/32)\n",
            "\n",
            "Evaluation - loss: 1.446104  acc: 36.9664%(407/1101) \n",
            "\n",
            "Epoch [13] Batch[3220] - loss: 1.392567  acc: 53.1250%(17/32)\n",
            "Epoch [13] Batch[3240] - loss: 1.396215  acc: 43.7500%(14/32)\n",
            "Epoch [13] Batch[3260] - loss: 1.343378  acc: 46.8750%(15/32)\n",
            "Epoch [13] Batch[3280] - loss: 1.472433  acc: 34.3750%(11/32)\n",
            "Epoch [13] Batch[3300] - loss: 1.404016  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.442245  acc: 37.5114%(413/1101) \n",
            "\n",
            "Epoch [13] Batch[3320] - loss: 1.358178  acc: 53.1250%(17/32)\n",
            "Epoch [13] Batch[3340] - loss: 1.412483  acc: 40.6250%(13/32)\n",
            "Epoch [13] Batch[3360] - loss: 1.459795  acc: 40.6250%(13/32)\n",
            "Epoch [13] Batch[3380] - loss: 1.438454  acc: 43.7500%(14/32)\n",
            "Epoch [13] Batch[3400] - loss: 1.428612  acc: 37.5000%(12/32)\n",
            "\n",
            "Evaluation - loss: 1.439263  acc: 37.1480%(409/1101) \n",
            "\n",
            "Epoch [13] Batch[3420] - loss: 1.394554  acc: 43.7500%(14/32)\n",
            "Epoch [13] Batch[3440] - loss: 1.390349  acc: 37.5000%(12/32)\n",
            "Epoch [13] Batch[3460] - loss: 1.382445  acc: 46.8750%(15/32)\n",
            "Epoch [14] Batch[3480] - loss: 1.325731  acc: 37.5000%(12/32)\n",
            "Epoch [14] Batch[3500] - loss: 1.359021  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.435343  acc: 37.6022%(414/1101) \n",
            "\n",
            "save best_model.pt, metric: 37.60217983651226\n",
            "Epoch [14] Batch[3520] - loss: 1.423757  acc: 43.7500%(14/32)\n",
            "Epoch [14] Batch[3540] - loss: 1.302052  acc: 62.5000%(20/32)\n",
            "Epoch [14] Batch[3560] - loss: 1.467689  acc: 31.2500%(10/32)\n",
            "Epoch [14] Batch[3580] - loss: 1.498204  acc: 18.7500%(6/32)\n",
            "Epoch [14] Batch[3600] - loss: 1.304442  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.431926  acc: 37.2389%(410/1101) \n",
            "\n",
            "Epoch [14] Batch[3620] - loss: 1.213477  acc: 59.3750%(19/32)\n",
            "Epoch [14] Batch[3640] - loss: 1.319269  acc: 46.8750%(15/32)\n",
            "Epoch [14] Batch[3660] - loss: 1.453928  acc: 34.3750%(11/32)\n",
            "Epoch [14] Batch[3680] - loss: 1.428998  acc: 31.2500%(10/32)\n",
            "Epoch [14] Batch[3700] - loss: 1.514434  acc: 34.3750%(11/32)\n",
            "\n",
            "Evaluation - loss: 1.428277  acc: 37.8747%(417/1101) \n",
            "\n",
            "save best_model.pt, metric: 37.87465940054496\n",
            "Epoch [14] Batch[3720] - loss: 1.403549  acc: 46.8750%(15/32)\n",
            "Epoch [15] Batch[3740] - loss: 1.346022  acc: 50.0000%(16/32)\n",
            "Epoch [15] Batch[3760] - loss: 1.364717  acc: 40.6250%(13/32)\n",
            "Epoch [15] Batch[3780] - loss: 1.387172  acc: 50.0000%(16/32)\n",
            "Epoch [15] Batch[3800] - loss: 1.322512  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.425908  acc: 37.2389%(410/1101) \n",
            "\n",
            "Epoch [15] Batch[3820] - loss: 1.372564  acc: 46.8750%(15/32)\n",
            "Epoch [15] Batch[3840] - loss: 1.361969  acc: 53.1250%(17/32)\n",
            "Epoch [15] Batch[3860] - loss: 1.308073  acc: 56.2500%(18/32)\n",
            "Epoch [15] Batch[3880] - loss: 1.396682  acc: 37.5000%(12/32)\n",
            "Epoch [15] Batch[3900] - loss: 1.334839  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.421808  acc: 39.0554%(430/1101) \n",
            "\n",
            "save best_model.pt, metric: 39.055404178019984\n",
            "Epoch [15] Batch[3920] - loss: 1.306909  acc: 53.1250%(17/32)\n",
            "Epoch [15] Batch[3940] - loss: 1.340878  acc: 46.8750%(15/32)\n",
            "Epoch [15] Batch[3960] - loss: 1.290919  acc: 53.1250%(17/32)\n",
            "Epoch [15] Batch[3980] - loss: 1.490406  acc: 25.0000%(8/32)\n",
            "Epoch [15] Batch[4000] - loss: 1.335111  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.418633  acc: 37.9655%(418/1101) \n",
            "\n",
            "Epoch [16] Batch[4020] - loss: 1.371289  acc: 46.8750%(15/32)\n",
            "Epoch [16] Batch[4040] - loss: 1.557242  acc: 34.3750%(11/32)\n",
            "Epoch [16] Batch[4060] - loss: 1.403209  acc: 43.7500%(14/32)\n",
            "Epoch [16] Batch[4080] - loss: 1.406644  acc: 34.3750%(11/32)\n",
            "Epoch [16] Batch[4100] - loss: 1.352606  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.415308  acc: 38.2380%(421/1101) \n",
            "\n",
            "Epoch [16] Batch[4120] - loss: 1.469558  acc: 31.2500%(10/32)\n",
            "Epoch [16] Batch[4140] - loss: 1.454743  acc: 37.5000%(12/32)\n",
            "Epoch [16] Batch[4160] - loss: 1.290411  acc: 53.1250%(17/32)\n",
            "Epoch [16] Batch[4180] - loss: 1.380934  acc: 46.8750%(15/32)\n",
            "Epoch [16] Batch[4200] - loss: 1.333012  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.413082  acc: 37.8747%(417/1101) \n",
            "\n",
            "Epoch [16] Batch[4220] - loss: 1.319631  acc: 40.6250%(13/32)\n",
            "Epoch [16] Batch[4240] - loss: 1.396907  acc: 40.6250%(13/32)\n",
            "Epoch [16] Batch[4260] - loss: 1.417874  acc: 40.6250%(13/32)\n",
            "Epoch [17] Batch[4280] - loss: 1.316497  acc: 56.2500%(18/32)\n",
            "Epoch [17] Batch[4300] - loss: 1.426586  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.409474  acc: 38.2380%(421/1101) \n",
            "\n",
            "Epoch [17] Batch[4320] - loss: 1.212995  acc: 62.5000%(20/32)\n",
            "Epoch [17] Batch[4340] - loss: 1.300110  acc: 43.7500%(14/32)\n",
            "Epoch [17] Batch[4360] - loss: 1.429930  acc: 40.6250%(13/32)\n",
            "Epoch [17] Batch[4380] - loss: 1.347677  acc: 46.8750%(15/32)\n",
            "Epoch [17] Batch[4400] - loss: 1.264259  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.405460  acc: 39.8728%(439/1101) \n",
            "\n",
            "save best_model.pt, metric: 39.87284287011808\n",
            "Epoch [17] Batch[4420] - loss: 1.390144  acc: 34.3750%(11/32)\n",
            "Epoch [17] Batch[4440] - loss: 1.314540  acc: 53.1250%(17/32)\n",
            "Epoch [17] Batch[4460] - loss: 1.313482  acc: 56.2500%(18/32)\n",
            "Epoch [17] Batch[4480] - loss: 1.375265  acc: 50.0000%(16/32)\n",
            "Epoch [17] Batch[4500] - loss: 1.210447  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.402864  acc: 39.8728%(439/1101) \n",
            "\n",
            "Epoch [17] Batch[4520] - loss: 1.325612  acc: 50.0000%(16/32)\n",
            "Epoch [18] Batch[4540] - loss: 1.311256  acc: 46.8750%(15/32)\n",
            "Epoch [18] Batch[4560] - loss: 1.293639  acc: 50.0000%(16/32)\n",
            "Epoch [18] Batch[4580] - loss: 1.322319  acc: 46.8750%(15/32)\n",
            "Epoch [18] Batch[4600] - loss: 1.442102  acc: 34.3750%(11/32)\n",
            "\n",
            "Evaluation - loss: 1.400913  acc: 38.2380%(421/1101) \n",
            "\n",
            "Epoch [18] Batch[4620] - loss: 1.328715  acc: 43.7500%(14/32)\n",
            "Epoch [18] Batch[4640] - loss: 1.192224  acc: 71.8750%(23/32)\n",
            "Epoch [18] Batch[4660] - loss: 1.242651  acc: 53.1250%(17/32)\n",
            "Epoch [18] Batch[4680] - loss: 1.285217  acc: 50.0000%(16/32)\n",
            "Epoch [18] Batch[4700] - loss: 1.276263  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.397427  acc: 39.4187%(434/1101) \n",
            "\n",
            "Epoch [18] Batch[4720] - loss: 1.259197  acc: 59.3750%(19/32)\n",
            "Epoch [18] Batch[4740] - loss: 1.354136  acc: 50.0000%(16/32)\n",
            "Epoch [18] Batch[4760] - loss: 1.385299  acc: 40.6250%(13/32)\n",
            "Epoch [18] Batch[4780] - loss: 1.284632  acc: 46.8750%(15/32)\n",
            "Epoch [18] Batch[4800] - loss: 1.327548  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.396248  acc: 38.9646%(429/1101) \n",
            "\n",
            "Epoch [19] Batch[4820] - loss: 1.319178  acc: 50.0000%(16/32)\n",
            "Epoch [19] Batch[4840] - loss: 1.400715  acc: 37.5000%(12/32)\n",
            "Epoch [19] Batch[4860] - loss: 1.354604  acc: 46.8750%(15/32)\n",
            "Epoch [19] Batch[4880] - loss: 1.262148  acc: 50.0000%(16/32)\n",
            "Epoch [19] Batch[4900] - loss: 1.406050  acc: 34.3750%(11/32)\n",
            "\n",
            "Evaluation - loss: 1.392552  acc: 39.0554%(430/1101) \n",
            "\n",
            "Epoch [19] Batch[4920] - loss: 1.437463  acc: 31.2500%(10/32)\n",
            "Epoch [19] Batch[4940] - loss: 1.345584  acc: 40.6250%(13/32)\n",
            "Epoch [19] Batch[4960] - loss: 1.309848  acc: 59.3750%(19/32)\n",
            "Epoch [19] Batch[4980] - loss: 1.279464  acc: 56.2500%(18/32)\n",
            "Epoch [19] Batch[5000] - loss: 1.357347  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.389544  acc: 39.6004%(436/1101) \n",
            "\n",
            "Epoch [19] Batch[5020] - loss: 1.374942  acc: 46.8750%(15/32)\n",
            "Epoch [19] Batch[5040] - loss: 1.393571  acc: 50.0000%(16/32)\n",
            "Epoch [19] Batch[5060] - loss: 1.166560  acc: 56.2500%(18/32)\n",
            "Epoch [20] Batch[5080] - loss: 1.399601  acc: 37.5000%(12/32)\n",
            "Epoch [20] Batch[5100] - loss: 1.443605  acc: 31.2500%(10/32)\n",
            "\n",
            "Evaluation - loss: 1.389974  acc: 38.3288%(422/1101) \n",
            "\n",
            "Epoch [20] Batch[5120] - loss: 1.332959  acc: 46.8750%(15/32)\n",
            "Epoch [20] Batch[5140] - loss: 1.366917  acc: 37.5000%(12/32)\n",
            "Epoch [20] Batch[5160] - loss: 1.221737  acc: 59.3750%(19/32)\n",
            "Epoch [20] Batch[5180] - loss: 1.306444  acc: 50.0000%(16/32)\n",
            "Epoch [20] Batch[5200] - loss: 1.302721  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.384125  acc: 39.6912%(437/1101) \n",
            "\n",
            "Epoch [20] Batch[5220] - loss: 1.326819  acc: 50.0000%(16/32)\n",
            "Epoch [20] Batch[5240] - loss: 1.381138  acc: 46.8750%(15/32)\n",
            "Epoch [20] Batch[5260] - loss: 1.361690  acc: 34.3750%(11/32)\n",
            "Epoch [20] Batch[5280] - loss: 1.249926  acc: 62.5000%(20/32)\n",
            "Epoch [20] Batch[5300] - loss: 1.164247  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.383136  acc: 39.6004%(436/1101) \n",
            "\n",
            "Epoch [20] Batch[5320] - loss: 1.248258  acc: 56.2500%(18/32)\n",
            "Epoch [20] Batch[5340] - loss: 1.423639  acc: 37.5000%(12/32)\n",
            "Epoch [21] Batch[5360] - loss: 1.345028  acc: 37.5000%(12/32)\n",
            "Epoch [21] Batch[5380] - loss: 1.443682  acc: 50.0000%(16/32)\n",
            "Epoch [21] Batch[5400] - loss: 1.421079  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.381332  acc: 39.2371%(432/1101) \n",
            "\n",
            "Epoch [21] Batch[5420] - loss: 1.161758  acc: 59.3750%(19/32)\n",
            "Epoch [21] Batch[5440] - loss: 1.207687  acc: 43.7500%(14/32)\n",
            "Epoch [21] Batch[5460] - loss: 1.362859  acc: 37.5000%(12/32)\n",
            "Epoch [21] Batch[5480] - loss: 1.387796  acc: 31.2500%(10/32)\n",
            "Epoch [21] Batch[5500] - loss: 1.336478  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.378411  acc: 39.6004%(436/1101) \n",
            "\n",
            "Epoch [21] Batch[5520] - loss: 1.184440  acc: 56.2500%(18/32)\n",
            "Epoch [21] Batch[5540] - loss: 1.295152  acc: 34.3750%(11/32)\n",
            "Epoch [21] Batch[5560] - loss: 1.325794  acc: 56.2500%(18/32)\n",
            "Epoch [21] Batch[5580] - loss: 1.379501  acc: 34.3750%(11/32)\n",
            "Epoch [21] Batch[5600] - loss: 1.294864  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.376643  acc: 38.7829%(427/1101) \n",
            "\n",
            "Epoch [22] Batch[5620] - loss: 1.183330  acc: 65.6250%(21/32)\n",
            "Epoch [22] Batch[5640] - loss: 1.209077  acc: 62.5000%(20/32)\n",
            "Epoch [22] Batch[5660] - loss: 1.211799  acc: 53.1250%(17/32)\n",
            "Epoch [22] Batch[5680] - loss: 1.292673  acc: 40.6250%(13/32)\n",
            "Epoch [22] Batch[5700] - loss: 1.370510  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.374797  acc: 39.1462%(431/1101) \n",
            "\n",
            "Epoch [22] Batch[5720] - loss: 1.295824  acc: 50.0000%(16/32)\n",
            "Epoch [22] Batch[5740] - loss: 1.445770  acc: 31.2500%(10/32)\n",
            "Epoch [22] Batch[5760] - loss: 1.269447  acc: 53.1250%(17/32)\n",
            "Epoch [22] Batch[5780] - loss: 1.282985  acc: 40.6250%(13/32)\n",
            "Epoch [22] Batch[5800] - loss: 1.298469  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.371871  acc: 39.2371%(432/1101) \n",
            "\n",
            "Epoch [22] Batch[5820] - loss: 1.365097  acc: 50.0000%(16/32)\n",
            "Epoch [22] Batch[5840] - loss: 1.256128  acc: 40.6250%(13/32)\n",
            "Epoch [22] Batch[5860] - loss: 1.362921  acc: 43.7500%(14/32)\n",
            "Epoch [23] Batch[5880] - loss: 1.426588  acc: 31.2500%(10/32)\n",
            "Epoch [23] Batch[5900] - loss: 1.290775  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.369868  acc: 39.8728%(439/1101) \n",
            "\n",
            "Epoch [23] Batch[5920] - loss: 1.265760  acc: 50.0000%(16/32)\n",
            "Epoch [23] Batch[5940] - loss: 1.236274  acc: 56.2500%(18/32)\n",
            "Epoch [23] Batch[5960] - loss: 1.141462  acc: 68.7500%(22/32)\n",
            "Epoch [23] Batch[5980] - loss: 1.350756  acc: 46.8750%(15/32)\n",
            "Epoch [23] Batch[6000] - loss: 1.313643  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.368237  acc: 39.8728%(439/1101) \n",
            "\n",
            "Epoch [23] Batch[6020] - loss: 1.454253  acc: 28.1250%(9/32)\n",
            "Epoch [23] Batch[6040] - loss: 1.370804  acc: 34.3750%(11/32)\n",
            "Epoch [23] Batch[6060] - loss: 1.418639  acc: 34.3750%(11/32)\n",
            "Epoch [23] Batch[6080] - loss: 1.229610  acc: 53.1250%(17/32)\n",
            "Epoch [23] Batch[6100] - loss: 1.166359  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.365741  acc: 39.4187%(434/1101) \n",
            "\n",
            "Epoch [23] Batch[6120] - loss: 1.259758  acc: 50.0000%(16/32)\n",
            "Epoch [23] Batch[6140] - loss: 1.270013  acc: 46.8750%(15/32)\n",
            "Epoch [24] Batch[6160] - loss: 1.307863  acc: 37.5000%(12/32)\n",
            "Epoch [24] Batch[6180] - loss: 1.271575  acc: 50.0000%(16/32)\n",
            "Epoch [24] Batch[6200] - loss: 1.455897  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.363570  acc: 39.2371%(432/1101) \n",
            "\n",
            "Epoch [24] Batch[6220] - loss: 1.199286  acc: 43.7500%(14/32)\n",
            "Epoch [24] Batch[6240] - loss: 1.353757  acc: 40.6250%(13/32)\n",
            "Epoch [24] Batch[6260] - loss: 1.239386  acc: 50.0000%(16/32)\n",
            "Epoch [24] Batch[6280] - loss: 1.222387  acc: 56.2500%(18/32)\n",
            "Epoch [24] Batch[6300] - loss: 1.345534  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.362940  acc: 39.2371%(432/1101) \n",
            "\n",
            "Epoch [24] Batch[6320] - loss: 1.314099  acc: 43.7500%(14/32)\n",
            "Epoch [24] Batch[6340] - loss: 1.166146  acc: 53.1250%(17/32)\n",
            "Epoch [24] Batch[6360] - loss: 1.266265  acc: 43.7500%(14/32)\n",
            "Epoch [24] Batch[6380] - loss: 1.214712  acc: 59.3750%(19/32)\n",
            "Epoch [24] Batch[6400] - loss: 1.292436  acc: 37.5000%(12/32)\n",
            "\n",
            "Evaluation - loss: 1.359200  acc: 40.0545%(441/1101) \n",
            "\n",
            "save best_model.pt, metric: 40.05449591280654\n",
            "Epoch [25] Batch[6420] - loss: 1.189411  acc: 46.8750%(15/32)\n",
            "Epoch [25] Batch[6440] - loss: 1.299325  acc: 53.1250%(17/32)\n",
            "Epoch [25] Batch[6460] - loss: 1.221506  acc: 50.0000%(16/32)\n",
            "Epoch [25] Batch[6480] - loss: 1.219952  acc: 43.7500%(14/32)\n",
            "Epoch [25] Batch[6500] - loss: 1.111539  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.358147  acc: 39.6912%(437/1101) \n",
            "\n",
            "Epoch [25] Batch[6520] - loss: 1.239012  acc: 46.8750%(15/32)\n",
            "Epoch [25] Batch[6540] - loss: 1.288501  acc: 37.5000%(12/32)\n",
            "Epoch [25] Batch[6560] - loss: 1.204176  acc: 46.8750%(15/32)\n",
            "Epoch [25] Batch[6580] - loss: 1.205731  acc: 56.2500%(18/32)\n",
            "Epoch [25] Batch[6600] - loss: 1.252366  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.357222  acc: 39.4187%(434/1101) \n",
            "\n",
            "Epoch [25] Batch[6620] - loss: 1.212144  acc: 59.3750%(19/32)\n",
            "Epoch [25] Batch[6640] - loss: 1.205967  acc: 59.3750%(19/32)\n",
            "Epoch [25] Batch[6660] - loss: 1.170321  acc: 59.3750%(19/32)\n",
            "Epoch [26] Batch[6680] - loss: 1.258397  acc: 59.3750%(19/32)\n",
            "Epoch [26] Batch[6700] - loss: 1.350507  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.353930  acc: 40.2361%(443/1101) \n",
            "\n",
            "save best_model.pt, metric: 40.236148955495004\n",
            "Epoch [26] Batch[6720] - loss: 1.133321  acc: 65.6250%(21/32)\n",
            "Epoch [26] Batch[6740] - loss: 1.304191  acc: 40.6250%(13/32)\n",
            "Epoch [26] Batch[6760] - loss: 1.262746  acc: 56.2500%(18/32)\n",
            "Epoch [26] Batch[6780] - loss: 1.354578  acc: 46.8750%(15/32)\n",
            "Epoch [26] Batch[6800] - loss: 1.295924  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.353373  acc: 39.8728%(439/1101) \n",
            "\n",
            "Epoch [26] Batch[6820] - loss: 1.279461  acc: 43.7500%(14/32)\n",
            "Epoch [26] Batch[6840] - loss: 1.261280  acc: 46.8750%(15/32)\n",
            "Epoch [26] Batch[6860] - loss: 1.265159  acc: 56.2500%(18/32)\n",
            "Epoch [26] Batch[6880] - loss: 1.347837  acc: 46.8750%(15/32)\n",
            "Epoch [26] Batch[6900] - loss: 1.176086  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.351496  acc: 39.9637%(440/1101) \n",
            "\n",
            "Epoch [26] Batch[6920] - loss: 1.225045  acc: 50.0000%(16/32)\n",
            "Epoch [26] Batch[6940] - loss: 1.321379  acc: 46.8750%(15/32)\n",
            "Epoch [27] Batch[6960] - loss: 1.303284  acc: 37.5000%(12/32)\n",
            "Epoch [27] Batch[6980] - loss: 1.311838  acc: 53.1250%(17/32)\n",
            "Epoch [27] Batch[7000] - loss: 1.213333  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.350515  acc: 39.4187%(434/1101) \n",
            "\n",
            "Epoch [27] Batch[7020] - loss: 1.221002  acc: 50.0000%(16/32)\n",
            "Epoch [27] Batch[7040] - loss: 1.265289  acc: 40.6250%(13/32)\n",
            "Epoch [27] Batch[7060] - loss: 1.312222  acc: 40.6250%(13/32)\n",
            "Epoch [27] Batch[7080] - loss: 1.201989  acc: 46.8750%(15/32)\n",
            "Epoch [27] Batch[7100] - loss: 1.316084  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.348160  acc: 39.6912%(437/1101) \n",
            "\n",
            "Epoch [27] Batch[7120] - loss: 1.215537  acc: 59.3750%(19/32)\n",
            "Epoch [27] Batch[7140] - loss: 1.335618  acc: 50.0000%(16/32)\n",
            "Epoch [27] Batch[7160] - loss: 1.183226  acc: 53.1250%(17/32)\n",
            "Epoch [27] Batch[7180] - loss: 1.248813  acc: 43.7500%(14/32)\n",
            "Epoch [27] Batch[7200] - loss: 1.134476  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.348026  acc: 39.7820%(438/1101) \n",
            "\n",
            "Epoch [28] Batch[7220] - loss: 1.241531  acc: 62.5000%(20/32)\n",
            "Epoch [28] Batch[7240] - loss: 1.180946  acc: 46.8750%(15/32)\n",
            "Epoch [28] Batch[7260] - loss: 1.253376  acc: 46.8750%(15/32)\n",
            "Epoch [28] Batch[7280] - loss: 1.283848  acc: 37.5000%(12/32)\n",
            "Epoch [28] Batch[7300] - loss: 1.259846  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.346010  acc: 40.0545%(441/1101) \n",
            "\n",
            "Epoch [28] Batch[7320] - loss: 1.241478  acc: 34.3750%(11/32)\n",
            "Epoch [28] Batch[7340] - loss: 1.224179  acc: 56.2500%(18/32)\n",
            "Epoch [28] Batch[7360] - loss: 1.169953  acc: 59.3750%(19/32)\n",
            "Epoch [28] Batch[7380] - loss: 1.266421  acc: 50.0000%(16/32)\n",
            "Epoch [28] Batch[7400] - loss: 1.305361  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.344512  acc: 39.8728%(439/1101) \n",
            "\n",
            "Epoch [28] Batch[7420] - loss: 1.166963  acc: 56.2500%(18/32)\n",
            "Epoch [28] Batch[7440] - loss: 1.137372  acc: 56.2500%(18/32)\n",
            "Epoch [28] Batch[7460] - loss: 1.228128  acc: 53.1250%(17/32)\n",
            "Epoch [29] Batch[7480] - loss: 1.147027  acc: 59.3750%(19/32)\n",
            "Epoch [29] Batch[7500] - loss: 1.236871  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.343127  acc: 39.7820%(438/1101) \n",
            "\n",
            "Epoch [29] Batch[7520] - loss: 1.206021  acc: 50.0000%(16/32)\n",
            "Epoch [29] Batch[7540] - loss: 1.185182  acc: 56.2500%(18/32)\n",
            "Epoch [29] Batch[7560] - loss: 1.263963  acc: 53.1250%(17/32)\n",
            "Epoch [29] Batch[7580] - loss: 1.231630  acc: 53.1250%(17/32)\n",
            "Epoch [29] Batch[7600] - loss: 1.098996  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.341366  acc: 39.8728%(439/1101) \n",
            "\n",
            "Epoch [29] Batch[7620] - loss: 1.239937  acc: 53.1250%(17/32)\n",
            "Epoch [29] Batch[7640] - loss: 1.236341  acc: 62.5000%(20/32)\n",
            "Epoch [29] Batch[7660] - loss: 1.256476  acc: 46.8750%(15/32)\n",
            "Epoch [29] Batch[7680] - loss: 1.287450  acc: 43.7500%(14/32)\n",
            "Epoch [29] Batch[7700] - loss: 1.334864  acc: 28.1250%(9/32)\n",
            "\n",
            "Evaluation - loss: 1.339050  acc: 39.9637%(440/1101) \n",
            "\n",
            "Epoch [29] Batch[7720] - loss: 1.209942  acc: 56.2500%(18/32)\n",
            "Epoch [29] Batch[7740] - loss: 1.517011  acc: 28.1250%(9/32)\n",
            "Epoch [30] Batch[7760] - loss: 1.217172  acc: 50.0000%(16/32)\n",
            "Epoch [30] Batch[7780] - loss: 1.305956  acc: 34.3750%(11/32)\n",
            "Epoch [30] Batch[7800] - loss: 1.321793  acc: 37.5000%(12/32)\n",
            "\n",
            "Evaluation - loss: 1.340612  acc: 40.1453%(442/1101) \n",
            "\n",
            "Epoch [30] Batch[7820] - loss: 1.173465  acc: 59.3750%(19/32)\n",
            "Epoch [30] Batch[7840] - loss: 1.229895  acc: 65.6250%(21/32)\n",
            "Epoch [30] Batch[7860] - loss: 1.173634  acc: 59.3750%(19/32)\n",
            "Epoch [30] Batch[7880] - loss: 1.207661  acc: 59.3750%(19/32)\n",
            "Epoch [30] Batch[7900] - loss: 1.307445  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.337678  acc: 40.1453%(442/1101) \n",
            "\n",
            "Epoch [30] Batch[7920] - loss: 1.230005  acc: 53.1250%(17/32)\n",
            "Epoch [30] Batch[7940] - loss: 1.158455  acc: 53.1250%(17/32)\n",
            "Epoch [30] Batch[7960] - loss: 1.153443  acc: 53.1250%(17/32)\n",
            "Epoch [30] Batch[7980] - loss: 1.185390  acc: 62.5000%(20/32)\n",
            "Epoch [30] Batch[8000] - loss: 1.256447  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.335398  acc: 40.5995%(447/1101) \n",
            "\n",
            "save best_model.pt, metric: 40.59945504087194\n",
            "Epoch [31] Batch[8020] - loss: 1.322763  acc: 34.3750%(11/32)\n",
            "Epoch [31] Batch[8040] - loss: 1.223188  acc: 56.2500%(18/32)\n",
            "Epoch [31] Batch[8060] - loss: 1.255552  acc: 56.2500%(18/32)\n",
            "Epoch [31] Batch[8080] - loss: 1.239321  acc: 43.7500%(14/32)\n",
            "Epoch [31] Batch[8100] - loss: 1.269998  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.335163  acc: 40.3270%(444/1101) \n",
            "\n",
            "Epoch [31] Batch[8120] - loss: 1.176727  acc: 59.3750%(19/32)\n",
            "Epoch [31] Batch[8140] - loss: 1.177370  acc: 53.1250%(17/32)\n",
            "Epoch [31] Batch[8160] - loss: 1.167745  acc: 43.7500%(14/32)\n",
            "Epoch [31] Batch[8180] - loss: 1.261616  acc: 37.5000%(12/32)\n",
            "Epoch [31] Batch[8200] - loss: 1.118440  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.333303  acc: 40.6903%(448/1101) \n",
            "\n",
            "save best_model.pt, metric: 40.690281562216164\n",
            "Epoch [31] Batch[8220] - loss: 1.511339  acc: 37.5000%(12/32)\n",
            "Epoch [31] Batch[8240] - loss: 1.143102  acc: 62.5000%(20/32)\n",
            "Epoch [31] Batch[8260] - loss: 1.244985  acc: 46.8750%(15/32)\n",
            "Epoch [32] Batch[8280] - loss: 1.318125  acc: 40.6250%(13/32)\n",
            "Epoch [32] Batch[8300] - loss: 1.172212  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.333512  acc: 40.5995%(447/1101) \n",
            "\n",
            "Epoch [32] Batch[8320] - loss: 1.173731  acc: 59.3750%(19/32)\n",
            "Epoch [32] Batch[8340] - loss: 1.108246  acc: 59.3750%(19/32)\n",
            "Epoch [32] Batch[8360] - loss: 1.095381  acc: 59.3750%(19/32)\n",
            "Epoch [32] Batch[8380] - loss: 1.150424  acc: 56.2500%(18/32)\n",
            "Epoch [32] Batch[8400] - loss: 1.124043  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.330735  acc: 40.4178%(445/1101) \n",
            "\n",
            "Epoch [32] Batch[8420] - loss: 1.175596  acc: 56.2500%(18/32)\n",
            "Epoch [32] Batch[8440] - loss: 1.105306  acc: 65.6250%(21/32)\n",
            "Epoch [32] Batch[8460] - loss: 1.092376  acc: 59.3750%(19/32)\n",
            "Epoch [32] Batch[8480] - loss: 1.211315  acc: 50.0000%(16/32)\n",
            "Epoch [32] Batch[8500] - loss: 1.091729  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.328442  acc: 40.6903%(448/1101) \n",
            "\n",
            "Epoch [32] Batch[8520] - loss: 1.219865  acc: 53.1250%(17/32)\n",
            "Epoch [32] Batch[8540] - loss: 1.189785  acc: 34.3750%(11/32)\n",
            "Epoch [33] Batch[8560] - loss: 1.227005  acc: 53.1250%(17/32)\n",
            "Epoch [33] Batch[8580] - loss: 1.180888  acc: 53.1250%(17/32)\n",
            "Epoch [33] Batch[8600] - loss: 1.285207  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.328127  acc: 40.7811%(449/1101) \n",
            "\n",
            "save best_model.pt, metric: 40.7811080835604\n",
            "Epoch [33] Batch[8620] - loss: 1.192495  acc: 56.2500%(18/32)\n",
            "Epoch [33] Batch[8640] - loss: 1.228993  acc: 56.2500%(18/32)\n",
            "Epoch [33] Batch[8660] - loss: 1.253015  acc: 46.8750%(15/32)\n",
            "Epoch [33] Batch[8680] - loss: 1.135380  acc: 56.2500%(18/32)\n",
            "Epoch [33] Batch[8700] - loss: 1.177705  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.329195  acc: 40.6903%(448/1101) \n",
            "\n",
            "Epoch [33] Batch[8720] - loss: 1.132769  acc: 59.3750%(19/32)\n",
            "Epoch [33] Batch[8740] - loss: 1.175552  acc: 43.7500%(14/32)\n",
            "Epoch [33] Batch[8760] - loss: 1.327921  acc: 46.8750%(15/32)\n",
            "Epoch [33] Batch[8780] - loss: 1.211426  acc: 43.7500%(14/32)\n",
            "Epoch [33] Batch[8800] - loss: 1.196493  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.326502  acc: 40.8719%(450/1101) \n",
            "\n",
            "save best_model.pt, metric: 40.87193460490463\n",
            "Epoch [34] Batch[8820] - loss: 1.093067  acc: 68.7500%(22/32)\n",
            "Epoch [34] Batch[8840] - loss: 1.182733  acc: 56.2500%(18/32)\n",
            "Epoch [34] Batch[8860] - loss: 1.154581  acc: 50.0000%(16/32)\n",
            "Epoch [34] Batch[8880] - loss: 1.198713  acc: 46.8750%(15/32)\n",
            "Epoch [34] Batch[8900] - loss: 1.254094  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.325855  acc: 41.0536%(452/1101) \n",
            "\n",
            "save best_model.pt, metric: 41.0535876475931\n",
            "Epoch [34] Batch[8920] - loss: 1.131837  acc: 46.8750%(15/32)\n",
            "Epoch [34] Batch[8940] - loss: 1.175595  acc: 53.1250%(17/32)\n",
            "Epoch [34] Batch[8960] - loss: 1.187538  acc: 53.1250%(17/32)\n",
            "Epoch [34] Batch[8980] - loss: 1.066983  acc: 81.2500%(26/32)\n",
            "Epoch [34] Batch[9000] - loss: 1.182339  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.325094  acc: 39.7820%(438/1101) \n",
            "\n",
            "Epoch [34] Batch[9020] - loss: 0.960412  acc: 81.2500%(26/32)\n",
            "Epoch [34] Batch[9040] - loss: 1.245823  acc: 43.7500%(14/32)\n",
            "Epoch [34] Batch[9060] - loss: 1.146908  acc: 56.2500%(18/32)\n",
            "Epoch [35] Batch[9080] - loss: 1.200425  acc: 43.7500%(14/32)\n",
            "Epoch [35] Batch[9100] - loss: 1.114628  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.324604  acc: 40.7811%(449/1101) \n",
            "\n",
            "Epoch [35] Batch[9120] - loss: 1.175701  acc: 59.3750%(19/32)\n",
            "Epoch [35] Batch[9140] - loss: 1.231440  acc: 50.0000%(16/32)\n",
            "Epoch [35] Batch[9160] - loss: 1.182170  acc: 46.8750%(15/32)\n",
            "Epoch [35] Batch[9180] - loss: 1.227095  acc: 50.0000%(16/32)\n",
            "Epoch [35] Batch[9200] - loss: 1.128167  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.322718  acc: 40.5086%(446/1101) \n",
            "\n",
            "Epoch [35] Batch[9220] - loss: 1.261687  acc: 53.1250%(17/32)\n",
            "Epoch [35] Batch[9240] - loss: 1.160973  acc: 75.0000%(24/32)\n",
            "Epoch [35] Batch[9260] - loss: 1.292367  acc: 43.7500%(14/32)\n",
            "Epoch [35] Batch[9280] - loss: 1.143447  acc: 53.1250%(17/32)\n",
            "Epoch [35] Batch[9300] - loss: 1.244921  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.321600  acc: 40.7811%(449/1101) \n",
            "\n",
            "Epoch [35] Batch[9320] - loss: 1.231616  acc: 53.1250%(17/32)\n",
            "Epoch [35] Batch[9340] - loss: 1.122736  acc: 65.6250%(21/32)\n",
            "Epoch [36] Batch[9360] - loss: 1.318126  acc: 53.1250%(17/32)\n",
            "Epoch [36] Batch[9380] - loss: 1.106155  acc: 53.1250%(17/32)\n",
            "Epoch [36] Batch[9400] - loss: 1.032414  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.322595  acc: 41.1444%(453/1101) \n",
            "\n",
            "save best_model.pt, metric: 41.14441416893733\n",
            "Epoch [36] Batch[9420] - loss: 1.102219  acc: 68.7500%(22/32)\n",
            "Epoch [36] Batch[9440] - loss: 1.216715  acc: 59.3750%(19/32)\n",
            "Epoch [36] Batch[9460] - loss: 1.188774  acc: 53.1250%(17/32)\n",
            "Epoch [36] Batch[9480] - loss: 1.076396  acc: 62.5000%(20/32)\n",
            "Epoch [36] Batch[9500] - loss: 1.247505  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.319300  acc: 41.0536%(452/1101) \n",
            "\n",
            "Epoch [36] Batch[9520] - loss: 1.144999  acc: 53.1250%(17/32)\n",
            "Epoch [36] Batch[9540] - loss: 1.023493  acc: 56.2500%(18/32)\n",
            "Epoch [36] Batch[9560] - loss: 1.299050  acc: 59.3750%(19/32)\n",
            "Epoch [36] Batch[9580] - loss: 1.057790  acc: 65.6250%(21/32)\n",
            "Epoch [36] Batch[9600] - loss: 1.182049  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.319203  acc: 40.8719%(450/1101) \n",
            "\n",
            "Epoch [37] Batch[9620] - loss: 1.180419  acc: 62.5000%(20/32)\n",
            "Epoch [37] Batch[9640] - loss: 1.022514  acc: 62.5000%(20/32)\n",
            "Epoch [37] Batch[9660] - loss: 1.030872  acc: 59.3750%(19/32)\n",
            "Epoch [37] Batch[9680] - loss: 1.061678  acc: 71.8750%(23/32)\n",
            "Epoch [37] Batch[9700] - loss: 1.131910  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.317453  acc: 41.1444%(453/1101) \n",
            "\n",
            "Epoch [37] Batch[9720] - loss: 1.227577  acc: 53.1250%(17/32)\n",
            "Epoch [37] Batch[9740] - loss: 1.077477  acc: 56.2500%(18/32)\n",
            "Epoch [37] Batch[9760] - loss: 1.180973  acc: 62.5000%(20/32)\n",
            "Epoch [37] Batch[9780] - loss: 1.077888  acc: 65.6250%(21/32)\n",
            "Epoch [37] Batch[9800] - loss: 1.149267  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.317918  acc: 40.8719%(450/1101) \n",
            "\n",
            "Epoch [37] Batch[9820] - loss: 1.119363  acc: 53.1250%(17/32)\n",
            "Epoch [37] Batch[9840] - loss: 1.176243  acc: 62.5000%(20/32)\n",
            "Epoch [37] Batch[9860] - loss: 1.119486  acc: 50.0000%(16/32)\n",
            "Epoch [38] Batch[9880] - loss: 1.183028  acc: 62.5000%(20/32)\n",
            "Epoch [38] Batch[9900] - loss: 1.057635  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.317020  acc: 41.7802%(460/1101) \n",
            "\n",
            "save best_model.pt, metric: 41.78019981834696\n",
            "Epoch [38] Batch[9920] - loss: 1.110653  acc: 65.6250%(21/32)\n",
            "Epoch [38] Batch[9940] - loss: 1.106191  acc: 59.3750%(19/32)\n",
            "Epoch [38] Batch[9960] - loss: 1.191238  acc: 53.1250%(17/32)\n",
            "Epoch [38] Batch[9980] - loss: 1.212918  acc: 46.8750%(15/32)\n",
            "Epoch [38] Batch[10000] - loss: 1.128382  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.314482  acc: 41.7802%(460/1101) \n",
            "\n",
            "Epoch [38] Batch[10020] - loss: 1.086488  acc: 59.3750%(19/32)\n",
            "Epoch [38] Batch[10040] - loss: 1.120647  acc: 59.3750%(19/32)\n",
            "Epoch [38] Batch[10060] - loss: 1.195360  acc: 50.0000%(16/32)\n",
            "Epoch [38] Batch[10080] - loss: 1.333637  acc: 28.1250%(9/32)\n",
            "Epoch [38] Batch[10100] - loss: 1.071277  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.315574  acc: 40.8719%(450/1101) \n",
            "\n",
            "Epoch [38] Batch[10120] - loss: 1.131220  acc: 68.7500%(22/32)\n",
            "Epoch [38] Batch[10140] - loss: 1.190197  acc: 50.0000%(16/32)\n",
            "Epoch [39] Batch[10160] - loss: 1.107051  acc: 56.2500%(18/32)\n",
            "Epoch [39] Batch[10180] - loss: 1.082807  acc: 53.1250%(17/32)\n",
            "Epoch [39] Batch[10200] - loss: 1.099255  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.313074  acc: 41.3261%(455/1101) \n",
            "\n",
            "Epoch [39] Batch[10220] - loss: 1.216340  acc: 56.2500%(18/32)\n",
            "Epoch [39] Batch[10240] - loss: 1.185036  acc: 40.6250%(13/32)\n",
            "Epoch [39] Batch[10260] - loss: 1.322091  acc: 37.5000%(12/32)\n",
            "Epoch [39] Batch[10280] - loss: 1.133888  acc: 56.2500%(18/32)\n",
            "Epoch [39] Batch[10300] - loss: 1.149879  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.312455  acc: 41.2352%(454/1101) \n",
            "\n",
            "Epoch [39] Batch[10320] - loss: 1.077396  acc: 62.5000%(20/32)\n",
            "Epoch [39] Batch[10340] - loss: 1.207857  acc: 50.0000%(16/32)\n",
            "Epoch [39] Batch[10360] - loss: 1.181705  acc: 56.2500%(18/32)\n",
            "Epoch [39] Batch[10380] - loss: 0.983631  acc: 75.0000%(24/32)\n",
            "Epoch [39] Batch[10400] - loss: 1.162243  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.314604  acc: 40.7811%(449/1101) \n",
            "\n",
            "Epoch [40] Batch[10420] - loss: 1.222667  acc: 56.2500%(18/32)\n",
            "Epoch [40] Batch[10440] - loss: 1.145190  acc: 43.7500%(14/32)\n",
            "Epoch [40] Batch[10460] - loss: 1.166819  acc: 59.3750%(19/32)\n",
            "Epoch [40] Batch[10480] - loss: 1.056956  acc: 62.5000%(20/32)\n",
            "Epoch [40] Batch[10500] - loss: 1.287707  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.310464  acc: 41.5077%(457/1101) \n",
            "\n",
            "Epoch [40] Batch[10520] - loss: 1.078185  acc: 59.3750%(19/32)\n",
            "Epoch [40] Batch[10540] - loss: 1.216364  acc: 62.5000%(20/32)\n",
            "Epoch [40] Batch[10560] - loss: 1.030614  acc: 62.5000%(20/32)\n",
            "Epoch [40] Batch[10580] - loss: 1.066779  acc: 62.5000%(20/32)\n",
            "Epoch [40] Batch[10600] - loss: 1.152059  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.311891  acc: 40.6903%(448/1101) \n",
            "\n",
            "Epoch [40] Batch[10620] - loss: 1.090732  acc: 62.5000%(20/32)\n",
            "Epoch [40] Batch[10640] - loss: 1.171326  acc: 53.1250%(17/32)\n",
            "Epoch [40] Batch[10660] - loss: 1.101324  acc: 62.5000%(20/32)\n",
            "Epoch [40] Batch[10680] - loss: 1.155747  acc: 62.5000%(20/32)\n",
            "Epoch [41] Batch[10700] - loss: 1.103752  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.310353  acc: 41.3261%(455/1101) \n",
            "\n",
            "Epoch [41] Batch[10720] - loss: 1.004252  acc: 68.7500%(22/32)\n",
            "Epoch [41] Batch[10740] - loss: 1.192086  acc: 53.1250%(17/32)\n",
            "Epoch [41] Batch[10760] - loss: 1.235273  acc: 53.1250%(17/32)\n",
            "Epoch [41] Batch[10780] - loss: 1.117246  acc: 62.5000%(20/32)\n",
            "Epoch [41] Batch[10800] - loss: 1.264788  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.309469  acc: 41.1444%(453/1101) \n",
            "\n",
            "Epoch [41] Batch[10820] - loss: 0.975874  acc: 78.1250%(25/32)\n",
            "Epoch [41] Batch[10840] - loss: 1.092504  acc: 59.3750%(19/32)\n",
            "Epoch [41] Batch[10860] - loss: 1.218296  acc: 46.8750%(15/32)\n",
            "Epoch [41] Batch[10880] - loss: 1.051498  acc: 65.6250%(21/32)\n",
            "Epoch [41] Batch[10900] - loss: 1.151079  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.309282  acc: 41.2352%(454/1101) \n",
            "\n",
            "Epoch [41] Batch[10920] - loss: 1.139013  acc: 56.2500%(18/32)\n",
            "Epoch [41] Batch[10940] - loss: 1.208340  acc: 56.2500%(18/32)\n",
            "Epoch [42] Batch[10960] - loss: 1.116744  acc: 53.1250%(17/32)\n",
            "Epoch [42] Batch[10980] - loss: 1.279033  acc: 50.0000%(16/32)\n",
            "Epoch [42] Batch[11000] - loss: 1.094312  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.307781  acc: 41.8710%(461/1101) \n",
            "\n",
            "save best_model.pt, metric: 41.87102633969119\n",
            "Epoch [42] Batch[11020] - loss: 1.138206  acc: 65.6250%(21/32)\n",
            "Epoch [42] Batch[11040] - loss: 1.102629  acc: 53.1250%(17/32)\n",
            "Epoch [42] Batch[11060] - loss: 1.090477  acc: 56.2500%(18/32)\n",
            "Epoch [42] Batch[11080] - loss: 1.151360  acc: 65.6250%(21/32)\n",
            "Epoch [42] Batch[11100] - loss: 1.190985  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.307780  acc: 41.0536%(452/1101) \n",
            "\n",
            "Epoch [42] Batch[11120] - loss: 1.045266  acc: 75.0000%(24/32)\n",
            "Epoch [42] Batch[11140] - loss: 1.180472  acc: 62.5000%(20/32)\n",
            "Epoch [42] Batch[11160] - loss: 1.243438  acc: 46.8750%(15/32)\n",
            "Epoch [42] Batch[11180] - loss: 1.089473  acc: 53.1250%(17/32)\n",
            "Epoch [42] Batch[11200] - loss: 1.117361  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.308317  acc: 41.3261%(455/1101) \n",
            "\n",
            "Epoch [43] Batch[11220] - loss: 1.134988  acc: 65.6250%(21/32)\n",
            "Epoch [43] Batch[11240] - loss: 1.058566  acc: 68.7500%(22/32)\n",
            "Epoch [43] Batch[11260] - loss: 1.100127  acc: 62.5000%(20/32)\n",
            "Epoch [43] Batch[11280] - loss: 1.075475  acc: 56.2500%(18/32)\n",
            "Epoch [43] Batch[11300] - loss: 0.997667  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.306034  acc: 41.4169%(456/1101) \n",
            "\n",
            "Epoch [43] Batch[11320] - loss: 1.081981  acc: 56.2500%(18/32)\n",
            "Epoch [43] Batch[11340] - loss: 1.125272  acc: 59.3750%(19/32)\n",
            "Epoch [43] Batch[11360] - loss: 1.074320  acc: 50.0000%(16/32)\n",
            "Epoch [43] Batch[11380] - loss: 1.056295  acc: 53.1250%(17/32)\n",
            "Epoch [43] Batch[11400] - loss: 1.180797  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.304371  acc: 41.5077%(457/1101) \n",
            "\n",
            "Epoch [43] Batch[11420] - loss: 0.930933  acc: 75.0000%(24/32)\n",
            "Epoch [43] Batch[11440] - loss: 1.210820  acc: 53.1250%(17/32)\n",
            "Epoch [43] Batch[11460] - loss: 0.993331  acc: 59.3750%(19/32)\n",
            "Epoch [43] Batch[11480] - loss: 0.995290  acc: 59.3750%(19/32)\n",
            "Epoch [44] Batch[11500] - loss: 1.091514  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.305927  acc: 40.9628%(451/1101) \n",
            "\n",
            "Epoch [44] Batch[11520] - loss: 1.061811  acc: 59.3750%(19/32)\n",
            "Epoch [44] Batch[11540] - loss: 1.198519  acc: 46.8750%(15/32)\n",
            "Epoch [44] Batch[11560] - loss: 1.096294  acc: 65.6250%(21/32)\n",
            "Epoch [44] Batch[11580] - loss: 1.155377  acc: 50.0000%(16/32)\n",
            "Epoch [44] Batch[11600] - loss: 1.026619  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.305766  acc: 41.3261%(455/1101) \n",
            "\n",
            "Epoch [44] Batch[11620] - loss: 1.058025  acc: 50.0000%(16/32)\n",
            "Epoch [44] Batch[11640] - loss: 0.970848  acc: 71.8750%(23/32)\n",
            "Epoch [44] Batch[11660] - loss: 1.262665  acc: 56.2500%(18/32)\n",
            "Epoch [44] Batch[11680] - loss: 1.217433  acc: 37.5000%(12/32)\n",
            "Epoch [44] Batch[11700] - loss: 1.011292  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.305808  acc: 41.4169%(456/1101) \n",
            "\n",
            "Epoch [44] Batch[11720] - loss: 1.048558  acc: 59.3750%(19/32)\n",
            "Epoch [44] Batch[11740] - loss: 1.115223  acc: 68.7500%(22/32)\n",
            "Epoch [45] Batch[11760] - loss: 1.149629  acc: 56.2500%(18/32)\n",
            "Epoch [45] Batch[11780] - loss: 1.129163  acc: 56.2500%(18/32)\n",
            "Epoch [45] Batch[11800] - loss: 1.231242  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.303375  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [45] Batch[11820] - loss: 1.047079  acc: 62.5000%(20/32)\n",
            "Epoch [45] Batch[11840] - loss: 1.103901  acc: 62.5000%(20/32)\n",
            "Epoch [45] Batch[11860] - loss: 1.040963  acc: 68.7500%(22/32)\n",
            "Epoch [45] Batch[11880] - loss: 1.068808  acc: 50.0000%(16/32)\n",
            "Epoch [45] Batch[11900] - loss: 1.251833  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.302786  acc: 41.5077%(457/1101) \n",
            "\n",
            "Epoch [45] Batch[11920] - loss: 0.994185  acc: 65.6250%(21/32)\n",
            "Epoch [45] Batch[11940] - loss: 1.051876  acc: 71.8750%(23/32)\n",
            "Epoch [45] Batch[11960] - loss: 1.099599  acc: 50.0000%(16/32)\n",
            "Epoch [45] Batch[11980] - loss: 1.155595  acc: 53.1250%(17/32)\n",
            "Epoch [45] Batch[12000] - loss: 1.056413  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.300541  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [46] Batch[12020] - loss: 1.099361  acc: 62.5000%(20/32)\n",
            "Epoch [46] Batch[12040] - loss: 1.057410  acc: 62.5000%(20/32)\n",
            "Epoch [46] Batch[12060] - loss: 1.040981  acc: 62.5000%(20/32)\n",
            "Epoch [46] Batch[12080] - loss: 1.151322  acc: 62.5000%(20/32)\n",
            "Epoch [46] Batch[12100] - loss: 1.066555  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.302695  acc: 42.0527%(463/1101) \n",
            "\n",
            "save best_model.pt, metric: 42.05267938237966\n",
            "Epoch [46] Batch[12120] - loss: 1.194079  acc: 50.0000%(16/32)\n",
            "Epoch [46] Batch[12140] - loss: 1.047354  acc: 62.5000%(20/32)\n",
            "Epoch [46] Batch[12160] - loss: 1.114972  acc: 50.0000%(16/32)\n",
            "Epoch [46] Batch[12180] - loss: 1.034355  acc: 75.0000%(24/32)\n",
            "Epoch [46] Batch[12200] - loss: 1.161231  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.301895  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [46] Batch[12220] - loss: 1.122815  acc: 65.6250%(21/32)\n",
            "Epoch [46] Batch[12240] - loss: 1.139342  acc: 59.3750%(19/32)\n",
            "Epoch [46] Batch[12260] - loss: 1.115348  acc: 53.1250%(17/32)\n",
            "Epoch [46] Batch[12280] - loss: 1.141935  acc: 50.0000%(16/32)\n",
            "Epoch [47] Batch[12300] - loss: 1.084892  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.299120  acc: 41.7802%(460/1101) \n",
            "\n",
            "Epoch [47] Batch[12320] - loss: 1.170214  acc: 46.8750%(15/32)\n",
            "Epoch [47] Batch[12340] - loss: 1.153263  acc: 62.5000%(20/32)\n",
            "Epoch [47] Batch[12360] - loss: 1.008757  acc: 62.5000%(20/32)\n",
            "Epoch [47] Batch[12380] - loss: 1.011058  acc: 65.6250%(21/32)\n",
            "Epoch [47] Batch[12400] - loss: 1.105962  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.299577  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [47] Batch[12420] - loss: 1.113160  acc: 56.2500%(18/32)\n",
            "Epoch [47] Batch[12440] - loss: 1.171659  acc: 46.8750%(15/32)\n",
            "Epoch [47] Batch[12460] - loss: 1.043926  acc: 56.2500%(18/32)\n",
            "Epoch [47] Batch[12480] - loss: 1.102960  acc: 59.3750%(19/32)\n",
            "Epoch [47] Batch[12500] - loss: 1.118864  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.299345  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [47] Batch[12520] - loss: 1.027920  acc: 68.7500%(22/32)\n",
            "Epoch [47] Batch[12540] - loss: 1.009547  acc: 75.0000%(24/32)\n",
            "Epoch [48] Batch[12560] - loss: 1.016207  acc: 62.5000%(20/32)\n",
            "Epoch [48] Batch[12580] - loss: 1.092936  acc: 46.8750%(15/32)\n",
            "Epoch [48] Batch[12600] - loss: 1.124740  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.298407  acc: 42.3252%(466/1101) \n",
            "\n",
            "save best_model.pt, metric: 42.32515894641235\n",
            "Epoch [48] Batch[12620] - loss: 1.132338  acc: 62.5000%(20/32)\n",
            "Epoch [48] Batch[12640] - loss: 1.103966  acc: 59.3750%(19/32)\n",
            "Epoch [48] Batch[12660] - loss: 0.922548  acc: 78.1250%(25/32)\n",
            "Epoch [48] Batch[12680] - loss: 1.135812  acc: 59.3750%(19/32)\n",
            "Epoch [48] Batch[12700] - loss: 1.141532  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.297480  acc: 41.7802%(460/1101) \n",
            "\n",
            "Epoch [48] Batch[12720] - loss: 1.119085  acc: 62.5000%(20/32)\n",
            "Epoch [48] Batch[12740] - loss: 1.058049  acc: 62.5000%(20/32)\n",
            "Epoch [48] Batch[12760] - loss: 1.110222  acc: 53.1250%(17/32)\n",
            "Epoch [48] Batch[12780] - loss: 1.084542  acc: 59.3750%(19/32)\n",
            "Epoch [48] Batch[12800] - loss: 1.024327  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.299305  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [49] Batch[12820] - loss: 1.169812  acc: 50.0000%(16/32)\n",
            "Epoch [49] Batch[12840] - loss: 1.039832  acc: 59.3750%(19/32)\n",
            "Epoch [49] Batch[12860] - loss: 0.995404  acc: 62.5000%(20/32)\n",
            "Epoch [49] Batch[12880] - loss: 1.115903  acc: 50.0000%(16/32)\n",
            "Epoch [49] Batch[12900] - loss: 1.087386  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.296291  acc: 42.4160%(467/1101) \n",
            "\n",
            "save best_model.pt, metric: 42.415985467756585\n",
            "Epoch [49] Batch[12920] - loss: 1.128651  acc: 68.7500%(22/32)\n",
            "Epoch [49] Batch[12940] - loss: 1.074767  acc: 56.2500%(18/32)\n",
            "Epoch [49] Batch[12960] - loss: 0.955196  acc: 62.5000%(20/32)\n",
            "Epoch [49] Batch[12980] - loss: 1.213715  acc: 43.7500%(14/32)\n",
            "Epoch [49] Batch[13000] - loss: 1.039773  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.298366  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [49] Batch[13020] - loss: 1.078854  acc: 56.2500%(18/32)\n",
            "Epoch [49] Batch[13040] - loss: 1.109030  acc: 56.2500%(18/32)\n",
            "Epoch [49] Batch[13060] - loss: 1.108624  acc: 50.0000%(16/32)\n",
            "Epoch [49] Batch[13080] - loss: 1.164631  acc: 50.0000%(16/32)\n",
            "Epoch [50] Batch[13100] - loss: 1.052792  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.295627  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [50] Batch[13120] - loss: 1.116947  acc: 43.7500%(14/32)\n",
            "Epoch [50] Batch[13140] - loss: 1.081745  acc: 59.3750%(19/32)\n",
            "Epoch [50] Batch[13160] - loss: 1.178573  acc: 62.5000%(20/32)\n",
            "Epoch [50] Batch[13180] - loss: 1.078745  acc: 62.5000%(20/32)\n",
            "Epoch [50] Batch[13200] - loss: 1.134906  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.295872  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [50] Batch[13220] - loss: 1.024838  acc: 62.5000%(20/32)\n",
            "Epoch [50] Batch[13240] - loss: 1.155943  acc: 53.1250%(17/32)\n",
            "Epoch [50] Batch[13260] - loss: 0.977897  acc: 78.1250%(25/32)\n",
            "Epoch [50] Batch[13280] - loss: 1.315064  acc: 40.6250%(13/32)\n",
            "Epoch [50] Batch[13300] - loss: 0.942927  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.294946  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [50] Batch[13320] - loss: 1.053948  acc: 62.5000%(20/32)\n",
            "Epoch [50] Batch[13340] - loss: 1.051744  acc: 53.1250%(17/32)\n",
            "Epoch [51] Batch[13360] - loss: 1.005899  acc: 56.2500%(18/32)\n",
            "Epoch [51] Batch[13380] - loss: 1.050170  acc: 68.7500%(22/32)\n",
            "Epoch [51] Batch[13400] - loss: 0.976737  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.295269  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [51] Batch[13420] - loss: 0.993921  acc: 62.5000%(20/32)\n",
            "Epoch [51] Batch[13440] - loss: 1.152944  acc: 59.3750%(19/32)\n",
            "Epoch [51] Batch[13460] - loss: 1.092614  acc: 59.3750%(19/32)\n",
            "Epoch [51] Batch[13480] - loss: 1.006058  acc: 56.2500%(18/32)\n",
            "Epoch [51] Batch[13500] - loss: 1.047987  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.294324  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [51] Batch[13520] - loss: 1.019732  acc: 65.6250%(21/32)\n",
            "Epoch [51] Batch[13540] - loss: 1.090787  acc: 65.6250%(21/32)\n",
            "Epoch [51] Batch[13560] - loss: 1.223805  acc: 56.2500%(18/32)\n",
            "Epoch [51] Batch[13580] - loss: 1.077854  acc: 56.2500%(18/32)\n",
            "Epoch [51] Batch[13600] - loss: 1.038972  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.294428  acc: 42.6885%(470/1101) \n",
            "\n",
            "save best_model.pt, metric: 42.688465031789285\n",
            "Epoch [52] Batch[13620] - loss: 0.802567  acc: 78.1250%(25/32)\n",
            "Epoch [52] Batch[13640] - loss: 1.167630  acc: 50.0000%(16/32)\n",
            "Epoch [52] Batch[13660] - loss: 1.227030  acc: 53.1250%(17/32)\n",
            "Epoch [52] Batch[13680] - loss: 0.975540  acc: 62.5000%(20/32)\n",
            "Epoch [52] Batch[13700] - loss: 1.170212  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.294913  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [52] Batch[13720] - loss: 0.989616  acc: 59.3750%(19/32)\n",
            "Epoch [52] Batch[13740] - loss: 1.126906  acc: 56.2500%(18/32)\n",
            "Epoch [52] Batch[13760] - loss: 1.041090  acc: 68.7500%(22/32)\n",
            "Epoch [52] Batch[13780] - loss: 1.045948  acc: 59.3750%(19/32)\n",
            "Epoch [52] Batch[13800] - loss: 1.238573  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.293410  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [52] Batch[13820] - loss: 1.185218  acc: 59.3750%(19/32)\n",
            "Epoch [52] Batch[13840] - loss: 0.998866  acc: 71.8750%(23/32)\n",
            "Epoch [52] Batch[13860] - loss: 0.922907  acc: 65.6250%(21/32)\n",
            "Epoch [52] Batch[13880] - loss: 1.097311  acc: 59.3750%(19/32)\n",
            "Epoch [53] Batch[13900] - loss: 1.131077  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.293153  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [53] Batch[13920] - loss: 1.109268  acc: 65.6250%(21/32)\n",
            "Epoch [53] Batch[13940] - loss: 1.010080  acc: 59.3750%(19/32)\n",
            "Epoch [53] Batch[13960] - loss: 1.052993  acc: 65.6250%(21/32)\n",
            "Epoch [53] Batch[13980] - loss: 0.952968  acc: 71.8750%(23/32)\n",
            "Epoch [53] Batch[14000] - loss: 0.953328  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.292354  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [53] Batch[14020] - loss: 1.043525  acc: 65.6250%(21/32)\n",
            "Epoch [53] Batch[14040] - loss: 0.900370  acc: 75.0000%(24/32)\n",
            "Epoch [53] Batch[14060] - loss: 1.073523  acc: 62.5000%(20/32)\n",
            "Epoch [53] Batch[14080] - loss: 0.925795  acc: 71.8750%(23/32)\n",
            "Epoch [53] Batch[14100] - loss: 0.995037  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.293524  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [53] Batch[14120] - loss: 1.035903  acc: 65.6250%(21/32)\n",
            "Epoch [53] Batch[14140] - loss: 1.134141  acc: 53.1250%(17/32)\n",
            "Epoch [54] Batch[14160] - loss: 1.083163  acc: 71.8750%(23/32)\n",
            "Epoch [54] Batch[14180] - loss: 0.932086  acc: 71.8750%(23/32)\n",
            "Epoch [54] Batch[14200] - loss: 1.127582  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.293068  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [54] Batch[14220] - loss: 1.099168  acc: 65.6250%(21/32)\n",
            "Epoch [54] Batch[14240] - loss: 1.191198  acc: 43.7500%(14/32)\n",
            "Epoch [54] Batch[14260] - loss: 1.103036  acc: 56.2500%(18/32)\n",
            "Epoch [54] Batch[14280] - loss: 1.179052  acc: 62.5000%(20/32)\n",
            "Epoch [54] Batch[14300] - loss: 1.101441  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.292105  acc: 42.5976%(469/1101) \n",
            "\n",
            "Epoch [54] Batch[14320] - loss: 1.097949  acc: 59.3750%(19/32)\n",
            "Epoch [54] Batch[14340] - loss: 0.952830  acc: 71.8750%(23/32)\n",
            "Epoch [54] Batch[14360] - loss: 0.965926  acc: 71.8750%(23/32)\n",
            "Epoch [54] Batch[14380] - loss: 1.116401  acc: 59.3750%(19/32)\n",
            "Epoch [54] Batch[14400] - loss: 0.999205  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.293804  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [55] Batch[14420] - loss: 1.068647  acc: 62.5000%(20/32)\n",
            "Epoch [55] Batch[14440] - loss: 1.017395  acc: 65.6250%(21/32)\n",
            "Epoch [55] Batch[14460] - loss: 0.910478  acc: 78.1250%(25/32)\n",
            "Epoch [55] Batch[14480] - loss: 1.014203  acc: 62.5000%(20/32)\n",
            "Epoch [55] Batch[14500] - loss: 1.182348  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.291977  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [55] Batch[14520] - loss: 1.004029  acc: 71.8750%(23/32)\n",
            "Epoch [55] Batch[14540] - loss: 1.014637  acc: 71.8750%(23/32)\n",
            "Epoch [55] Batch[14560] - loss: 1.171407  acc: 56.2500%(18/32)\n",
            "Epoch [55] Batch[14580] - loss: 1.069041  acc: 65.6250%(21/32)\n",
            "Epoch [55] Batch[14600] - loss: 0.942145  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.290270  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [55] Batch[14620] - loss: 1.056438  acc: 62.5000%(20/32)\n",
            "Epoch [55] Batch[14640] - loss: 1.117381  acc: 78.1250%(25/32)\n",
            "Epoch [55] Batch[14660] - loss: 0.992043  acc: 71.8750%(23/32)\n",
            "Epoch [55] Batch[14680] - loss: 0.895625  acc: 71.8750%(23/32)\n",
            "Epoch [56] Batch[14700] - loss: 0.896555  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.292513  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [56] Batch[14720] - loss: 0.979816  acc: 68.7500%(22/32)\n",
            "Epoch [56] Batch[14740] - loss: 0.997242  acc: 65.6250%(21/32)\n",
            "Epoch [56] Batch[14760] - loss: 1.238450  acc: 50.0000%(16/32)\n",
            "Epoch [56] Batch[14780] - loss: 1.013533  acc: 68.7500%(22/32)\n",
            "Epoch [56] Batch[14800] - loss: 1.047802  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.291898  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [56] Batch[14820] - loss: 0.984271  acc: 71.8750%(23/32)\n",
            "Epoch [56] Batch[14840] - loss: 1.016400  acc: 65.6250%(21/32)\n",
            "Epoch [56] Batch[14860] - loss: 0.949442  acc: 68.7500%(22/32)\n",
            "Epoch [56] Batch[14880] - loss: 0.939120  acc: 62.5000%(20/32)\n",
            "Epoch [56] Batch[14900] - loss: 1.123128  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.289188  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [56] Batch[14920] - loss: 0.944952  acc: 68.7500%(22/32)\n",
            "Epoch [56] Batch[14940] - loss: 1.039316  acc: 65.6250%(21/32)\n",
            "Epoch [57] Batch[14960] - loss: 1.161210  acc: 50.0000%(16/32)\n",
            "Epoch [57] Batch[14980] - loss: 1.143580  acc: 62.5000%(20/32)\n",
            "Epoch [57] Batch[15000] - loss: 1.010413  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.289510  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [57] Batch[15020] - loss: 0.967352  acc: 68.7500%(22/32)\n",
            "Epoch [57] Batch[15040] - loss: 0.954542  acc: 62.5000%(20/32)\n",
            "Epoch [57] Batch[15060] - loss: 1.012993  acc: 62.5000%(20/32)\n",
            "Epoch [57] Batch[15080] - loss: 0.950822  acc: 68.7500%(22/32)\n",
            "Epoch [57] Batch[15100] - loss: 0.954295  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.288534  acc: 42.7793%(471/1101) \n",
            "\n",
            "save best_model.pt, metric: 42.77929155313352\n",
            "Epoch [57] Batch[15120] - loss: 0.795916  acc: 78.1250%(25/32)\n",
            "Epoch [57] Batch[15140] - loss: 0.983502  acc: 62.5000%(20/32)\n",
            "Epoch [57] Batch[15160] - loss: 1.128321  acc: 62.5000%(20/32)\n",
            "Epoch [57] Batch[15180] - loss: 1.069431  acc: 62.5000%(20/32)\n",
            "Epoch [57] Batch[15200] - loss: 1.066079  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.288690  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [58] Batch[15220] - loss: 1.104151  acc: 53.1250%(17/32)\n",
            "Epoch [58] Batch[15240] - loss: 1.029316  acc: 65.6250%(21/32)\n",
            "Epoch [58] Batch[15260] - loss: 1.025119  acc: 62.5000%(20/32)\n",
            "Epoch [58] Batch[15280] - loss: 1.063196  acc: 62.5000%(20/32)\n",
            "Epoch [58] Batch[15300] - loss: 1.058160  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.290004  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [58] Batch[15320] - loss: 1.128142  acc: 43.7500%(14/32)\n",
            "Epoch [58] Batch[15340] - loss: 1.003739  acc: 68.7500%(22/32)\n",
            "Epoch [58] Batch[15360] - loss: 1.100228  acc: 56.2500%(18/32)\n",
            "Epoch [58] Batch[15380] - loss: 0.982862  acc: 65.6250%(21/32)\n",
            "Epoch [58] Batch[15400] - loss: 0.909855  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.289569  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [58] Batch[15420] - loss: 1.005641  acc: 65.6250%(21/32)\n",
            "Epoch [58] Batch[15440] - loss: 1.002501  acc: 65.6250%(21/32)\n",
            "Epoch [58] Batch[15460] - loss: 1.019901  acc: 56.2500%(18/32)\n",
            "Epoch [58] Batch[15480] - loss: 0.992619  acc: 68.7500%(22/32)\n",
            "Epoch [59] Batch[15500] - loss: 1.082773  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.286485  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [59] Batch[15520] - loss: 0.984105  acc: 62.5000%(20/32)\n",
            "Epoch [59] Batch[15540] - loss: 0.953705  acc: 65.6250%(21/32)\n",
            "Epoch [59] Batch[15560] - loss: 0.985363  acc: 59.3750%(19/32)\n",
            "Epoch [59] Batch[15580] - loss: 1.086370  acc: 62.5000%(20/32)\n",
            "Epoch [59] Batch[15600] - loss: 1.060768  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.288611  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [59] Batch[15620] - loss: 1.018253  acc: 59.3750%(19/32)\n",
            "Epoch [59] Batch[15640] - loss: 1.208287  acc: 53.1250%(17/32)\n",
            "Epoch [59] Batch[15660] - loss: 1.072824  acc: 59.3750%(19/32)\n",
            "Epoch [59] Batch[15680] - loss: 1.001845  acc: 68.7500%(22/32)\n",
            "Epoch [59] Batch[15700] - loss: 0.956970  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.286501  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [59] Batch[15720] - loss: 0.939112  acc: 68.7500%(22/32)\n",
            "Epoch [59] Batch[15740] - loss: 1.075424  acc: 62.5000%(20/32)\n",
            "Epoch [60] Batch[15760] - loss: 1.017998  acc: 62.5000%(20/32)\n",
            "Epoch [60] Batch[15780] - loss: 0.954247  acc: 65.6250%(21/32)\n",
            "Epoch [60] Batch[15800] - loss: 1.007600  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.286342  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [60] Batch[15820] - loss: 0.877849  acc: 78.1250%(25/32)\n",
            "Epoch [60] Batch[15840] - loss: 1.075863  acc: 43.7500%(14/32)\n",
            "Epoch [60] Batch[15860] - loss: 1.042449  acc: 65.6250%(21/32)\n",
            "Epoch [60] Batch[15880] - loss: 1.095735  acc: 62.5000%(20/32)\n",
            "Epoch [60] Batch[15900] - loss: 1.076909  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.287517  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [60] Batch[15920] - loss: 0.927835  acc: 65.6250%(21/32)\n",
            "Epoch [60] Batch[15940] - loss: 0.976862  acc: 65.6250%(21/32)\n",
            "Epoch [60] Batch[15960] - loss: 1.008405  acc: 65.6250%(21/32)\n",
            "Epoch [60] Batch[15980] - loss: 0.976246  acc: 65.6250%(21/32)\n",
            "Epoch [60] Batch[16000] - loss: 1.180011  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.286977  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [60] Batch[16020] - loss: 1.031460  acc: 68.7500%(22/32)\n",
            "Epoch [61] Batch[16040] - loss: 0.996031  acc: 75.0000%(24/32)\n",
            "Epoch [61] Batch[16060] - loss: 0.824090  acc: 68.7500%(22/32)\n",
            "Epoch [61] Batch[16080] - loss: 0.938470  acc: 68.7500%(22/32)\n",
            "Epoch [61] Batch[16100] - loss: 0.981482  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.287075  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [61] Batch[16120] - loss: 0.930468  acc: 68.7500%(22/32)\n",
            "Epoch [61] Batch[16140] - loss: 1.125811  acc: 59.3750%(19/32)\n",
            "Epoch [61] Batch[16160] - loss: 1.133084  acc: 59.3750%(19/32)\n",
            "Epoch [61] Batch[16180] - loss: 1.075443  acc: 56.2500%(18/32)\n",
            "Epoch [61] Batch[16200] - loss: 1.155269  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.287067  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [61] Batch[16220] - loss: 0.974417  acc: 62.5000%(20/32)\n",
            "Epoch [61] Batch[16240] - loss: 1.052742  acc: 53.1250%(17/32)\n",
            "Epoch [61] Batch[16260] - loss: 1.094801  acc: 56.2500%(18/32)\n",
            "Epoch [61] Batch[16280] - loss: 0.820551  acc: 78.1250%(25/32)\n",
            "Epoch [62] Batch[16300] - loss: 0.901062  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.286365  acc: 42.7793%(471/1101) \n",
            "\n",
            "Epoch [62] Batch[16320] - loss: 0.906550  acc: 75.0000%(24/32)\n",
            "Epoch [62] Batch[16340] - loss: 0.918932  acc: 78.1250%(25/32)\n",
            "Epoch [62] Batch[16360] - loss: 1.051261  acc: 56.2500%(18/32)\n",
            "Epoch [62] Batch[16380] - loss: 0.998856  acc: 53.1250%(17/32)\n",
            "Epoch [62] Batch[16400] - loss: 1.050282  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.286806  acc: 42.5976%(469/1101) \n",
            "\n",
            "Epoch [62] Batch[16420] - loss: 1.075868  acc: 56.2500%(18/32)\n",
            "Epoch [62] Batch[16440] - loss: 1.037042  acc: 65.6250%(21/32)\n",
            "Epoch [62] Batch[16460] - loss: 1.045768  acc: 71.8750%(23/32)\n",
            "Epoch [62] Batch[16480] - loss: 1.015192  acc: 65.6250%(21/32)\n",
            "Epoch [62] Batch[16500] - loss: 0.954511  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.285366  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [62] Batch[16520] - loss: 1.077981  acc: 59.3750%(19/32)\n",
            "Epoch [62] Batch[16540] - loss: 1.074428  acc: 50.0000%(16/32)\n",
            "Epoch [63] Batch[16560] - loss: 1.087507  acc: 56.2500%(18/32)\n",
            "Epoch [63] Batch[16580] - loss: 1.111395  acc: 46.8750%(15/32)\n",
            "Epoch [63] Batch[16600] - loss: 0.945093  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.284445  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [63] Batch[16620] - loss: 0.863515  acc: 65.6250%(21/32)\n",
            "Epoch [63] Batch[16640] - loss: 1.034412  acc: 59.3750%(19/32)\n",
            "Epoch [63] Batch[16660] - loss: 0.990217  acc: 62.5000%(20/32)\n",
            "Epoch [63] Batch[16680] - loss: 0.966425  acc: 65.6250%(21/32)\n",
            "Epoch [63] Batch[16700] - loss: 1.015400  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.284960  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [63] Batch[16720] - loss: 1.063002  acc: 65.6250%(21/32)\n",
            "Epoch [63] Batch[16740] - loss: 1.067504  acc: 56.2500%(18/32)\n",
            "Epoch [63] Batch[16760] - loss: 1.034443  acc: 62.5000%(20/32)\n",
            "Epoch [63] Batch[16780] - loss: 1.065686  acc: 62.5000%(20/32)\n",
            "Epoch [63] Batch[16800] - loss: 0.873598  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.286708  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [63] Batch[16820] - loss: 0.979325  acc: 68.7500%(22/32)\n",
            "Epoch [64] Batch[16840] - loss: 1.039660  acc: 56.2500%(18/32)\n",
            "Epoch [64] Batch[16860] - loss: 0.959498  acc: 75.0000%(24/32)\n",
            "Epoch [64] Batch[16880] - loss: 0.969537  acc: 62.5000%(20/32)\n",
            "Epoch [64] Batch[16900] - loss: 0.832693  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.285524  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [64] Batch[16920] - loss: 0.848762  acc: 81.2500%(26/32)\n",
            "Epoch [64] Batch[16940] - loss: 0.972949  acc: 59.3750%(19/32)\n",
            "Epoch [64] Batch[16960] - loss: 1.044873  acc: 50.0000%(16/32)\n",
            "Epoch [64] Batch[16980] - loss: 0.835263  acc: 71.8750%(23/32)\n",
            "Epoch [64] Batch[17000] - loss: 0.924289  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.284638  acc: 42.5976%(469/1101) \n",
            "\n",
            "Epoch [64] Batch[17020] - loss: 1.045691  acc: 65.6250%(21/32)\n",
            "Epoch [64] Batch[17040] - loss: 0.928417  acc: 81.2500%(26/32)\n",
            "Epoch [64] Batch[17060] - loss: 1.006309  acc: 59.3750%(19/32)\n",
            "Epoch [64] Batch[17080] - loss: 0.981780  acc: 68.7500%(22/32)\n",
            "Epoch [65] Batch[17100] - loss: 0.973605  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.284249  acc: 42.7793%(471/1101) \n",
            "\n",
            "Epoch [65] Batch[17120] - loss: 1.014323  acc: 68.7500%(22/32)\n",
            "Epoch [65] Batch[17140] - loss: 1.035257  acc: 62.5000%(20/32)\n",
            "Epoch [65] Batch[17160] - loss: 1.054649  acc: 68.7500%(22/32)\n",
            "Epoch [65] Batch[17180] - loss: 1.052099  acc: 53.1250%(17/32)\n",
            "Epoch [65] Batch[17200] - loss: 0.929045  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.283938  acc: 42.7793%(471/1101) \n",
            "\n",
            "Epoch [65] Batch[17220] - loss: 1.117405  acc: 46.8750%(15/32)\n",
            "Epoch [65] Batch[17240] - loss: 1.074255  acc: 59.3750%(19/32)\n",
            "Epoch [65] Batch[17260] - loss: 0.990774  acc: 68.7500%(22/32)\n",
            "Epoch [65] Batch[17280] - loss: 0.863792  acc: 75.0000%(24/32)\n",
            "Epoch [65] Batch[17300] - loss: 1.029046  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.285388  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [65] Batch[17320] - loss: 0.972827  acc: 78.1250%(25/32)\n",
            "Epoch [65] Batch[17340] - loss: 1.061125  acc: 59.3750%(19/32)\n",
            "Epoch [66] Batch[17360] - loss: 0.956030  acc: 62.5000%(20/32)\n",
            "Epoch [66] Batch[17380] - loss: 0.809763  acc: 75.0000%(24/32)\n",
            "Epoch [66] Batch[17400] - loss: 1.064150  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.283791  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [66] Batch[17420] - loss: 1.032557  acc: 65.6250%(21/32)\n",
            "Epoch [66] Batch[17440] - loss: 0.884620  acc: 68.7500%(22/32)\n",
            "Epoch [66] Batch[17460] - loss: 0.908512  acc: 65.6250%(21/32)\n",
            "Epoch [66] Batch[17480] - loss: 1.204186  acc: 46.8750%(15/32)\n",
            "Epoch [66] Batch[17500] - loss: 0.961870  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.284248  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [66] Batch[17520] - loss: 0.916996  acc: 71.8750%(23/32)\n",
            "Epoch [66] Batch[17540] - loss: 1.016753  acc: 53.1250%(17/32)\n",
            "Epoch [66] Batch[17560] - loss: 0.920582  acc: 81.2500%(26/32)\n",
            "Epoch [66] Batch[17580] - loss: 1.188970  acc: 56.2500%(18/32)\n",
            "Epoch [66] Batch[17600] - loss: 1.177792  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.285496  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [66] Batch[17620] - loss: 1.159178  acc: 50.0000%(16/32)\n",
            "Epoch [67] Batch[17640] - loss: 0.936587  acc: 65.6250%(21/32)\n",
            "Epoch [67] Batch[17660] - loss: 0.949587  acc: 75.0000%(24/32)\n",
            "Epoch [67] Batch[17680] - loss: 1.195981  acc: 53.1250%(17/32)\n",
            "Epoch [67] Batch[17700] - loss: 0.895704  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.283842  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [67] Batch[17720] - loss: 1.043058  acc: 62.5000%(20/32)\n",
            "Epoch [67] Batch[17740] - loss: 0.977954  acc: 71.8750%(23/32)\n",
            "Epoch [67] Batch[17760] - loss: 1.001590  acc: 71.8750%(23/32)\n",
            "Epoch [67] Batch[17780] - loss: 0.882056  acc: 68.7500%(22/32)\n",
            "Epoch [67] Batch[17800] - loss: 1.036952  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.283513  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [67] Batch[17820] - loss: 0.937162  acc: 65.6250%(21/32)\n",
            "Epoch [67] Batch[17840] - loss: 0.894233  acc: 78.1250%(25/32)\n",
            "Epoch [67] Batch[17860] - loss: 0.887937  acc: 68.7500%(22/32)\n",
            "Epoch [67] Batch[17880] - loss: 1.010156  acc: 62.5000%(20/32)\n",
            "Epoch [68] Batch[17900] - loss: 0.992504  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.281178  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [68] Batch[17920] - loss: 1.017807  acc: 65.6250%(21/32)\n",
            "Epoch [68] Batch[17940] - loss: 1.083676  acc: 62.5000%(20/32)\n",
            "Epoch [68] Batch[17960] - loss: 1.022745  acc: 62.5000%(20/32)\n",
            "Epoch [68] Batch[17980] - loss: 0.928611  acc: 62.5000%(20/32)\n",
            "Epoch [68] Batch[18000] - loss: 1.030599  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.282665  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [68] Batch[18020] - loss: 1.108586  acc: 56.2500%(18/32)\n",
            "Epoch [68] Batch[18040] - loss: 0.836739  acc: 78.1250%(25/32)\n",
            "Epoch [68] Batch[18060] - loss: 1.022504  acc: 59.3750%(19/32)\n",
            "Epoch [68] Batch[18080] - loss: 0.846892  acc: 68.7500%(22/32)\n",
            "Epoch [68] Batch[18100] - loss: 0.829158  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.282784  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [68] Batch[18120] - loss: 1.048256  acc: 59.3750%(19/32)\n",
            "Epoch [68] Batch[18140] - loss: 0.823373  acc: 75.0000%(24/32)\n",
            "Epoch [69] Batch[18160] - loss: 0.914869  acc: 68.7500%(22/32)\n",
            "Epoch [69] Batch[18180] - loss: 1.025791  acc: 59.3750%(19/32)\n",
            "Epoch [69] Batch[18200] - loss: 0.978795  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.281791  acc: 42.6885%(470/1101) \n",
            "\n",
            "Epoch [69] Batch[18220] - loss: 0.842382  acc: 68.7500%(22/32)\n",
            "Epoch [69] Batch[18240] - loss: 1.010652  acc: 56.2500%(18/32)\n",
            "Epoch [69] Batch[18260] - loss: 0.964762  acc: 59.3750%(19/32)\n",
            "Epoch [69] Batch[18280] - loss: 1.038046  acc: 65.6250%(21/32)\n",
            "Epoch [69] Batch[18300] - loss: 0.889766  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.282418  acc: 42.7793%(471/1101) \n",
            "\n",
            "Epoch [69] Batch[18320] - loss: 1.059283  acc: 50.0000%(16/32)\n",
            "Epoch [69] Batch[18340] - loss: 0.972728  acc: 65.6250%(21/32)\n",
            "Epoch [69] Batch[18360] - loss: 0.903529  acc: 68.7500%(22/32)\n",
            "Epoch [69] Batch[18380] - loss: 0.844779  acc: 71.8750%(23/32)\n",
            "Epoch [69] Batch[18400] - loss: 0.893786  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.283388  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [69] Batch[18420] - loss: 0.895965  acc: 65.6250%(21/32)\n",
            "Epoch [70] Batch[18440] - loss: 0.867701  acc: 75.0000%(24/32)\n",
            "Epoch [70] Batch[18460] - loss: 1.134613  acc: 62.5000%(20/32)\n",
            "Epoch [70] Batch[18480] - loss: 0.843835  acc: 81.2500%(26/32)\n",
            "Epoch [70] Batch[18500] - loss: 1.118219  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.283361  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [70] Batch[18520] - loss: 1.004871  acc: 68.7500%(22/32)\n",
            "Epoch [70] Batch[18540] - loss: 0.852032  acc: 75.0000%(24/32)\n",
            "Epoch [70] Batch[18560] - loss: 1.023288  acc: 56.2500%(18/32)\n",
            "Epoch [70] Batch[18580] - loss: 0.983400  acc: 65.6250%(21/32)\n",
            "Epoch [70] Batch[18600] - loss: 0.841890  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.282333  acc: 42.7793%(471/1101) \n",
            "\n",
            "Epoch [70] Batch[18620] - loss: 0.945566  acc: 78.1250%(25/32)\n",
            "Epoch [70] Batch[18640] - loss: 0.891499  acc: 68.7500%(22/32)\n",
            "Epoch [70] Batch[18660] - loss: 0.847678  acc: 75.0000%(24/32)\n",
            "Epoch [70] Batch[18680] - loss: 1.082850  acc: 68.7500%(22/32)\n",
            "Epoch [71] Batch[18700] - loss: 0.857501  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.282032  acc: 42.7793%(471/1101) \n",
            "\n",
            "Epoch [71] Batch[18720] - loss: 0.850544  acc: 81.2500%(26/32)\n",
            "Epoch [71] Batch[18740] - loss: 0.979751  acc: 50.0000%(16/32)\n",
            "Epoch [71] Batch[18760] - loss: 0.970238  acc: 65.6250%(21/32)\n",
            "Epoch [71] Batch[18780] - loss: 0.975173  acc: 62.5000%(20/32)\n",
            "Epoch [71] Batch[18800] - loss: 0.902423  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.282994  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [71] Batch[18820] - loss: 1.023575  acc: 62.5000%(20/32)\n",
            "Epoch [71] Batch[18840] - loss: 0.848450  acc: 65.6250%(21/32)\n",
            "Epoch [71] Batch[18860] - loss: 0.900964  acc: 68.7500%(22/32)\n",
            "Epoch [71] Batch[18880] - loss: 0.906752  acc: 65.6250%(21/32)\n",
            "Epoch [71] Batch[18900] - loss: 0.889807  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.279996  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [71] Batch[18920] - loss: 1.013453  acc: 68.7500%(22/32)\n",
            "Epoch [71] Batch[18940] - loss: 0.826009  acc: 71.8750%(23/32)\n",
            "Epoch [72] Batch[18960] - loss: 0.757346  acc: 78.1250%(25/32)\n",
            "Epoch [72] Batch[18980] - loss: 1.080071  acc: 56.2500%(18/32)\n",
            "Epoch [72] Batch[19000] - loss: 0.924559  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.282793  acc: 42.7793%(471/1101) \n",
            "\n",
            "Epoch [72] Batch[19020] - loss: 1.067176  acc: 62.5000%(20/32)\n",
            "Epoch [72] Batch[19040] - loss: 0.878995  acc: 75.0000%(24/32)\n",
            "Epoch [72] Batch[19060] - loss: 1.096013  acc: 50.0000%(16/32)\n",
            "Epoch [72] Batch[19080] - loss: 0.990159  acc: 68.7500%(22/32)\n",
            "Epoch [72] Batch[19100] - loss: 0.871547  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.282691  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [72] Batch[19120] - loss: 0.787162  acc: 81.2500%(26/32)\n",
            "Epoch [72] Batch[19140] - loss: 1.019364  acc: 68.7500%(22/32)\n",
            "Epoch [72] Batch[19160] - loss: 0.970309  acc: 65.6250%(21/32)\n",
            "Epoch [72] Batch[19180] - loss: 1.038719  acc: 68.7500%(22/32)\n",
            "Epoch [72] Batch[19200] - loss: 0.939008  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.282248  acc: 42.6885%(470/1101) \n",
            "\n",
            "Epoch [72] Batch[19220] - loss: 0.833082  acc: 71.8750%(23/32)\n",
            "Epoch [73] Batch[19240] - loss: 0.933298  acc: 65.6250%(21/32)\n",
            "Epoch [73] Batch[19260] - loss: 0.947757  acc: 59.3750%(19/32)\n",
            "Epoch [73] Batch[19280] - loss: 0.927672  acc: 65.6250%(21/32)\n",
            "Epoch [73] Batch[19300] - loss: 0.820506  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.281030  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [73] Batch[19320] - loss: 0.907912  acc: 75.0000%(24/32)\n",
            "Epoch [73] Batch[19340] - loss: 0.808654  acc: 75.0000%(24/32)\n",
            "Epoch [73] Batch[19360] - loss: 0.873357  acc: 75.0000%(24/32)\n",
            "Epoch [73] Batch[19380] - loss: 1.111260  acc: 62.5000%(20/32)\n",
            "Epoch [73] Batch[19400] - loss: 0.842520  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.282155  acc: 42.5976%(469/1101) \n",
            "\n",
            "Epoch [73] Batch[19420] - loss: 0.959308  acc: 68.7500%(22/32)\n",
            "Epoch [73] Batch[19440] - loss: 0.947990  acc: 71.8750%(23/32)\n",
            "Epoch [73] Batch[19460] - loss: 0.945207  acc: 62.5000%(20/32)\n",
            "Epoch [73] Batch[19480] - loss: 1.045443  acc: 65.6250%(21/32)\n",
            "Epoch [74] Batch[19500] - loss: 0.979813  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.283870  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [74] Batch[19520] - loss: 1.084467  acc: 62.5000%(20/32)\n",
            "Epoch [74] Batch[19540] - loss: 0.815325  acc: 84.3750%(27/32)\n",
            "Epoch [74] Batch[19560] - loss: 0.950966  acc: 71.8750%(23/32)\n",
            "Epoch [74] Batch[19580] - loss: 0.809126  acc: 75.0000%(24/32)\n",
            "Epoch [74] Batch[19600] - loss: 0.858743  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.284055  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [74] Batch[19620] - loss: 0.982663  acc: 62.5000%(20/32)\n",
            "Epoch [74] Batch[19640] - loss: 1.062608  acc: 53.1250%(17/32)\n",
            "Epoch [74] Batch[19660] - loss: 0.873737  acc: 71.8750%(23/32)\n",
            "Epoch [74] Batch[19680] - loss: 0.943043  acc: 65.6250%(21/32)\n",
            "Epoch [74] Batch[19700] - loss: 0.889881  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.281416  acc: 42.7793%(471/1101) \n",
            "\n",
            "Epoch [74] Batch[19720] - loss: 1.014327  acc: 71.8750%(23/32)\n",
            "Epoch [74] Batch[19740] - loss: 0.917774  acc: 65.6250%(21/32)\n",
            "Epoch [75] Batch[19760] - loss: 0.732299  acc: 87.5000%(28/32)\n",
            "Epoch [75] Batch[19780] - loss: 0.803005  acc: 71.8750%(23/32)\n",
            "Epoch [75] Batch[19800] - loss: 0.904450  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.281008  acc: 42.6885%(470/1101) \n",
            "\n",
            "Epoch [75] Batch[19820] - loss: 0.927997  acc: 68.7500%(22/32)\n",
            "Epoch [75] Batch[19840] - loss: 0.795201  acc: 78.1250%(25/32)\n",
            "Epoch [75] Batch[19860] - loss: 0.870408  acc: 65.6250%(21/32)\n",
            "Epoch [75] Batch[19880] - loss: 0.914882  acc: 65.6250%(21/32)\n",
            "Epoch [75] Batch[19900] - loss: 0.913877  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.282978  acc: 42.6885%(470/1101) \n",
            "\n",
            "Epoch [75] Batch[19920] - loss: 0.936347  acc: 71.8750%(23/32)\n",
            "Epoch [75] Batch[19940] - loss: 0.837748  acc: 71.8750%(23/32)\n",
            "Epoch [75] Batch[19960] - loss: 1.037118  acc: 65.6250%(21/32)\n",
            "Epoch [75] Batch[19980] - loss: 1.118300  acc: 59.3750%(19/32)\n",
            "Epoch [75] Batch[20000] - loss: 0.901979  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.280082  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [75] Batch[20020] - loss: 1.048845  acc: 62.5000%(20/32)\n",
            "Epoch [76] Batch[20040] - loss: 1.046751  acc: 56.2500%(18/32)\n",
            "Epoch [76] Batch[20060] - loss: 0.888763  acc: 75.0000%(24/32)\n",
            "Epoch [76] Batch[20080] - loss: 0.795815  acc: 71.8750%(23/32)\n",
            "Epoch [76] Batch[20100] - loss: 0.973168  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.280600  acc: 43.1426%(475/1101) \n",
            "\n",
            "save best_model.pt, metric: 43.142597638510445\n",
            "Epoch [76] Batch[20120] - loss: 0.922308  acc: 75.0000%(24/32)\n",
            "Epoch [76] Batch[20140] - loss: 0.945215  acc: 68.7500%(22/32)\n",
            "Epoch [76] Batch[20160] - loss: 0.899063  acc: 68.7500%(22/32)\n",
            "Epoch [76] Batch[20180] - loss: 0.962736  acc: 62.5000%(20/32)\n",
            "Epoch [76] Batch[20200] - loss: 0.923353  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.281242  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [76] Batch[20220] - loss: 0.971246  acc: 62.5000%(20/32)\n",
            "Epoch [76] Batch[20240] - loss: 1.000172  acc: 71.8750%(23/32)\n",
            "Epoch [76] Batch[20260] - loss: 0.904900  acc: 68.7500%(22/32)\n",
            "Epoch [76] Batch[20280] - loss: 0.872472  acc: 65.6250%(21/32)\n",
            "Epoch [77] Batch[20300] - loss: 0.927084  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.279938  acc: 42.6885%(470/1101) \n",
            "\n",
            "Epoch [77] Batch[20320] - loss: 0.772894  acc: 78.1250%(25/32)\n",
            "Epoch [77] Batch[20340] - loss: 0.950845  acc: 62.5000%(20/32)\n",
            "Epoch [77] Batch[20360] - loss: 0.861686  acc: 78.1250%(25/32)\n",
            "Epoch [77] Batch[20380] - loss: 0.731621  acc: 81.2500%(26/32)\n",
            "Epoch [77] Batch[20400] - loss: 0.926915  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.283692  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [77] Batch[20420] - loss: 0.791310  acc: 78.1250%(25/32)\n",
            "Epoch [77] Batch[20440] - loss: 0.928368  acc: 68.7500%(22/32)\n",
            "Epoch [77] Batch[20460] - loss: 0.888721  acc: 81.2500%(26/32)\n",
            "Epoch [77] Batch[20480] - loss: 1.135137  acc: 53.1250%(17/32)\n",
            "Epoch [77] Batch[20500] - loss: 0.841603  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.281051  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [77] Batch[20520] - loss: 0.835467  acc: 68.7500%(22/32)\n",
            "Epoch [77] Batch[20540] - loss: 0.915756  acc: 65.6250%(21/32)\n",
            "Epoch [78] Batch[20560] - loss: 0.891491  acc: 62.5000%(20/32)\n",
            "Epoch [78] Batch[20580] - loss: 0.941888  acc: 68.7500%(22/32)\n",
            "Epoch [78] Batch[20600] - loss: 0.933845  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.280101  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [78] Batch[20620] - loss: 0.851943  acc: 71.8750%(23/32)\n",
            "Epoch [78] Batch[20640] - loss: 0.773073  acc: 81.2500%(26/32)\n",
            "Epoch [78] Batch[20660] - loss: 0.930290  acc: 59.3750%(19/32)\n",
            "Epoch [78] Batch[20680] - loss: 1.012917  acc: 68.7500%(22/32)\n",
            "Epoch [78] Batch[20700] - loss: 0.865901  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.280013  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [78] Batch[20720] - loss: 0.951434  acc: 75.0000%(24/32)\n",
            "Epoch [78] Batch[20740] - loss: 0.901007  acc: 62.5000%(20/32)\n",
            "Epoch [78] Batch[20760] - loss: 0.945504  acc: 65.6250%(21/32)\n",
            "Epoch [78] Batch[20780] - loss: 1.004284  acc: 71.8750%(23/32)\n",
            "Epoch [78] Batch[20800] - loss: 0.913738  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.281435  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [78] Batch[20820] - loss: 0.896141  acc: 71.8750%(23/32)\n",
            "Epoch [79] Batch[20840] - loss: 0.863665  acc: 68.7500%(22/32)\n",
            "Epoch [79] Batch[20860] - loss: 0.738012  acc: 81.2500%(26/32)\n",
            "Epoch [79] Batch[20880] - loss: 0.883705  acc: 68.7500%(22/32)\n",
            "Epoch [79] Batch[20900] - loss: 0.911672  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.279400  acc: 42.5976%(469/1101) \n",
            "\n",
            "Epoch [79] Batch[20920] - loss: 0.848489  acc: 75.0000%(24/32)\n",
            "Epoch [79] Batch[20940] - loss: 0.768484  acc: 84.3750%(27/32)\n",
            "Epoch [79] Batch[20960] - loss: 0.953011  acc: 65.6250%(21/32)\n",
            "Epoch [79] Batch[20980] - loss: 0.845943  acc: 78.1250%(25/32)\n",
            "Epoch [79] Batch[21000] - loss: 0.862315  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.282059  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [79] Batch[21020] - loss: 0.843585  acc: 84.3750%(27/32)\n",
            "Epoch [79] Batch[21040] - loss: 0.918419  acc: 68.7500%(22/32)\n",
            "Epoch [79] Batch[21060] - loss: 0.868113  acc: 56.2500%(18/32)\n",
            "Epoch [79] Batch[21080] - loss: 0.923646  acc: 84.3750%(27/32)\n",
            "Epoch [80] Batch[21100] - loss: 0.888908  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.281206  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [80] Batch[21120] - loss: 0.860593  acc: 81.2500%(26/32)\n",
            "Epoch [80] Batch[21140] - loss: 0.790758  acc: 81.2500%(26/32)\n",
            "Epoch [80] Batch[21160] - loss: 0.984822  acc: 65.6250%(21/32)\n",
            "Epoch [80] Batch[21180] - loss: 0.983947  acc: 65.6250%(21/32)\n",
            "Epoch [80] Batch[21200] - loss: 0.832617  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.280193  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [80] Batch[21220] - loss: 0.792504  acc: 62.5000%(20/32)\n",
            "Epoch [80] Batch[21240] - loss: 0.937077  acc: 59.3750%(19/32)\n",
            "Epoch [80] Batch[21260] - loss: 0.825410  acc: 78.1250%(25/32)\n",
            "Epoch [80] Batch[21280] - loss: 0.842978  acc: 78.1250%(25/32)\n",
            "Epoch [80] Batch[21300] - loss: 0.853896  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.280879  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [80] Batch[21320] - loss: 0.832611  acc: 62.5000%(20/32)\n",
            "Epoch [80] Batch[21340] - loss: 0.940540  acc: 65.6250%(21/32)\n",
            "Epoch [80] Batch[21360] - loss: 0.766172  acc: 78.1250%(25/32)\n",
            "Epoch [81] Batch[21380] - loss: 0.849909  acc: 75.0000%(24/32)\n",
            "Epoch [81] Batch[21400] - loss: 0.947316  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.280562  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [81] Batch[21420] - loss: 0.644198  acc: 90.6250%(29/32)\n",
            "Epoch [81] Batch[21440] - loss: 1.001720  acc: 59.3750%(19/32)\n",
            "Epoch [81] Batch[21460] - loss: 0.956478  acc: 59.3750%(19/32)\n",
            "Epoch [81] Batch[21480] - loss: 0.948033  acc: 59.3750%(19/32)\n",
            "Epoch [81] Batch[21500] - loss: 0.890876  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.278762  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [81] Batch[21520] - loss: 0.915493  acc: 81.2500%(26/32)\n",
            "Epoch [81] Batch[21540] - loss: 0.894154  acc: 78.1250%(25/32)\n",
            "Epoch [81] Batch[21560] - loss: 0.881456  acc: 68.7500%(22/32)\n",
            "Epoch [81] Batch[21580] - loss: 0.820333  acc: 68.7500%(22/32)\n",
            "Epoch [81] Batch[21600] - loss: 0.822386  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.281381  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [81] Batch[21620] - loss: 0.794161  acc: 71.8750%(23/32)\n",
            "Epoch [82] Batch[21640] - loss: 0.905384  acc: 71.8750%(23/32)\n",
            "Epoch [82] Batch[21660] - loss: 0.890436  acc: 65.6250%(21/32)\n",
            "Epoch [82] Batch[21680] - loss: 0.985790  acc: 56.2500%(18/32)\n",
            "Epoch [82] Batch[21700] - loss: 0.874805  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.280546  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [82] Batch[21720] - loss: 1.110143  acc: 65.6250%(21/32)\n",
            "Epoch [82] Batch[21740] - loss: 1.098846  acc: 56.2500%(18/32)\n",
            "Epoch [82] Batch[21760] - loss: 0.841769  acc: 65.6250%(21/32)\n",
            "Epoch [82] Batch[21780] - loss: 0.759902  acc: 78.1250%(25/32)\n",
            "Epoch [82] Batch[21800] - loss: 1.004027  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.280399  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [82] Batch[21820] - loss: 0.853901  acc: 71.8750%(23/32)\n",
            "Epoch [82] Batch[21840] - loss: 0.769590  acc: 78.1250%(25/32)\n",
            "Epoch [82] Batch[21860] - loss: 0.818198  acc: 81.2500%(26/32)\n",
            "Epoch [82] Batch[21880] - loss: 0.912973  acc: 59.3750%(19/32)\n",
            "Epoch [83] Batch[21900] - loss: 0.772843  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.279843  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [83] Batch[21920] - loss: 0.983213  acc: 71.8750%(23/32)\n",
            "Epoch [83] Batch[21940] - loss: 0.933969  acc: 75.0000%(24/32)\n",
            "Epoch [83] Batch[21960] - loss: 0.898169  acc: 75.0000%(24/32)\n",
            "Epoch [83] Batch[21980] - loss: 0.975737  acc: 65.6250%(21/32)\n",
            "Epoch [83] Batch[22000] - loss: 0.935286  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.280777  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [83] Batch[22020] - loss: 0.961776  acc: 65.6250%(21/32)\n",
            "Epoch [83] Batch[22040] - loss: 1.122702  acc: 50.0000%(16/32)\n",
            "Epoch [83] Batch[22060] - loss: 0.850999  acc: 78.1250%(25/32)\n",
            "Epoch [83] Batch[22080] - loss: 0.889030  acc: 75.0000%(24/32)\n",
            "Epoch [83] Batch[22100] - loss: 0.787989  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.280446  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [83] Batch[22120] - loss: 0.918036  acc: 65.6250%(21/32)\n",
            "Epoch [83] Batch[22140] - loss: 1.046385  acc: 71.8750%(23/32)\n",
            "Epoch [83] Batch[22160] - loss: 1.062119  acc: 62.5000%(20/32)\n",
            "Epoch [84] Batch[22180] - loss: 0.873346  acc: 68.7500%(22/32)\n",
            "Epoch [84] Batch[22200] - loss: 1.024860  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.278957  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [84] Batch[22220] - loss: 0.984706  acc: 75.0000%(24/32)\n",
            "Epoch [84] Batch[22240] - loss: 0.772889  acc: 65.6250%(21/32)\n",
            "Epoch [84] Batch[22260] - loss: 0.705612  acc: 84.3750%(27/32)\n",
            "Epoch [84] Batch[22280] - loss: 0.784715  acc: 78.1250%(25/32)\n",
            "Epoch [84] Batch[22300] - loss: 0.794198  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.280998  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [84] Batch[22320] - loss: 0.866801  acc: 75.0000%(24/32)\n",
            "Epoch [84] Batch[22340] - loss: 0.899179  acc: 59.3750%(19/32)\n",
            "Epoch [84] Batch[22360] - loss: 0.859720  acc: 87.5000%(28/32)\n",
            "Epoch [84] Batch[22380] - loss: 0.836845  acc: 71.8750%(23/32)\n",
            "Epoch [84] Batch[22400] - loss: 0.855502  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.280873  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [84] Batch[22420] - loss: 0.822574  acc: 71.8750%(23/32)\n",
            "Epoch [85] Batch[22440] - loss: 0.826310  acc: 78.1250%(25/32)\n",
            "Epoch [85] Batch[22460] - loss: 0.773285  acc: 75.0000%(24/32)\n",
            "Epoch [85] Batch[22480] - loss: 0.918008  acc: 75.0000%(24/32)\n",
            "Epoch [85] Batch[22500] - loss: 0.859973  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.280706  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [85] Batch[22520] - loss: 0.953876  acc: 65.6250%(21/32)\n",
            "Epoch [85] Batch[22540] - loss: 0.927785  acc: 59.3750%(19/32)\n",
            "Epoch [85] Batch[22560] - loss: 0.676962  acc: 75.0000%(24/32)\n",
            "Epoch [85] Batch[22580] - loss: 0.890593  acc: 75.0000%(24/32)\n",
            "Epoch [85] Batch[22600] - loss: 0.912170  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.281571  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [85] Batch[22620] - loss: 0.814850  acc: 78.1250%(25/32)\n",
            "Epoch [85] Batch[22640] - loss: 0.939239  acc: 65.6250%(21/32)\n",
            "Epoch [85] Batch[22660] - loss: 0.952674  acc: 59.3750%(19/32)\n",
            "Epoch [85] Batch[22680] - loss: 0.758678  acc: 75.0000%(24/32)\n",
            "Epoch [86] Batch[22700] - loss: 0.902773  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.279441  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [86] Batch[22720] - loss: 0.916900  acc: 68.7500%(22/32)\n",
            "Epoch [86] Batch[22740] - loss: 0.750906  acc: 81.2500%(26/32)\n",
            "Epoch [86] Batch[22760] - loss: 0.807617  acc: 78.1250%(25/32)\n",
            "Epoch [86] Batch[22780] - loss: 0.832815  acc: 75.0000%(24/32)\n",
            "Epoch [86] Batch[22800] - loss: 0.929338  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.281626  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [86] Batch[22820] - loss: 1.002770  acc: 65.6250%(21/32)\n",
            "Epoch [86] Batch[22840] - loss: 0.784614  acc: 75.0000%(24/32)\n",
            "Epoch [86] Batch[22860] - loss: 0.850264  acc: 65.6250%(21/32)\n",
            "Epoch [86] Batch[22880] - loss: 0.773214  acc: 71.8750%(23/32)\n",
            "Epoch [86] Batch[22900] - loss: 0.715879  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.278754  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [86] Batch[22920] - loss: 0.823066  acc: 75.0000%(24/32)\n",
            "Epoch [86] Batch[22940] - loss: 0.943997  acc: 62.5000%(20/32)\n",
            "Epoch [86] Batch[22960] - loss: 0.737673  acc: 78.1250%(25/32)\n",
            "Epoch [87] Batch[22980] - loss: 0.716365  acc: 90.6250%(29/32)\n",
            "Epoch [87] Batch[23000] - loss: 0.806189  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.278478  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [87] Batch[23020] - loss: 0.833614  acc: 78.1250%(25/32)\n",
            "Epoch [87] Batch[23040] - loss: 0.900653  acc: 62.5000%(20/32)\n",
            "Epoch [87] Batch[23060] - loss: 0.777970  acc: 78.1250%(25/32)\n",
            "Epoch [87] Batch[23080] - loss: 0.726050  acc: 84.3750%(27/32)\n",
            "Epoch [87] Batch[23100] - loss: 0.879676  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.279759  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [87] Batch[23120] - loss: 0.692319  acc: 84.3750%(27/32)\n",
            "Epoch [87] Batch[23140] - loss: 0.857671  acc: 75.0000%(24/32)\n",
            "Epoch [87] Batch[23160] - loss: 0.808062  acc: 75.0000%(24/32)\n",
            "Epoch [87] Batch[23180] - loss: 0.889894  acc: 68.7500%(22/32)\n",
            "Epoch [87] Batch[23200] - loss: 0.817093  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.280781  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [87] Batch[23220] - loss: 0.890540  acc: 65.6250%(21/32)\n",
            "Epoch [88] Batch[23240] - loss: 0.883343  acc: 65.6250%(21/32)\n",
            "Epoch [88] Batch[23260] - loss: 0.724122  acc: 84.3750%(27/32)\n",
            "Epoch [88] Batch[23280] - loss: 0.801439  acc: 81.2500%(26/32)\n",
            "Epoch [88] Batch[23300] - loss: 0.802643  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.278540  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [88] Batch[23320] - loss: 0.805060  acc: 75.0000%(24/32)\n",
            "Epoch [88] Batch[23340] - loss: 0.799864  acc: 81.2500%(26/32)\n",
            "Epoch [88] Batch[23360] - loss: 0.816706  acc: 71.8750%(23/32)\n",
            "Epoch [88] Batch[23380] - loss: 0.839048  acc: 75.0000%(24/32)\n",
            "Epoch [88] Batch[23400] - loss: 0.906584  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.279833  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [88] Batch[23420] - loss: 0.884079  acc: 68.7500%(22/32)\n",
            "Epoch [88] Batch[23440] - loss: 0.824416  acc: 68.7500%(22/32)\n",
            "Epoch [88] Batch[23460] - loss: 1.002171  acc: 59.3750%(19/32)\n",
            "Epoch [88] Batch[23480] - loss: 0.801073  acc: 78.1250%(25/32)\n",
            "Epoch [89] Batch[23500] - loss: 0.732198  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.280477  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [89] Batch[23520] - loss: 0.685121  acc: 75.0000%(24/32)\n",
            "Epoch [89] Batch[23540] - loss: 0.941022  acc: 68.7500%(22/32)\n",
            "Epoch [89] Batch[23560] - loss: 0.670108  acc: 81.2500%(26/32)\n",
            "Epoch [89] Batch[23580] - loss: 0.805410  acc: 65.6250%(21/32)\n",
            "Epoch [89] Batch[23600] - loss: 0.881539  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.279700  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [89] Batch[23620] - loss: 0.809372  acc: 71.8750%(23/32)\n",
            "Epoch [89] Batch[23640] - loss: 0.954922  acc: 65.6250%(21/32)\n",
            "Epoch [89] Batch[23660] - loss: 0.813567  acc: 71.8750%(23/32)\n",
            "Epoch [89] Batch[23680] - loss: 0.931977  acc: 65.6250%(21/32)\n",
            "Epoch [89] Batch[23700] - loss: 1.036905  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.280108  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [89] Batch[23720] - loss: 0.757841  acc: 87.5000%(28/32)\n",
            "Epoch [89] Batch[23740] - loss: 0.866196  acc: 65.6250%(21/32)\n",
            "Epoch [89] Batch[23760] - loss: 0.756108  acc: 78.1250%(25/32)\n",
            "Epoch [90] Batch[23780] - loss: 0.828488  acc: 68.7500%(22/32)\n",
            "Epoch [90] Batch[23800] - loss: 0.773592  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.281003  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [90] Batch[23820] - loss: 0.794723  acc: 87.5000%(28/32)\n",
            "Epoch [90] Batch[23840] - loss: 0.855816  acc: 68.7500%(22/32)\n",
            "Epoch [90] Batch[23860] - loss: 1.039976  acc: 53.1250%(17/32)\n",
            "Epoch [90] Batch[23880] - loss: 0.822608  acc: 78.1250%(25/32)\n",
            "Epoch [90] Batch[23900] - loss: 0.791488  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.280977  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [90] Batch[23920] - loss: 0.866858  acc: 62.5000%(20/32)\n",
            "Epoch [90] Batch[23940] - loss: 0.882812  acc: 68.7500%(22/32)\n",
            "Epoch [90] Batch[23960] - loss: 0.947235  acc: 59.3750%(19/32)\n",
            "Epoch [90] Batch[23980] - loss: 0.750543  acc: 75.0000%(24/32)\n",
            "Epoch [90] Batch[24000] - loss: 0.813398  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.279705  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [90] Batch[24020] - loss: 1.025566  acc: 62.5000%(20/32)\n",
            "Epoch [91] Batch[24040] - loss: 0.668386  acc: 78.1250%(25/32)\n",
            "Epoch [91] Batch[24060] - loss: 0.660909  acc: 93.7500%(30/32)\n",
            "Epoch [91] Batch[24080] - loss: 0.833119  acc: 71.8750%(23/32)\n",
            "Epoch [91] Batch[24100] - loss: 0.727859  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.279102  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [91] Batch[24120] - loss: 0.808386  acc: 68.7500%(22/32)\n",
            "Epoch [91] Batch[24140] - loss: 0.776057  acc: 78.1250%(25/32)\n",
            "Epoch [91] Batch[24160] - loss: 0.809764  acc: 78.1250%(25/32)\n",
            "Epoch [91] Batch[24180] - loss: 0.690590  acc: 81.2500%(26/32)\n",
            "Epoch [91] Batch[24200] - loss: 0.758106  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.280638  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [91] Batch[24220] - loss: 0.852320  acc: 71.8750%(23/32)\n",
            "Epoch [91] Batch[24240] - loss: 0.945144  acc: 71.8750%(23/32)\n",
            "Epoch [91] Batch[24260] - loss: 1.062557  acc: 65.6250%(21/32)\n",
            "Epoch [91] Batch[24280] - loss: 0.938858  acc: 71.8750%(23/32)\n",
            "Epoch [92] Batch[24300] - loss: 0.785294  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.280849  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [92] Batch[24320] - loss: 0.693947  acc: 81.2500%(26/32)\n",
            "Epoch [92] Batch[24340] - loss: 0.738207  acc: 84.3750%(27/32)\n",
            "Epoch [92] Batch[24360] - loss: 0.761966  acc: 75.0000%(24/32)\n",
            "Epoch [92] Batch[24380] - loss: 0.718011  acc: 81.2500%(26/32)\n",
            "Epoch [92] Batch[24400] - loss: 0.824673  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.280079  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [92] Batch[24420] - loss: 0.916360  acc: 75.0000%(24/32)\n",
            "Epoch [92] Batch[24440] - loss: 0.741588  acc: 81.2500%(26/32)\n",
            "Epoch [92] Batch[24460] - loss: 0.808004  acc: 78.1250%(25/32)\n",
            "Epoch [92] Batch[24480] - loss: 0.830409  acc: 71.8750%(23/32)\n",
            "Epoch [92] Batch[24500] - loss: 0.929501  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.279269  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [92] Batch[24520] - loss: 0.762922  acc: 71.8750%(23/32)\n",
            "Epoch [92] Batch[24540] - loss: 0.796825  acc: 78.1250%(25/32)\n",
            "Epoch [92] Batch[24560] - loss: 0.842308  acc: 68.7500%(22/32)\n",
            "Epoch [93] Batch[24580] - loss: 0.868131  acc: 71.8750%(23/32)\n",
            "Epoch [93] Batch[24600] - loss: 0.756115  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.279500  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [93] Batch[24620] - loss: 0.862472  acc: 75.0000%(24/32)\n",
            "Epoch [93] Batch[24640] - loss: 0.946575  acc: 75.0000%(24/32)\n",
            "Epoch [93] Batch[24660] - loss: 0.899698  acc: 65.6250%(21/32)\n",
            "Epoch [93] Batch[24680] - loss: 0.702000  acc: 78.1250%(25/32)\n",
            "Epoch [93] Batch[24700] - loss: 0.958998  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.280963  acc: 41.5077%(457/1101) \n",
            "\n",
            "Epoch [93] Batch[24720] - loss: 0.752497  acc: 84.3750%(27/32)\n",
            "Epoch [93] Batch[24740] - loss: 0.614078  acc: 84.3750%(27/32)\n",
            "Epoch [93] Batch[24760] - loss: 0.712799  acc: 78.1250%(25/32)\n",
            "Epoch [93] Batch[24780] - loss: 0.782626  acc: 75.0000%(24/32)\n",
            "Epoch [93] Batch[24800] - loss: 0.687312  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.279740  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [93] Batch[24820] - loss: 0.999055  acc: 68.7500%(22/32)\n",
            "Epoch [94] Batch[24840] - loss: 0.730404  acc: 81.2500%(26/32)\n",
            "Epoch [94] Batch[24860] - loss: 0.749794  acc: 84.3750%(27/32)\n",
            "Epoch [94] Batch[24880] - loss: 0.764738  acc: 78.1250%(25/32)\n",
            "Epoch [94] Batch[24900] - loss: 0.899423  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.280158  acc: 41.7802%(460/1101) \n",
            "\n",
            "Epoch [94] Batch[24920] - loss: 0.752434  acc: 75.0000%(24/32)\n",
            "Epoch [94] Batch[24940] - loss: 0.883202  acc: 78.1250%(25/32)\n",
            "Epoch [94] Batch[24960] - loss: 0.788147  acc: 75.0000%(24/32)\n",
            "Epoch [94] Batch[24980] - loss: 0.771566  acc: 75.0000%(24/32)\n",
            "Epoch [94] Batch[25000] - loss: 0.661240  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.280567  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [94] Batch[25020] - loss: 0.833321  acc: 71.8750%(23/32)\n",
            "Epoch [94] Batch[25040] - loss: 0.805041  acc: 75.0000%(24/32)\n",
            "Epoch [94] Batch[25060] - loss: 0.981580  acc: 62.5000%(20/32)\n",
            "Epoch [94] Batch[25080] - loss: 0.788164  acc: 81.2500%(26/32)\n",
            "Epoch [95] Batch[25100] - loss: 1.021302  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.281463  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [95] Batch[25120] - loss: 0.864779  acc: 68.7500%(22/32)\n",
            "Epoch [95] Batch[25140] - loss: 0.849314  acc: 71.8750%(23/32)\n",
            "Epoch [95] Batch[25160] - loss: 0.832048  acc: 84.3750%(27/32)\n",
            "Epoch [95] Batch[25180] - loss: 0.772430  acc: 68.7500%(22/32)\n",
            "Epoch [95] Batch[25200] - loss: 0.727745  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.281751  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [95] Batch[25220] - loss: 0.850398  acc: 62.5000%(20/32)\n",
            "Epoch [95] Batch[25240] - loss: 0.824531  acc: 59.3750%(19/32)\n",
            "Epoch [95] Batch[25260] - loss: 0.861875  acc: 71.8750%(23/32)\n",
            "Epoch [95] Batch[25280] - loss: 0.791593  acc: 75.0000%(24/32)\n",
            "Epoch [95] Batch[25300] - loss: 0.884938  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.278890  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [95] Batch[25320] - loss: 0.736742  acc: 75.0000%(24/32)\n",
            "Epoch [95] Batch[25340] - loss: 0.790088  acc: 68.7500%(22/32)\n",
            "Epoch [95] Batch[25360] - loss: 0.776278  acc: 75.0000%(24/32)\n",
            "Epoch [96] Batch[25380] - loss: 0.739886  acc: 78.1250%(25/32)\n",
            "Epoch [96] Batch[25400] - loss: 0.775374  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.280997  acc: 41.7802%(460/1101) \n",
            "\n",
            "Epoch [96] Batch[25420] - loss: 0.758547  acc: 71.8750%(23/32)\n",
            "Epoch [96] Batch[25440] - loss: 0.709744  acc: 81.2500%(26/32)\n",
            "Epoch [96] Batch[25460] - loss: 0.758603  acc: 78.1250%(25/32)\n",
            "Epoch [96] Batch[25480] - loss: 0.686197  acc: 75.0000%(24/32)\n",
            "Epoch [96] Batch[25500] - loss: 0.887405  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.280252  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [96] Batch[25520] - loss: 0.761541  acc: 87.5000%(28/32)\n",
            "Epoch [96] Batch[25540] - loss: 0.776111  acc: 78.1250%(25/32)\n",
            "Epoch [96] Batch[25560] - loss: 0.729548  acc: 81.2500%(26/32)\n",
            "Epoch [96] Batch[25580] - loss: 0.963113  acc: 75.0000%(24/32)\n",
            "Epoch [96] Batch[25600] - loss: 0.950620  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.280898  acc: 41.7802%(460/1101) \n",
            "\n",
            "Epoch [96] Batch[25620] - loss: 0.900923  acc: 65.6250%(21/32)\n",
            "Epoch [97] Batch[25640] - loss: 0.818782  acc: 68.7500%(22/32)\n",
            "Epoch [97] Batch[25660] - loss: 0.768271  acc: 71.8750%(23/32)\n",
            "Epoch [97] Batch[25680] - loss: 0.677519  acc: 84.3750%(27/32)\n",
            "Epoch [97] Batch[25700] - loss: 0.837389  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.278487  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [97] Batch[25720] - loss: 0.734570  acc: 81.2500%(26/32)\n",
            "Epoch [97] Batch[25740] - loss: 0.835981  acc: 65.6250%(21/32)\n",
            "Epoch [97] Batch[25760] - loss: 0.793908  acc: 78.1250%(25/32)\n",
            "Epoch [97] Batch[25780] - loss: 0.656630  acc: 90.6250%(29/32)\n",
            "Epoch [97] Batch[25800] - loss: 0.830119  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.282073  acc: 41.4169%(456/1101) \n",
            "\n",
            "Epoch [97] Batch[25820] - loss: 0.650425  acc: 87.5000%(28/32)\n",
            "Epoch [97] Batch[25840] - loss: 0.723817  acc: 78.1250%(25/32)\n",
            "Epoch [97] Batch[25860] - loss: 0.678963  acc: 78.1250%(25/32)\n",
            "Epoch [97] Batch[25880] - loss: 0.821640  acc: 78.1250%(25/32)\n",
            "Epoch [98] Batch[25900] - loss: 0.632069  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.281190  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [98] Batch[25920] - loss: 0.809047  acc: 75.0000%(24/32)\n",
            "Epoch [98] Batch[25940] - loss: 0.684226  acc: 81.2500%(26/32)\n",
            "Epoch [98] Batch[25960] - loss: 0.915892  acc: 62.5000%(20/32)\n",
            "Epoch [98] Batch[25980] - loss: 0.721696  acc: 81.2500%(26/32)\n",
            "Epoch [98] Batch[26000] - loss: 0.642882  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.279980  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [98] Batch[26020] - loss: 0.745401  acc: 81.2500%(26/32)\n",
            "Epoch [98] Batch[26040] - loss: 0.756976  acc: 87.5000%(28/32)\n",
            "Epoch [98] Batch[26060] - loss: 0.775762  acc: 75.0000%(24/32)\n",
            "Epoch [98] Batch[26080] - loss: 0.724220  acc: 78.1250%(25/32)\n",
            "Epoch [98] Batch[26100] - loss: 0.825912  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.283650  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [98] Batch[26120] - loss: 0.802560  acc: 75.0000%(24/32)\n",
            "Epoch [98] Batch[26140] - loss: 0.813348  acc: 71.8750%(23/32)\n",
            "Epoch [98] Batch[26160] - loss: 0.689954  acc: 81.2500%(26/32)\n",
            "Epoch [99] Batch[26180] - loss: 0.700130  acc: 75.0000%(24/32)\n",
            "Epoch [99] Batch[26200] - loss: 0.838643  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.282210  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [99] Batch[26220] - loss: 0.698494  acc: 87.5000%(28/32)\n",
            "Epoch [99] Batch[26240] - loss: 0.808138  acc: 78.1250%(25/32)\n",
            "Epoch [99] Batch[26260] - loss: 0.606428  acc: 87.5000%(28/32)\n",
            "Epoch [99] Batch[26280] - loss: 0.588839  acc: 90.6250%(29/32)\n",
            "Epoch [99] Batch[26300] - loss: 0.845479  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.282111  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [99] Batch[26320] - loss: 0.840558  acc: 75.0000%(24/32)\n",
            "Epoch [99] Batch[26340] - loss: 0.748885  acc: 78.1250%(25/32)\n",
            "Epoch [99] Batch[26360] - loss: 0.967392  acc: 65.6250%(21/32)\n",
            "Epoch [99] Batch[26380] - loss: 0.883068  acc: 65.6250%(21/32)\n",
            "Epoch [99] Batch[26400] - loss: 0.853495  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.280998  acc: 41.7802%(460/1101) \n",
            "\n",
            "Epoch [99] Batch[26420] - loss: 0.560760  acc: 87.5000%(28/32)\n",
            "Epoch [100] Batch[26440] - loss: 0.748616  acc: 75.0000%(24/32)\n",
            "Epoch [100] Batch[26460] - loss: 0.822669  acc: 68.7500%(22/32)\n",
            "Epoch [100] Batch[26480] - loss: 0.776476  acc: 84.3750%(27/32)\n",
            "Epoch [100] Batch[26500] - loss: 0.651731  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.280965  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [100] Batch[26520] - loss: 0.685942  acc: 87.5000%(28/32)\n",
            "Epoch [100] Batch[26540] - loss: 0.833483  acc: 71.8750%(23/32)\n",
            "Epoch [100] Batch[26560] - loss: 0.727121  acc: 78.1250%(25/32)\n",
            "Epoch [100] Batch[26580] - loss: 0.730489  acc: 84.3750%(27/32)\n",
            "Epoch [100] Batch[26600] - loss: 0.717143  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.282408  acc: 41.7802%(460/1101) \n",
            "\n",
            "Epoch [100] Batch[26620] - loss: 0.988280  acc: 56.2500%(18/32)\n",
            "Epoch [100] Batch[26640] - loss: 0.697228  acc: 78.1250%(25/32)\n",
            "Epoch [100] Batch[26660] - loss: 0.794808  acc: 75.0000%(24/32)\n",
            "Epoch [100] Batch[26680] - loss: 0.664455  acc: 81.2500%(26/32)\n",
            "Epoch [100] Batch[26700] - loss: 0.711111  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.281111  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [101] Batch[26720] - loss: 0.653581  acc: 84.3750%(27/32)\n",
            "Epoch [101] Batch[26740] - loss: 0.756969  acc: 68.7500%(22/32)\n",
            "Epoch [101] Batch[26760] - loss: 0.647449  acc: 90.6250%(29/32)\n",
            "Epoch [101] Batch[26780] - loss: 0.698433  acc: 71.8750%(23/32)\n",
            "Epoch [101] Batch[26800] - loss: 0.864573  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.281686  acc: 41.7802%(460/1101) \n",
            "\n",
            "Epoch [101] Batch[26820] - loss: 0.616082  acc: 87.5000%(28/32)\n",
            "Epoch [101] Batch[26840] - loss: 0.741486  acc: 78.1250%(25/32)\n",
            "Epoch [101] Batch[26860] - loss: 0.687973  acc: 78.1250%(25/32)\n",
            "Epoch [101] Batch[26880] - loss: 0.812175  acc: 62.5000%(20/32)\n",
            "Epoch [101] Batch[26900] - loss: 0.678676  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.283611  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [101] Batch[26920] - loss: 0.815999  acc: 78.1250%(25/32)\n",
            "Epoch [101] Batch[26940] - loss: 0.670844  acc: 87.5000%(28/32)\n",
            "Epoch [101] Batch[26960] - loss: 0.724331  acc: 68.7500%(22/32)\n",
            "Epoch [102] Batch[26980] - loss: 0.831454  acc: 75.0000%(24/32)\n",
            "Epoch [102] Batch[27000] - loss: 0.780518  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.283491  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [102] Batch[27020] - loss: 0.766687  acc: 71.8750%(23/32)\n",
            "Epoch [102] Batch[27040] - loss: 0.896975  acc: 68.7500%(22/32)\n",
            "Epoch [102] Batch[27060] - loss: 0.643036  acc: 87.5000%(28/32)\n",
            "Epoch [102] Batch[27080] - loss: 0.551523  acc: 93.7500%(30/32)\n",
            "Epoch [102] Batch[27100] - loss: 0.806389  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.282581  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [102] Batch[27120] - loss: 0.694152  acc: 87.5000%(28/32)\n",
            "Epoch [102] Batch[27140] - loss: 0.669586  acc: 87.5000%(28/32)\n",
            "Epoch [102] Batch[27160] - loss: 0.705772  acc: 81.2500%(26/32)\n",
            "Epoch [102] Batch[27180] - loss: 0.788289  acc: 81.2500%(26/32)\n",
            "Epoch [102] Batch[27200] - loss: 0.738169  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.280750  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [102] Batch[27220] - loss: 0.814035  acc: 68.7500%(22/32)\n",
            "Epoch [103] Batch[27240] - loss: 0.728393  acc: 81.2500%(26/32)\n",
            "Epoch [103] Batch[27260] - loss: 0.666880  acc: 84.3750%(27/32)\n",
            "Epoch [103] Batch[27280] - loss: 0.694014  acc: 75.0000%(24/32)\n",
            "Epoch [103] Batch[27300] - loss: 0.786763  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.284078  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [103] Batch[27320] - loss: 0.717333  acc: 71.8750%(23/32)\n",
            "Epoch [103] Batch[27340] - loss: 0.866879  acc: 71.8750%(23/32)\n",
            "Epoch [103] Batch[27360] - loss: 0.826159  acc: 71.8750%(23/32)\n",
            "Epoch [103] Batch[27380] - loss: 0.851257  acc: 65.6250%(21/32)\n",
            "Epoch [103] Batch[27400] - loss: 0.825285  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.283820  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [103] Batch[27420] - loss: 0.783084  acc: 71.8750%(23/32)\n",
            "Epoch [103] Batch[27440] - loss: 0.767347  acc: 87.5000%(28/32)\n",
            "Epoch [103] Batch[27460] - loss: 0.739094  acc: 81.2500%(26/32)\n",
            "Epoch [103] Batch[27480] - loss: 0.778066  acc: 71.8750%(23/32)\n",
            "Epoch [103] Batch[27500] - loss: 0.813115  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.282937  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [104] Batch[27520] - loss: 0.764693  acc: 84.3750%(27/32)\n",
            "Epoch [104] Batch[27540] - loss: 0.688601  acc: 81.2500%(26/32)\n",
            "Epoch [104] Batch[27560] - loss: 0.878568  acc: 71.8750%(23/32)\n",
            "Epoch [104] Batch[27580] - loss: 0.631351  acc: 90.6250%(29/32)\n",
            "Epoch [104] Batch[27600] - loss: 0.621779  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.282875  acc: 41.7802%(460/1101) \n",
            "\n",
            "Epoch [104] Batch[27620] - loss: 0.756946  acc: 75.0000%(24/32)\n",
            "Epoch [104] Batch[27640] - loss: 0.554803  acc: 84.3750%(27/32)\n",
            "Epoch [104] Batch[27660] - loss: 0.710135  acc: 71.8750%(23/32)\n",
            "Epoch [104] Batch[27680] - loss: 0.857529  acc: 71.8750%(23/32)\n",
            "Epoch [104] Batch[27700] - loss: 0.618012  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.284855  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [104] Batch[27720] - loss: 0.606929  acc: 87.5000%(28/32)\n",
            "Epoch [104] Batch[27740] - loss: 0.674140  acc: 81.2500%(26/32)\n",
            "Epoch [104] Batch[27760] - loss: 0.760965  acc: 71.8750%(23/32)\n",
            "Epoch [105] Batch[27780] - loss: 0.613231  acc: 93.7500%(30/32)\n",
            "Epoch [105] Batch[27800] - loss: 0.586185  acc: 93.7500%(30/32)\n",
            "\n",
            "Evaluation - loss: 1.281996  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [105] Batch[27820] - loss: 0.894040  acc: 68.7500%(22/32)\n",
            "Epoch [105] Batch[27840] - loss: 0.592710  acc: 93.7500%(30/32)\n",
            "Epoch [105] Batch[27860] - loss: 0.791773  acc: 65.6250%(21/32)\n",
            "Epoch [105] Batch[27880] - loss: 0.674406  acc: 81.2500%(26/32)\n",
            "Epoch [105] Batch[27900] - loss: 0.737145  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.283616  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [105] Batch[27920] - loss: 0.633744  acc: 78.1250%(25/32)\n",
            "Epoch [105] Batch[27940] - loss: 0.739575  acc: 78.1250%(25/32)\n",
            "Epoch [105] Batch[27960] - loss: 0.542519  acc: 87.5000%(28/32)\n",
            "Epoch [105] Batch[27980] - loss: 0.650343  acc: 81.2500%(26/32)\n",
            "Epoch [105] Batch[28000] - loss: 0.678472  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.282393  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [105] Batch[28020] - loss: 0.620662  acc: 87.5000%(28/32)\n",
            "Epoch [106] Batch[28040] - loss: 0.819280  acc: 71.8750%(23/32)\n",
            "Epoch [106] Batch[28060] - loss: 0.707427  acc: 84.3750%(27/32)\n",
            "Epoch [106] Batch[28080] - loss: 0.824385  acc: 81.2500%(26/32)\n",
            "Epoch [106] Batch[28100] - loss: 0.676916  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.283596  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [106] Batch[28120] - loss: 0.677981  acc: 81.2500%(26/32)\n",
            "Epoch [106] Batch[28140] - loss: 0.870734  acc: 71.8750%(23/32)\n",
            "Epoch [106] Batch[28160] - loss: 0.801507  acc: 71.8750%(23/32)\n",
            "Epoch [106] Batch[28180] - loss: 0.720083  acc: 87.5000%(28/32)\n",
            "Epoch [106] Batch[28200] - loss: 0.583605  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.286744  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [106] Batch[28220] - loss: 0.663568  acc: 75.0000%(24/32)\n",
            "Epoch [106] Batch[28240] - loss: 0.780098  acc: 71.8750%(23/32)\n",
            "Epoch [106] Batch[28260] - loss: 0.703279  acc: 84.3750%(27/32)\n",
            "Epoch [106] Batch[28280] - loss: 0.911636  acc: 59.3750%(19/32)\n",
            "Epoch [106] Batch[28300] - loss: 0.739595  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.282157  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [107] Batch[28320] - loss: 0.853924  acc: 75.0000%(24/32)\n",
            "Epoch [107] Batch[28340] - loss: 0.758046  acc: 78.1250%(25/32)\n",
            "Epoch [107] Batch[28360] - loss: 0.871732  acc: 65.6250%(21/32)\n",
            "Epoch [107] Batch[28380] - loss: 0.767347  acc: 75.0000%(24/32)\n",
            "Epoch [107] Batch[28400] - loss: 0.867330  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.284660  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [107] Batch[28420] - loss: 0.785629  acc: 68.7500%(22/32)\n",
            "Epoch [107] Batch[28440] - loss: 0.660307  acc: 84.3750%(27/32)\n",
            "Epoch [107] Batch[28460] - loss: 0.676565  acc: 78.1250%(25/32)\n",
            "Epoch [107] Batch[28480] - loss: 0.781420  acc: 71.8750%(23/32)\n",
            "Epoch [107] Batch[28500] - loss: 0.740500  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.285818  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [107] Batch[28520] - loss: 0.813037  acc: 71.8750%(23/32)\n",
            "Epoch [107] Batch[28540] - loss: 0.705793  acc: 78.1250%(25/32)\n",
            "Epoch [107] Batch[28560] - loss: 0.729059  acc: 81.2500%(26/32)\n",
            "Epoch [108] Batch[28580] - loss: 0.697878  acc: 87.5000%(28/32)\n",
            "Epoch [108] Batch[28600] - loss: 0.738306  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.287392  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [108] Batch[28620] - loss: 0.680300  acc: 78.1250%(25/32)\n",
            "Epoch [108] Batch[28640] - loss: 0.708330  acc: 71.8750%(23/32)\n",
            "Epoch [108] Batch[28660] - loss: 0.673620  acc: 81.2500%(26/32)\n",
            "Epoch [108] Batch[28680] - loss: 0.623818  acc: 90.6250%(29/32)\n",
            "Epoch [108] Batch[28700] - loss: 0.596858  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.281351  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [108] Batch[28720] - loss: 0.715936  acc: 81.2500%(26/32)\n",
            "Epoch [108] Batch[28740] - loss: 0.594991  acc: 84.3750%(27/32)\n",
            "Epoch [108] Batch[28760] - loss: 0.685474  acc: 78.1250%(25/32)\n",
            "Epoch [108] Batch[28780] - loss: 0.699321  acc: 81.2500%(26/32)\n",
            "Epoch [108] Batch[28800] - loss: 0.708653  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.286686  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [108] Batch[28820] - loss: 0.851578  acc: 65.6250%(21/32)\n",
            "Epoch [109] Batch[28840] - loss: 0.684105  acc: 81.2500%(26/32)\n",
            "Epoch [109] Batch[28860] - loss: 0.849809  acc: 65.6250%(21/32)\n",
            "Epoch [109] Batch[28880] - loss: 0.681900  acc: 84.3750%(27/32)\n",
            "Epoch [109] Batch[28900] - loss: 0.754312  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.282894  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [109] Batch[28920] - loss: 0.603519  acc: 78.1250%(25/32)\n",
            "Epoch [109] Batch[28940] - loss: 0.668436  acc: 81.2500%(26/32)\n",
            "Epoch [109] Batch[28960] - loss: 0.738248  acc: 75.0000%(24/32)\n",
            "Epoch [109] Batch[28980] - loss: 0.768883  acc: 78.1250%(25/32)\n",
            "Epoch [109] Batch[29000] - loss: 0.696318  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.284907  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [109] Batch[29020] - loss: 0.641660  acc: 78.1250%(25/32)\n",
            "Epoch [109] Batch[29040] - loss: 0.624240  acc: 90.6250%(29/32)\n",
            "Epoch [109] Batch[29060] - loss: 0.996909  acc: 75.0000%(24/32)\n",
            "Epoch [109] Batch[29080] - loss: 0.851067  acc: 71.8750%(23/32)\n",
            "Epoch [109] Batch[29100] - loss: 0.757357  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.286973  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [110] Batch[29120] - loss: 0.643802  acc: 84.3750%(27/32)\n",
            "Epoch [110] Batch[29140] - loss: 0.691401  acc: 87.5000%(28/32)\n",
            "Epoch [110] Batch[29160] - loss: 0.694148  acc: 81.2500%(26/32)\n",
            "Epoch [110] Batch[29180] - loss: 0.597389  acc: 78.1250%(25/32)\n",
            "Epoch [110] Batch[29200] - loss: 0.733933  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.284754  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [110] Batch[29220] - loss: 0.781773  acc: 68.7500%(22/32)\n",
            "Epoch [110] Batch[29240] - loss: 0.760839  acc: 68.7500%(22/32)\n",
            "Epoch [110] Batch[29260] - loss: 0.632769  acc: 78.1250%(25/32)\n",
            "Epoch [110] Batch[29280] - loss: 0.677699  acc: 84.3750%(27/32)\n",
            "Epoch [110] Batch[29300] - loss: 0.703262  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.285858  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [110] Batch[29320] - loss: 0.820320  acc: 68.7500%(22/32)\n",
            "Epoch [110] Batch[29340] - loss: 0.602141  acc: 84.3750%(27/32)\n",
            "Epoch [110] Batch[29360] - loss: 0.823211  acc: 75.0000%(24/32)\n",
            "Epoch [111] Batch[29380] - loss: 0.822454  acc: 78.1250%(25/32)\n",
            "Epoch [111] Batch[29400] - loss: 0.678971  acc: 90.6250%(29/32)\n",
            "\n",
            "Evaluation - loss: 1.285571  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [111] Batch[29420] - loss: 0.852115  acc: 71.8750%(23/32)\n",
            "Epoch [111] Batch[29440] - loss: 0.584684  acc: 81.2500%(26/32)\n",
            "Epoch [111] Batch[29460] - loss: 0.596583  acc: 87.5000%(28/32)\n",
            "Epoch [111] Batch[29480] - loss: 0.547002  acc: 96.8750%(31/32)\n",
            "Epoch [111] Batch[29500] - loss: 0.722940  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.286919  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [111] Batch[29520] - loss: 0.639892  acc: 75.0000%(24/32)\n",
            "Epoch [111] Batch[29540] - loss: 0.623947  acc: 84.3750%(27/32)\n",
            "Epoch [111] Batch[29560] - loss: 0.825240  acc: 68.7500%(22/32)\n",
            "Epoch [111] Batch[29580] - loss: 0.791269  acc: 75.0000%(24/32)\n",
            "Epoch [111] Batch[29600] - loss: 0.681893  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.286562  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [111] Batch[29620] - loss: 0.676270  acc: 84.3750%(27/32)\n",
            "Epoch [112] Batch[29640] - loss: 0.790163  acc: 84.3750%(27/32)\n",
            "Epoch [112] Batch[29660] - loss: 0.657177  acc: 84.3750%(27/32)\n",
            "Epoch [112] Batch[29680] - loss: 0.890678  acc: 68.7500%(22/32)\n",
            "Epoch [112] Batch[29700] - loss: 0.867536  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.284886  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [112] Batch[29720] - loss: 0.707710  acc: 71.8750%(23/32)\n",
            "Epoch [112] Batch[29740] - loss: 0.603267  acc: 93.7500%(30/32)\n",
            "Epoch [112] Batch[29760] - loss: 0.813204  acc: 78.1250%(25/32)\n",
            "Epoch [112] Batch[29780] - loss: 0.882810  acc: 59.3750%(19/32)\n",
            "Epoch [112] Batch[29800] - loss: 0.561402  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.287544  acc: 41.7802%(460/1101) \n",
            "\n",
            "Epoch [112] Batch[29820] - loss: 0.732684  acc: 71.8750%(23/32)\n",
            "Epoch [112] Batch[29840] - loss: 1.057583  acc: 62.5000%(20/32)\n",
            "Epoch [112] Batch[29860] - loss: 0.733888  acc: 78.1250%(25/32)\n",
            "Epoch [112] Batch[29880] - loss: 0.642563  acc: 84.3750%(27/32)\n",
            "Epoch [112] Batch[29900] - loss: 0.620541  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.285988  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [113] Batch[29920] - loss: 0.673140  acc: 78.1250%(25/32)\n",
            "Epoch [113] Batch[29940] - loss: 0.776720  acc: 75.0000%(24/32)\n",
            "Epoch [113] Batch[29960] - loss: 0.820476  acc: 78.1250%(25/32)\n",
            "Epoch [113] Batch[29980] - loss: 0.737421  acc: 65.6250%(21/32)\n",
            "Epoch [113] Batch[30000] - loss: 0.780609  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.284828  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [113] Batch[30020] - loss: 0.922180  acc: 68.7500%(22/32)\n",
            "Epoch [113] Batch[30040] - loss: 0.772481  acc: 68.7500%(22/32)\n",
            "Epoch [113] Batch[30060] - loss: 0.711382  acc: 75.0000%(24/32)\n",
            "Epoch [113] Batch[30080] - loss: 0.609430  acc: 84.3750%(27/32)\n",
            "Epoch [113] Batch[30100] - loss: 0.699255  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.286254  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [113] Batch[30120] - loss: 0.576128  acc: 81.2500%(26/32)\n",
            "Epoch [113] Batch[30140] - loss: 0.629429  acc: 68.7500%(22/32)\n",
            "Epoch [113] Batch[30160] - loss: 0.762119  acc: 75.0000%(24/32)\n",
            "Epoch [114] Batch[30180] - loss: 0.751177  acc: 81.2500%(26/32)\n",
            "Epoch [114] Batch[30200] - loss: 0.589753  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.287276  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [114] Batch[30220] - loss: 0.611990  acc: 78.1250%(25/32)\n",
            "Epoch [114] Batch[30240] - loss: 0.590196  acc: 78.1250%(25/32)\n",
            "Epoch [114] Batch[30260] - loss: 0.590489  acc: 84.3750%(27/32)\n",
            "Epoch [114] Batch[30280] - loss: 0.849731  acc: 62.5000%(20/32)\n",
            "Epoch [114] Batch[30300] - loss: 0.637479  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.291653  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [114] Batch[30320] - loss: 0.719049  acc: 65.6250%(21/32)\n",
            "Epoch [114] Batch[30340] - loss: 0.750367  acc: 78.1250%(25/32)\n",
            "Epoch [114] Batch[30360] - loss: 0.705510  acc: 75.0000%(24/32)\n",
            "Epoch [114] Batch[30380] - loss: 0.783197  acc: 78.1250%(25/32)\n",
            "Epoch [114] Batch[30400] - loss: 0.669864  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.286370  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [114] Batch[30420] - loss: 0.738398  acc: 81.2500%(26/32)\n",
            "Epoch [115] Batch[30440] - loss: 0.810448  acc: 71.8750%(23/32)\n",
            "Epoch [115] Batch[30460] - loss: 0.680950  acc: 81.2500%(26/32)\n",
            "Epoch [115] Batch[30480] - loss: 0.754946  acc: 71.8750%(23/32)\n",
            "Epoch [115] Batch[30500] - loss: 0.561658  acc: 93.7500%(30/32)\n",
            "\n",
            "Evaluation - loss: 1.285562  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [115] Batch[30520] - loss: 0.738580  acc: 81.2500%(26/32)\n",
            "Epoch [115] Batch[30540] - loss: 0.755515  acc: 81.2500%(26/32)\n",
            "Epoch [115] Batch[30560] - loss: 0.678066  acc: 71.8750%(23/32)\n",
            "Epoch [115] Batch[30580] - loss: 0.780347  acc: 68.7500%(22/32)\n",
            "Epoch [115] Batch[30600] - loss: 0.708822  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.288540  acc: 41.7802%(460/1101) \n",
            "\n",
            "Epoch [115] Batch[30620] - loss: 0.826179  acc: 71.8750%(23/32)\n",
            "Epoch [115] Batch[30640] - loss: 0.684120  acc: 71.8750%(23/32)\n",
            "Epoch [115] Batch[30660] - loss: 0.660036  acc: 87.5000%(28/32)\n",
            "Epoch [115] Batch[30680] - loss: 0.769163  acc: 68.7500%(22/32)\n",
            "Epoch [115] Batch[30700] - loss: 0.704047  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.288403  acc: 41.5077%(457/1101) \n",
            "\n",
            "Epoch [116] Batch[30720] - loss: 0.649125  acc: 78.1250%(25/32)\n",
            "Epoch [116] Batch[30740] - loss: 0.740087  acc: 78.1250%(25/32)\n",
            "Epoch [116] Batch[30760] - loss: 0.730023  acc: 81.2500%(26/32)\n",
            "Epoch [116] Batch[30780] - loss: 0.891477  acc: 65.6250%(21/32)\n",
            "Epoch [116] Batch[30800] - loss: 0.534646  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.290339  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [116] Batch[30820] - loss: 0.578455  acc: 93.7500%(30/32)\n",
            "Epoch [116] Batch[30840] - loss: 0.523719  acc: 96.8750%(31/32)\n",
            "Epoch [116] Batch[30860] - loss: 0.901494  acc: 56.2500%(18/32)\n",
            "Epoch [116] Batch[30880] - loss: 0.476848  acc: 90.6250%(29/32)\n",
            "Epoch [116] Batch[30900] - loss: 0.651010  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.289704  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [116] Batch[30920] - loss: 0.764901  acc: 71.8750%(23/32)\n",
            "Epoch [116] Batch[30940] - loss: 0.696944  acc: 84.3750%(27/32)\n",
            "Epoch [116] Batch[30960] - loss: 0.634500  acc: 90.6250%(29/32)\n",
            "Epoch [117] Batch[30980] - loss: 0.653293  acc: 84.3750%(27/32)\n",
            "Epoch [117] Batch[31000] - loss: 0.784892  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.288192  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [117] Batch[31020] - loss: 0.630284  acc: 84.3750%(27/32)\n",
            "Epoch [117] Batch[31040] - loss: 0.653835  acc: 81.2500%(26/32)\n",
            "Epoch [117] Batch[31060] - loss: 0.610649  acc: 81.2500%(26/32)\n",
            "Epoch [117] Batch[31080] - loss: 0.748690  acc: 84.3750%(27/32)\n",
            "Epoch [117] Batch[31100] - loss: 0.533698  acc: 93.7500%(30/32)\n",
            "\n",
            "Evaluation - loss: 1.291451  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [117] Batch[31120] - loss: 0.627955  acc: 90.6250%(29/32)\n",
            "Epoch [117] Batch[31140] - loss: 0.813094  acc: 75.0000%(24/32)\n",
            "Epoch [117] Batch[31160] - loss: 0.693023  acc: 81.2500%(26/32)\n",
            "Epoch [117] Batch[31180] - loss: 0.633946  acc: 81.2500%(26/32)\n",
            "Epoch [117] Batch[31200] - loss: 0.757727  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.290856  acc: 41.4169%(456/1101) \n",
            "\n",
            "Epoch [117] Batch[31220] - loss: 0.521251  acc: 90.6250%(29/32)\n",
            "Epoch [118] Batch[31240] - loss: 0.740399  acc: 78.1250%(25/32)\n",
            "Epoch [118] Batch[31260] - loss: 0.667822  acc: 75.0000%(24/32)\n",
            "Epoch [118] Batch[31280] - loss: 0.717045  acc: 84.3750%(27/32)\n",
            "Epoch [118] Batch[31300] - loss: 0.541635  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.287817  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [118] Batch[31320] - loss: 0.522375  acc: 87.5000%(28/32)\n",
            "Epoch [118] Batch[31340] - loss: 0.534962  acc: 93.7500%(30/32)\n",
            "Epoch [118] Batch[31360] - loss: 0.580664  acc: 78.1250%(25/32)\n",
            "Epoch [118] Batch[31380] - loss: 0.701288  acc: 87.5000%(28/32)\n",
            "Epoch [118] Batch[31400] - loss: 0.576982  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.295551  acc: 41.3261%(455/1101) \n",
            "\n",
            "Epoch [118] Batch[31420] - loss: 0.515584  acc: 87.5000%(28/32)\n",
            "Epoch [118] Batch[31440] - loss: 0.625335  acc: 81.2500%(26/32)\n",
            "Epoch [118] Batch[31460] - loss: 0.558280  acc: 84.3750%(27/32)\n",
            "Epoch [118] Batch[31480] - loss: 0.658230  acc: 78.1250%(25/32)\n",
            "Epoch [118] Batch[31500] - loss: 0.712056  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.289233  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [119] Batch[31520] - loss: 0.633071  acc: 81.2500%(26/32)\n",
            "Epoch [119] Batch[31540] - loss: 0.765902  acc: 78.1250%(25/32)\n",
            "Epoch [119] Batch[31560] - loss: 0.645153  acc: 87.5000%(28/32)\n",
            "Epoch [119] Batch[31580] - loss: 0.595686  acc: 84.3750%(27/32)\n",
            "Epoch [119] Batch[31600] - loss: 0.638654  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.289988  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [119] Batch[31620] - loss: 0.772055  acc: 71.8750%(23/32)\n",
            "Epoch [119] Batch[31640] - loss: 0.644048  acc: 78.1250%(25/32)\n",
            "Epoch [119] Batch[31660] - loss: 0.592537  acc: 93.7500%(30/32)\n",
            "Epoch [119] Batch[31680] - loss: 0.759766  acc: 71.8750%(23/32)\n",
            "Epoch [119] Batch[31700] - loss: 0.582006  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.290238  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [119] Batch[31720] - loss: 0.652452  acc: 84.3750%(27/32)\n",
            "Epoch [119] Batch[31740] - loss: 0.629805  acc: 84.3750%(27/32)\n",
            "Epoch [119] Batch[31760] - loss: 0.608753  acc: 90.6250%(29/32)\n",
            "Epoch [120] Batch[31780] - loss: 0.921760  acc: 68.7500%(22/32)\n",
            "Epoch [120] Batch[31800] - loss: 0.631760  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.291370  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [120] Batch[31820] - loss: 0.712589  acc: 84.3750%(27/32)\n",
            "Epoch [120] Batch[31840] - loss: 0.896264  acc: 68.7500%(22/32)\n",
            "Epoch [120] Batch[31860] - loss: 0.599969  acc: 84.3750%(27/32)\n",
            "Epoch [120] Batch[31880] - loss: 0.586645  acc: 81.2500%(26/32)\n",
            "Epoch [120] Batch[31900] - loss: 0.844772  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.289895  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [120] Batch[31920] - loss: 0.678483  acc: 84.3750%(27/32)\n",
            "Epoch [120] Batch[31940] - loss: 0.661594  acc: 81.2500%(26/32)\n",
            "Epoch [120] Batch[31960] - loss: 0.808287  acc: 78.1250%(25/32)\n",
            "Epoch [120] Batch[31980] - loss: 0.731221  acc: 75.0000%(24/32)\n",
            "Epoch [120] Batch[32000] - loss: 0.607353  acc: 90.6250%(29/32)\n",
            "\n",
            "Evaluation - loss: 1.289621  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [120] Batch[32020] - loss: 0.507223  acc: 90.6250%(29/32)\n",
            "Epoch [120] Batch[32040] - loss: 0.495433  acc: 96.8750%(31/32)\n",
            "Epoch [121] Batch[32060] - loss: 0.608623  acc: 81.2500%(26/32)\n",
            "Epoch [121] Batch[32080] - loss: 0.815280  acc: 71.8750%(23/32)\n",
            "Epoch [121] Batch[32100] - loss: 0.587041  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.292699  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [121] Batch[32120] - loss: 0.569899  acc: 87.5000%(28/32)\n",
            "Epoch [121] Batch[32140] - loss: 0.611834  acc: 93.7500%(30/32)\n",
            "Epoch [121] Batch[32160] - loss: 0.628772  acc: 87.5000%(28/32)\n",
            "Epoch [121] Batch[32180] - loss: 0.642202  acc: 81.2500%(26/32)\n",
            "Epoch [121] Batch[32200] - loss: 0.805860  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.293792  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [121] Batch[32220] - loss: 0.769298  acc: 75.0000%(24/32)\n",
            "Epoch [121] Batch[32240] - loss: 0.654442  acc: 84.3750%(27/32)\n",
            "Epoch [121] Batch[32260] - loss: 0.573007  acc: 90.6250%(29/32)\n",
            "Epoch [121] Batch[32280] - loss: 0.707180  acc: 81.2500%(26/32)\n",
            "Epoch [121] Batch[32300] - loss: 0.529289  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.290842  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [122] Batch[32320] - loss: 0.701790  acc: 78.1250%(25/32)\n",
            "Epoch [122] Batch[32340] - loss: 0.755871  acc: 84.3750%(27/32)\n",
            "Epoch [122] Batch[32360] - loss: 0.660995  acc: 87.5000%(28/32)\n",
            "Epoch [122] Batch[32380] - loss: 0.582074  acc: 87.5000%(28/32)\n",
            "Epoch [122] Batch[32400] - loss: 0.530182  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.292420  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [122] Batch[32420] - loss: 0.636935  acc: 87.5000%(28/32)\n",
            "Epoch [122] Batch[32440] - loss: 0.566240  acc: 87.5000%(28/32)\n",
            "Epoch [122] Batch[32460] - loss: 0.564614  acc: 87.5000%(28/32)\n",
            "Epoch [122] Batch[32480] - loss: 0.575337  acc: 87.5000%(28/32)\n",
            "Epoch [122] Batch[32500] - loss: 0.665149  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.292377  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [122] Batch[32520] - loss: 0.699730  acc: 75.0000%(24/32)\n",
            "Epoch [122] Batch[32540] - loss: 0.602032  acc: 93.7500%(30/32)\n",
            "Epoch [122] Batch[32560] - loss: 0.785925  acc: 75.0000%(24/32)\n",
            "Epoch [123] Batch[32580] - loss: 0.729128  acc: 71.8750%(23/32)\n",
            "Epoch [123] Batch[32600] - loss: 0.584574  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.292963  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [123] Batch[32620] - loss: 0.606779  acc: 84.3750%(27/32)\n",
            "Epoch [123] Batch[32640] - loss: 0.729808  acc: 78.1250%(25/32)\n",
            "Epoch [123] Batch[32660] - loss: 0.696593  acc: 81.2500%(26/32)\n",
            "Epoch [123] Batch[32680] - loss: 0.721251  acc: 75.0000%(24/32)\n",
            "Epoch [123] Batch[32700] - loss: 0.458538  acc: 93.7500%(30/32)\n",
            "\n",
            "Evaluation - loss: 1.293675  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [123] Batch[32720] - loss: 0.595596  acc: 81.2500%(26/32)\n",
            "Epoch [123] Batch[32740] - loss: 0.572525  acc: 90.6250%(29/32)\n",
            "Epoch [123] Batch[32760] - loss: 0.701439  acc: 90.6250%(29/32)\n",
            "Epoch [123] Batch[32780] - loss: 0.709014  acc: 75.0000%(24/32)\n",
            "Epoch [123] Batch[32800] - loss: 0.591807  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.294030  acc: 41.4169%(456/1101) \n",
            "\n",
            "Epoch [123] Batch[32820] - loss: 0.688429  acc: 81.2500%(26/32)\n",
            "Epoch [123] Batch[32840] - loss: 0.651679  acc: 78.1250%(25/32)\n",
            "Epoch [124] Batch[32860] - loss: 0.560168  acc: 84.3750%(27/32)\n",
            "Epoch [124] Batch[32880] - loss: 0.764422  acc: 68.7500%(22/32)\n",
            "Epoch [124] Batch[32900] - loss: 0.607957  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.294876  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [124] Batch[32920] - loss: 0.575539  acc: 84.3750%(27/32)\n",
            "Epoch [124] Batch[32940] - loss: 0.570340  acc: 81.2500%(26/32)\n",
            "Epoch [124] Batch[32960] - loss: 0.557648  acc: 84.3750%(27/32)\n",
            "Epoch [124] Batch[32980] - loss: 0.631812  acc: 81.2500%(26/32)\n",
            "Epoch [124] Batch[33000] - loss: 0.545054  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.295236  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [124] Batch[33020] - loss: 0.672203  acc: 84.3750%(27/32)\n",
            "Epoch [124] Batch[33040] - loss: 0.571695  acc: 93.7500%(30/32)\n",
            "Epoch [124] Batch[33060] - loss: 0.559379  acc: 90.6250%(29/32)\n",
            "Epoch [124] Batch[33080] - loss: 0.711596  acc: 78.1250%(25/32)\n",
            "Epoch [124] Batch[33100] - loss: 0.761055  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.295938  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [125] Batch[33120] - loss: 0.506550  acc: 90.6250%(29/32)\n",
            "Epoch [125] Batch[33140] - loss: 0.693462  acc: 75.0000%(24/32)\n",
            "Epoch [125] Batch[33160] - loss: 0.646434  acc: 87.5000%(28/32)\n",
            "Epoch [125] Batch[33180] - loss: 0.534201  acc: 90.6250%(29/32)\n",
            "Epoch [125] Batch[33200] - loss: 0.734572  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.294858  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [125] Batch[33220] - loss: 0.610427  acc: 90.6250%(29/32)\n",
            "Epoch [125] Batch[33240] - loss: 0.601875  acc: 81.2500%(26/32)\n",
            "Epoch [125] Batch[33260] - loss: 0.624569  acc: 81.2500%(26/32)\n",
            "Epoch [125] Batch[33280] - loss: 0.646898  acc: 81.2500%(26/32)\n",
            "Epoch [125] Batch[33300] - loss: 0.533610  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.295264  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [125] Batch[33320] - loss: 0.637493  acc: 90.6250%(29/32)\n",
            "Epoch [125] Batch[33340] - loss: 0.569803  acc: 84.3750%(27/32)\n",
            "Epoch [125] Batch[33360] - loss: 0.733545  acc: 75.0000%(24/32)\n",
            "Epoch [126] Batch[33380] - loss: 0.529732  acc: 90.6250%(29/32)\n",
            "Epoch [126] Batch[33400] - loss: 0.577394  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.295841  acc: 41.7802%(460/1101) \n",
            "\n",
            "Epoch [126] Batch[33420] - loss: 0.617002  acc: 81.2500%(26/32)\n",
            "Epoch [126] Batch[33440] - loss: 0.587434  acc: 87.5000%(28/32)\n",
            "Epoch [126] Batch[33460] - loss: 0.539281  acc: 84.3750%(27/32)\n",
            "Epoch [126] Batch[33480] - loss: 0.598104  acc: 87.5000%(28/32)\n",
            "Epoch [126] Batch[33500] - loss: 0.663408  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.296531  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [126] Batch[33520] - loss: 0.680946  acc: 81.2500%(26/32)\n",
            "Epoch [126] Batch[33540] - loss: 0.600286  acc: 84.3750%(27/32)\n",
            "Epoch [126] Batch[33560] - loss: 0.660513  acc: 84.3750%(27/32)\n",
            "Epoch [126] Batch[33580] - loss: 0.736315  acc: 65.6250%(21/32)\n",
            "Epoch [126] Batch[33600] - loss: 0.750359  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.297738  acc: 41.5077%(457/1101) \n",
            "\n",
            "Epoch [126] Batch[33620] - loss: 0.476487  acc: 100.0000%(32/32)\n",
            "Epoch [126] Batch[33640] - loss: 0.605511  acc: 78.1250%(25/32)\n",
            "Epoch [127] Batch[33660] - loss: 0.609217  acc: 93.7500%(30/32)\n",
            "Epoch [127] Batch[33680] - loss: 0.641961  acc: 81.2500%(26/32)\n",
            "Epoch [127] Batch[33700] - loss: 0.501433  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.298454  acc: 41.5077%(457/1101) \n",
            "\n",
            "Epoch [127] Batch[33720] - loss: 0.727324  acc: 78.1250%(25/32)\n",
            "Epoch [127] Batch[33740] - loss: 0.638873  acc: 75.0000%(24/32)\n",
            "Epoch [127] Batch[33760] - loss: 0.551651  acc: 87.5000%(28/32)\n",
            "Epoch [127] Batch[33780] - loss: 0.647028  acc: 81.2500%(26/32)\n",
            "Epoch [127] Batch[33800] - loss: 0.608172  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.295706  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [127] Batch[33820] - loss: 0.622064  acc: 81.2500%(26/32)\n",
            "Epoch [127] Batch[33840] - loss: 0.587833  acc: 84.3750%(27/32)\n",
            "Epoch [127] Batch[33860] - loss: 0.535298  acc: 87.5000%(28/32)\n",
            "Epoch [127] Batch[33880] - loss: 0.636730  acc: 84.3750%(27/32)\n",
            "Epoch [127] Batch[33900] - loss: 0.619277  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.296025  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [128] Batch[33920] - loss: 0.477463  acc: 93.7500%(30/32)\n",
            "Epoch [128] Batch[33940] - loss: 0.627028  acc: 90.6250%(29/32)\n",
            "Epoch [128] Batch[33960] - loss: 0.474968  acc: 96.8750%(31/32)\n",
            "Epoch [128] Batch[33980] - loss: 0.578837  acc: 90.6250%(29/32)\n",
            "Epoch [128] Batch[34000] - loss: 0.754048  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.297109  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [128] Batch[34020] - loss: 0.704245  acc: 75.0000%(24/32)\n",
            "Epoch [128] Batch[34040] - loss: 0.567588  acc: 90.6250%(29/32)\n",
            "Epoch [128] Batch[34060] - loss: 0.615380  acc: 81.2500%(26/32)\n",
            "Epoch [128] Batch[34080] - loss: 0.586373  acc: 84.3750%(27/32)\n",
            "Epoch [128] Batch[34100] - loss: 0.527263  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.299058  acc: 41.7802%(460/1101) \n",
            "\n",
            "Epoch [128] Batch[34120] - loss: 0.614781  acc: 81.2500%(26/32)\n",
            "Epoch [128] Batch[34140] - loss: 0.747930  acc: 81.2500%(26/32)\n",
            "Epoch [128] Batch[34160] - loss: 0.517798  acc: 90.6250%(29/32)\n",
            "Epoch [129] Batch[34180] - loss: 0.561763  acc: 81.2500%(26/32)\n",
            "Epoch [129] Batch[34200] - loss: 0.570688  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.298906  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [129] Batch[34220] - loss: 0.619797  acc: 75.0000%(24/32)\n",
            "Epoch [129] Batch[34240] - loss: 0.650983  acc: 81.2500%(26/32)\n",
            "Epoch [129] Batch[34260] - loss: 0.522357  acc: 93.7500%(30/32)\n",
            "Epoch [129] Batch[34280] - loss: 0.549933  acc: 87.5000%(28/32)\n",
            "Epoch [129] Batch[34300] - loss: 0.620694  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.297722  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [129] Batch[34320] - loss: 0.445622  acc: 87.5000%(28/32)\n",
            "Epoch [129] Batch[34340] - loss: 0.685646  acc: 87.5000%(28/32)\n",
            "Epoch [129] Batch[34360] - loss: 0.579675  acc: 84.3750%(27/32)\n",
            "Epoch [129] Batch[34380] - loss: 0.648020  acc: 84.3750%(27/32)\n",
            "Epoch [129] Batch[34400] - loss: 0.593945  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.297060  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [129] Batch[34420] - loss: 0.567487  acc: 81.2500%(26/32)\n",
            "Epoch [129] Batch[34440] - loss: 0.576814  acc: 84.3750%(27/32)\n",
            "Epoch [130] Batch[34460] - loss: 0.614274  acc: 78.1250%(25/32)\n",
            "Epoch [130] Batch[34480] - loss: 0.582622  acc: 84.3750%(27/32)\n",
            "Epoch [130] Batch[34500] - loss: 0.485137  acc: 90.6250%(29/32)\n",
            "\n",
            "Evaluation - loss: 1.298898  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [130] Batch[34520] - loss: 0.575483  acc: 90.6250%(29/32)\n",
            "Epoch [130] Batch[34540] - loss: 0.560306  acc: 87.5000%(28/32)\n",
            "Epoch [130] Batch[34560] - loss: 0.698168  acc: 81.2500%(26/32)\n",
            "Epoch [130] Batch[34580] - loss: 0.545489  acc: 84.3750%(27/32)\n",
            "Epoch [130] Batch[34600] - loss: 0.734088  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.299093  acc: 41.7802%(460/1101) \n",
            "\n",
            "Epoch [130] Batch[34620] - loss: 0.673392  acc: 87.5000%(28/32)\n",
            "Epoch [130] Batch[34640] - loss: 0.687207  acc: 75.0000%(24/32)\n",
            "Epoch [130] Batch[34660] - loss: 0.552793  acc: 87.5000%(28/32)\n",
            "Epoch [130] Batch[34680] - loss: 0.661226  acc: 78.1250%(25/32)\n",
            "Epoch [130] Batch[34700] - loss: 0.487445  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.299864  acc: 42.3252%(466/1101) \n",
            "\n",
            "Epoch [131] Batch[34720] - loss: 0.586412  acc: 81.2500%(26/32)\n",
            "Epoch [131] Batch[34740] - loss: 0.590858  acc: 84.3750%(27/32)\n",
            "Epoch [131] Batch[34760] - loss: 0.485085  acc: 87.5000%(28/32)\n",
            "Epoch [131] Batch[34780] - loss: 0.720788  acc: 81.2500%(26/32)\n",
            "Epoch [131] Batch[34800] - loss: 0.640559  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.301911  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [131] Batch[34820] - loss: 0.690361  acc: 84.3750%(27/32)\n",
            "Epoch [131] Batch[34840] - loss: 0.652082  acc: 75.0000%(24/32)\n",
            "Epoch [131] Batch[34860] - loss: 0.672393  acc: 81.2500%(26/32)\n",
            "Epoch [131] Batch[34880] - loss: 0.584795  acc: 81.2500%(26/32)\n",
            "Epoch [131] Batch[34900] - loss: 0.627131  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.299411  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [131] Batch[34920] - loss: 0.542750  acc: 90.6250%(29/32)\n",
            "Epoch [131] Batch[34940] - loss: 0.647986  acc: 81.2500%(26/32)\n",
            "Epoch [131] Batch[34960] - loss: 0.603989  acc: 81.2500%(26/32)\n",
            "Epoch [132] Batch[34980] - loss: 0.634869  acc: 68.7500%(22/32)\n",
            "Epoch [132] Batch[35000] - loss: 0.615804  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.300598  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [132] Batch[35020] - loss: 0.654433  acc: 81.2500%(26/32)\n",
            "Epoch [132] Batch[35040] - loss: 0.659741  acc: 81.2500%(26/32)\n",
            "Epoch [132] Batch[35060] - loss: 0.614512  acc: 87.5000%(28/32)\n",
            "Epoch [132] Batch[35080] - loss: 0.521353  acc: 93.7500%(30/32)\n",
            "Epoch [132] Batch[35100] - loss: 0.506692  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.300794  acc: 42.0527%(463/1101) \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f5474f74e2f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mis_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'early stop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: early stop"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlXAfooAhd-X",
        "colab_type": "code",
        "outputId": "d781fa09-0b75-4dbf-cc91-38521f169979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "best_model=torch.load('best_model.pt')\n",
        "best_model.eval()\n",
        "eval(test_iter, best_model, args)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation - loss: 1.249011  acc: 45.1584%(998/2210) \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45.158371040723985"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}