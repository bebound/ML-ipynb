{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textcnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hb6znk-DDmnu",
        "colab_type": "code",
        "outputId": "0c8fc2c1-f88e-4133-a517-e0e64c3434a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchtext numpy"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.3.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tscyOkDXESk5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchtext import data,datasets\n",
        "\n",
        "TEXT = data.Field(lower=True,batch_first=True)\n",
        "LABEL = data.Field(sequential=False)\n",
        "\n",
        "# make splits for data\n",
        "train, val, test = datasets.SST.splits(TEXT, LABEL, 'data/',fine_grained=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dNES61KHEinz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TEXT.build_vocab(train, vectors=\"fasttext.en.300d\")\n",
        "TEXT.build_vocab(train, vectors=\"glove.840B.300d\")\n",
        "LABEL.build_vocab(train,val,test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tsiDQBmDWj1N",
        "colab_type": "code",
        "outputId": "945581be-d3c9-4e21-cedc-a7341cf04c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
        "print(LABEL.vocab.itos)\n",
        "print('len(LABEL.vocab)', len(LABEL.vocab)-1)   # vocab include '<unk>'\n",
        "print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(TEXT.vocab) 16581\n",
            "['<unk>', 'negative', 'positive', 'neutral', 'very positive', 'very negative']\n",
            "len(LABEL.vocab) 5\n",
            "TEXT.vocab.vectors.size() torch.Size([16581, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B0ZHHoI3HtIJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "_DEBUG=False\n",
        "\n",
        "def ilog(*args,**kwargs):\n",
        "    if _DEBUG:\n",
        "        print(*args,**kwargs)\n",
        "    \n",
        "class textCNN(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super().__init__()\n",
        "        dim = args['dim']\n",
        "        n_class = args['n_class']\n",
        "        embedding_matrix=args['embedding_matrix']\n",
        "        kernels=[3,4,5]\n",
        "        kernel_number=[100,100,100]\n",
        "        self.embeding = nn.Embedding.from_pretrained(embedding_matrix)\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, number, (size, dim),padding=(size-1,0)) for (size,number) in zip(kernels,kernel_number)])\n",
        "        self.dropout=nn.Dropout()\n",
        "        self.out = nn.Linear(sum(kernel_number), n_class)\n",
        " \n",
        "    def forward(self, x):\n",
        "        ilog('ori input',x.size())\n",
        "        x = self.embeding(x)\n",
        "        ilog('after embeding',x.size())\n",
        "        x = x.unsqueeze(1)\n",
        "        ilog('unsqueeze',x.size())\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "        ilog(x[0].size())\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "    \n",
        "class textCNNMulti(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super().__init__()\n",
        "        dim = args['dim']\n",
        "        n_class = args['n_class']\n",
        "        embedding_matrix=args['embedding_matrix']\n",
        "        kernels=[3,4,5]\n",
        "        kernel_number=[100,100,100]\n",
        "        self.static_embed = nn.Embedding.from_pretrained(embedding_matrix)\n",
        "        self.non_static_embed = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(2, number, (size, dim),padding=(size-1,0)) for (size,number) in zip(kernels,kernel_number)])\n",
        "        self.dropout=nn.Dropout()\n",
        "        self.out = nn.Linear(sum(kernel_number), n_class)\n",
        " \n",
        "    def forward(self, x):\n",
        "        ilog('ori input',x.size())\n",
        "        non_static_input = self.non_static_embed(x)\n",
        "        static_input = self.static_embed(x)\n",
        "        x = torch.stack([non_static_input, static_input], dim=1)\n",
        "        ilog('after embeding',x.size())\n",
        "        ilog('unsqueeze',x.size())\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "        ilog(x[0].size())\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class textCNNNonStatic(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super().__init__()\n",
        "        dim = args['dim']\n",
        "        n_class = args['n_class']\n",
        "        embedding_matrix=args['embedding_matrix']\n",
        "        kernels=[3,4,5]\n",
        "        kernel_number=[100,100,100]\n",
        "        self.embeding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, number, (size, dim),padding=(size-1,0)) for (size,number) in zip(kernels,kernel_number)])\n",
        "        self.dropout=nn.Dropout()\n",
        "        self.out = nn.Linear(sum(kernel_number), n_class)\n",
        " \n",
        "    def forward(self, x):\n",
        "        ilog('ori input',x.size())\n",
        "        x = self.embeding(x)\n",
        "        ilog('after embeding',x.size())\n",
        "        x = x.unsqueeze(1)\n",
        "        ilog('unsqueeze',x.size())\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "        ilog(x[0].size())\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vtMDFmnqqVFK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train, val, test), batch_sizes=(32, 256, 256),shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bwZ494LkqwVc",
        "colab_type": "code",
        "outputId": "e99cf0aa-b7fd-4d41-a80c-08010086cbfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "args={}\n",
        "args['vocb_size']=len(TEXT.vocab)\n",
        "args['dim']=300\n",
        "args['n_class']=len(LABEL.vocab)-1\n",
        "args['embedding_matrix']=TEXT.vocab.vectors\n",
        "args['lr']=1e-5\n",
        "args['epochs']=400\n",
        "args['log_interval']=20\n",
        "args['test_interval']=100\n",
        "args['save_dir']='./'\n",
        "\n",
        "print(args['vocb_size'])\n",
        "print(args['n_class'])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16581\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "19sOr41FrGWT",
        "colab_type": "code",
        "outputId": "bab792b7-fdf5-4bbe-bd62-4ee0c4d47277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1)\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f147899ce50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "qR-sHoABrMg3",
        "colab_type": "code",
        "outputId": "58a2fcd3-720e-4607-88b3-56547b53e635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51184
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "\n",
        "def save(model, save_dir, save_prefix, steps):\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    save_prefix = os.path.join(save_dir, save_prefix)\n",
        "    save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "\n",
        "model=textCNNMulti(args)\n",
        "model.cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "best_acc = 0\n",
        "last_step = 0\n",
        "model.train()\n",
        "steps=0\n",
        "\n",
        "\n",
        "def create_early_stopping(patience):\n",
        "    recent_metric = deque(maxlen=patience)\n",
        "    best_metric = None\n",
        "\n",
        "    def check(metric, model):\n",
        "        nonlocal best_metric\n",
        "        is_stop = False\n",
        "        if not best_metric or metric > best_metric:\n",
        "            print('save best_model.pt, metric: {}'.format(metric))\n",
        "            best_metric = metric\n",
        "            torch.save(model, 'best_model.pt')\n",
        "\n",
        "        recent_metric.append(metric)\n",
        "\n",
        "        if all([i < best_metric for i in recent_metric]):\n",
        "            is_stop = True\n",
        "        return is_stop\n",
        "\n",
        "    return check\n",
        "\n",
        "\n",
        "def eval(data_iter, model, args):\n",
        "    model.eval()\n",
        "    corrects, avg_loss = 0, 0\n",
        "    for i,data in enumerate(data_iter):\n",
        "        x, target = data.text, data.label\n",
        "        x=x.cuda()\n",
        " \n",
        "        target.sub_(1)\n",
        "        target=target.cuda()\n",
        "\n",
        "        logit = model(x)\n",
        "        loss = F.cross_entropy(logit, target, reduction='sum')\n",
        "\n",
        "        avg_loss += loss.item()\n",
        "        corrects += (torch.max(logit, 1)\n",
        "                     [1].view(target.size()).data == target.data).sum()\n",
        "\n",
        "    size = len(data_iter.dataset)\n",
        "    avg_loss /= size\n",
        "    accuracy = 100.0 * int(corrects)/size\n",
        "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, \n",
        "                                                                       accuracy, \n",
        "                                                                       corrects, \n",
        "                                                                       size))\n",
        "    model.train()\n",
        "    return accuracy\n",
        "\n",
        "early_stop = create_early_stopping(150)\n",
        "\n",
        "for epoch in range(1, args['epochs']+1):\n",
        "    for i,data in enumerate(train_iter):\n",
        "        steps+=1\n",
        "\n",
        "        x, target = data.text, data.label\n",
        "        x=x.cuda()\n",
        "\n",
        "\n",
        "        target.sub_(1)\n",
        "        target=target.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if steps % args['log_interval'] == 0:\n",
        "            corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            accuracy = 100.0 * int(corrects)/data.batch_size\n",
        "            print(\n",
        "                'Epoch [{}] Batch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(epoch,\n",
        "                                                                         steps, \n",
        "                                                                         loss.item(), \n",
        "                                                                         accuracy,\n",
        "                                                                         corrects,\n",
        "                                                                         data.batch_size))\n",
        "        if steps % args['test_interval'] == 0:\n",
        "            val_acc = eval(val_iter, model, args)\n",
        "            is_stop = early_stop(val_acc, model)\n",
        "            if is_stop:\n",
        "                raise RuntimeError('early stop')\n",
        "\n",
        "        model.train()\n",
        "print('final_result')\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1] Batch[20] - loss: 1.640343  acc: 12.5000%(4/32)\n",
            "Epoch [1] Batch[40] - loss: 1.576709  acc: 37.5000%(12/32)\n",
            "Epoch [1] Batch[60] - loss: 1.643213  acc: 21.8750%(7/32)\n",
            "Epoch [1] Batch[80] - loss: 1.598190  acc: 21.8750%(7/32)\n",
            "Epoch [1] Batch[100] - loss: 1.499537  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.575289  acc: 27.0663%(298/1101) \n",
            "\n",
            "save best_model.pt, metric: 27.06630336058129\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type textCNNMulti. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1] Batch[120] - loss: 1.567643  acc: 34.3750%(11/32)\n",
            "Epoch [1] Batch[140] - loss: 1.636653  acc: 28.1250%(9/32)\n",
            "Epoch [1] Batch[160] - loss: 1.562044  acc: 31.2500%(10/32)\n",
            "Epoch [1] Batch[180] - loss: 1.609856  acc: 21.8750%(7/32)\n",
            "Epoch [1] Batch[200] - loss: 1.585996  acc: 21.8750%(7/32)\n",
            "\n",
            "Evaluation - loss: 1.562163  acc: 28.3379%(312/1101) \n",
            "\n",
            "save best_model.pt, metric: 28.337874659400544\n",
            "Epoch [1] Batch[220] - loss: 1.602201  acc: 18.7500%(6/32)\n",
            "Epoch [1] Batch[240] - loss: 1.570731  acc: 18.7500%(6/32)\n",
            "Epoch [1] Batch[260] - loss: 1.574784  acc: 25.0000%(8/32)\n",
            "Epoch [2] Batch[280] - loss: 1.561691  acc: 18.7500%(6/32)\n",
            "Epoch [2] Batch[300] - loss: 1.584436  acc: 31.2500%(10/32)\n",
            "\n",
            "Evaluation - loss: 1.554398  acc: 30.7902%(339/1101) \n",
            "\n",
            "save best_model.pt, metric: 30.79019073569482\n",
            "Epoch [2] Batch[320] - loss: 1.520215  acc: 31.2500%(10/32)\n",
            "Epoch [2] Batch[340] - loss: 1.422460  acc: 50.0000%(16/32)\n",
            "Epoch [2] Batch[360] - loss: 1.509055  acc: 40.6250%(13/32)\n",
            "Epoch [2] Batch[380] - loss: 1.544285  acc: 25.0000%(8/32)\n",
            "Epoch [2] Batch[400] - loss: 1.551106  acc: 28.1250%(9/32)\n",
            "\n",
            "Evaluation - loss: 1.548252  acc: 31.9709%(352/1101) \n",
            "\n",
            "save best_model.pt, metric: 31.970935513169845\n",
            "Epoch [2] Batch[420] - loss: 1.601420  acc: 15.6250%(5/32)\n",
            "Epoch [2] Batch[440] - loss: 1.583800  acc: 28.1250%(9/32)\n",
            "Epoch [2] Batch[460] - loss: 1.515034  acc: 31.2500%(10/32)\n",
            "Epoch [2] Batch[480] - loss: 1.533045  acc: 21.8750%(7/32)\n",
            "Epoch [2] Batch[500] - loss: 1.564909  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.541556  acc: 32.8792%(362/1101) \n",
            "\n",
            "save best_model.pt, metric: 32.87920072661217\n",
            "Epoch [2] Batch[520] - loss: 1.532541  acc: 34.3750%(11/32)\n",
            "Epoch [3] Batch[540] - loss: 1.539724  acc: 37.5000%(12/32)\n",
            "Epoch [3] Batch[560] - loss: 1.528396  acc: 31.2500%(10/32)\n",
            "Epoch [3] Batch[580] - loss: 1.570210  acc: 34.3750%(11/32)\n",
            "Epoch [3] Batch[600] - loss: 1.635533  acc: 25.0000%(8/32)\n",
            "\n",
            "Evaluation - loss: 1.535984  acc: 34.7866%(383/1101) \n",
            "\n",
            "save best_model.pt, metric: 34.786557674841056\n",
            "Epoch [3] Batch[620] - loss: 1.508496  acc: 25.0000%(8/32)\n",
            "Epoch [3] Batch[640] - loss: 1.556430  acc: 21.8750%(7/32)\n",
            "Epoch [3] Batch[660] - loss: 1.569755  acc: 34.3750%(11/32)\n",
            "Epoch [3] Batch[680] - loss: 1.542114  acc: 37.5000%(12/32)\n",
            "Epoch [3] Batch[700] - loss: 1.487282  acc: 31.2500%(10/32)\n",
            "\n",
            "Evaluation - loss: 1.530010  acc: 34.6049%(381/1101) \n",
            "\n",
            "Epoch [3] Batch[720] - loss: 1.524402  acc: 34.3750%(11/32)\n",
            "Epoch [3] Batch[740] - loss: 1.643247  acc: 12.5000%(4/32)\n",
            "Epoch [3] Batch[760] - loss: 1.514469  acc: 31.2500%(10/32)\n",
            "Epoch [3] Batch[780] - loss: 1.538235  acc: 40.6250%(13/32)\n",
            "Epoch [3] Batch[800] - loss: 1.518986  acc: 31.2500%(10/32)\n",
            "\n",
            "Evaluation - loss: 1.523876  acc: 35.1499%(387/1101) \n",
            "\n",
            "save best_model.pt, metric: 35.14986376021798\n",
            "Epoch [4] Batch[820] - loss: 1.489707  acc: 40.6250%(13/32)\n",
            "Epoch [4] Batch[840] - loss: 1.483738  acc: 37.5000%(12/32)\n",
            "Epoch [4] Batch[860] - loss: 1.502227  acc: 25.0000%(8/32)\n",
            "Epoch [4] Batch[880] - loss: 1.456862  acc: 25.0000%(8/32)\n",
            "Epoch [4] Batch[900] - loss: 1.512714  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.518339  acc: 35.5132%(391/1101) \n",
            "\n",
            "save best_model.pt, metric: 35.513169845594916\n",
            "Epoch [4] Batch[920] - loss: 1.508987  acc: 34.3750%(11/32)\n",
            "Epoch [4] Batch[940] - loss: 1.543918  acc: 40.6250%(13/32)\n",
            "Epoch [4] Batch[960] - loss: 1.373812  acc: 53.1250%(17/32)\n",
            "Epoch [4] Batch[980] - loss: 1.544093  acc: 34.3750%(11/32)\n",
            "Epoch [4] Batch[1000] - loss: 1.577917  acc: 31.2500%(10/32)\n",
            "\n",
            "Evaluation - loss: 1.512822  acc: 35.4223%(390/1101) \n",
            "\n",
            "Epoch [4] Batch[1020] - loss: 1.429838  acc: 40.6250%(13/32)\n",
            "Epoch [4] Batch[1040] - loss: 1.496782  acc: 25.0000%(8/32)\n",
            "Epoch [4] Batch[1060] - loss: 1.509052  acc: 31.2500%(10/32)\n",
            "Epoch [5] Batch[1080] - loss: 1.454249  acc: 53.1250%(17/32)\n",
            "Epoch [5] Batch[1100] - loss: 1.640175  acc: 15.6250%(5/32)\n",
            "\n",
            "Evaluation - loss: 1.506378  acc: 35.6948%(393/1101) \n",
            "\n",
            "save best_model.pt, metric: 35.694822888283376\n",
            "Epoch [5] Batch[1120] - loss: 1.453562  acc: 28.1250%(9/32)\n",
            "Epoch [5] Batch[1140] - loss: 1.429719  acc: 46.8750%(15/32)\n",
            "Epoch [5] Batch[1160] - loss: 1.451024  acc: 31.2500%(10/32)\n",
            "Epoch [5] Batch[1180] - loss: 1.471440  acc: 31.2500%(10/32)\n",
            "Epoch [5] Batch[1200] - loss: 1.463832  acc: 31.2500%(10/32)\n",
            "\n",
            "Evaluation - loss: 1.500874  acc: 35.8765%(395/1101) \n",
            "\n",
            "save best_model.pt, metric: 35.87647593097184\n",
            "Epoch [5] Batch[1220] - loss: 1.469868  acc: 37.5000%(12/32)\n",
            "Epoch [5] Batch[1240] - loss: 1.354711  acc: 46.8750%(15/32)\n",
            "Epoch [5] Batch[1260] - loss: 1.485523  acc: 34.3750%(11/32)\n",
            "Epoch [5] Batch[1280] - loss: 1.416001  acc: 37.5000%(12/32)\n",
            "Epoch [5] Batch[1300] - loss: 1.490791  acc: 25.0000%(8/32)\n",
            "\n",
            "Evaluation - loss: 1.495287  acc: 35.9673%(396/1101) \n",
            "\n",
            "save best_model.pt, metric: 35.967302452316076\n",
            "Epoch [5] Batch[1320] - loss: 1.470372  acc: 28.1250%(9/32)\n",
            "Epoch [6] Batch[1340] - loss: 1.488301  acc: 31.2500%(10/32)\n",
            "Epoch [6] Batch[1360] - loss: 1.563099  acc: 28.1250%(9/32)\n",
            "Epoch [6] Batch[1380] - loss: 1.467677  acc: 37.5000%(12/32)\n",
            "Epoch [6] Batch[1400] - loss: 1.519557  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.489384  acc: 36.0581%(397/1101) \n",
            "\n",
            "save best_model.pt, metric: 36.05812897366031\n",
            "Epoch [6] Batch[1420] - loss: 1.449694  acc: 34.3750%(11/32)\n",
            "Epoch [6] Batch[1440] - loss: 1.419939  acc: 43.7500%(14/32)\n",
            "Epoch [6] Batch[1460] - loss: 1.478782  acc: 31.2500%(10/32)\n",
            "Epoch [6] Batch[1480] - loss: 1.497242  acc: 37.5000%(12/32)\n",
            "Epoch [6] Batch[1500] - loss: 1.537308  acc: 34.3750%(11/32)\n",
            "\n",
            "Evaluation - loss: 1.483996  acc: 35.9673%(396/1101) \n",
            "\n",
            "Epoch [6] Batch[1520] - loss: 1.477836  acc: 40.6250%(13/32)\n",
            "Epoch [6] Batch[1540] - loss: 1.445063  acc: 31.2500%(10/32)\n",
            "Epoch [6] Batch[1560] - loss: 1.502520  acc: 37.5000%(12/32)\n",
            "Epoch [6] Batch[1580] - loss: 1.382931  acc: 50.0000%(16/32)\n",
            "Epoch [6] Batch[1600] - loss: 1.450672  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.478011  acc: 37.3297%(411/1101) \n",
            "\n",
            "save best_model.pt, metric: 37.32970027247956\n",
            "Epoch [7] Batch[1620] - loss: 1.417679  acc: 43.7500%(14/32)\n",
            "Epoch [7] Batch[1640] - loss: 1.388628  acc: 46.8750%(15/32)\n",
            "Epoch [7] Batch[1660] - loss: 1.479304  acc: 34.3750%(11/32)\n",
            "Epoch [7] Batch[1680] - loss: 1.466731  acc: 37.5000%(12/32)\n",
            "Epoch [7] Batch[1700] - loss: 1.463874  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.472989  acc: 37.5114%(413/1101) \n",
            "\n",
            "save best_model.pt, metric: 37.51135331516803\n",
            "Epoch [7] Batch[1720] - loss: 1.467566  acc: 28.1250%(9/32)\n",
            "Epoch [7] Batch[1740] - loss: 1.544265  acc: 25.0000%(8/32)\n",
            "Epoch [7] Batch[1760] - loss: 1.438711  acc: 37.5000%(12/32)\n",
            "Epoch [7] Batch[1780] - loss: 1.404578  acc: 37.5000%(12/32)\n",
            "Epoch [7] Batch[1800] - loss: 1.424756  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.466904  acc: 36.9664%(407/1101) \n",
            "\n",
            "Epoch [7] Batch[1820] - loss: 1.370027  acc: 40.6250%(13/32)\n",
            "Epoch [7] Batch[1840] - loss: 1.359386  acc: 50.0000%(16/32)\n",
            "Epoch [7] Batch[1860] - loss: 1.508106  acc: 31.2500%(10/32)\n",
            "Epoch [8] Batch[1880] - loss: 1.321537  acc: 59.3750%(19/32)\n",
            "Epoch [8] Batch[1900] - loss: 1.351835  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.461115  acc: 37.1480%(409/1101) \n",
            "\n",
            "Epoch [8] Batch[1920] - loss: 1.474400  acc: 34.3750%(11/32)\n",
            "Epoch [8] Batch[1940] - loss: 1.444966  acc: 46.8750%(15/32)\n",
            "Epoch [8] Batch[1960] - loss: 1.458003  acc: 40.6250%(13/32)\n",
            "Epoch [8] Batch[1980] - loss: 1.435114  acc: 40.6250%(13/32)\n",
            "Epoch [8] Batch[2000] - loss: 1.357232  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.455754  acc: 38.0563%(419/1101) \n",
            "\n",
            "save best_model.pt, metric: 38.05631244323342\n",
            "Epoch [8] Batch[2020] - loss: 1.357412  acc: 50.0000%(16/32)\n",
            "Epoch [8] Batch[2040] - loss: 1.385172  acc: 37.5000%(12/32)\n",
            "Epoch [8] Batch[2060] - loss: 1.464811  acc: 34.3750%(11/32)\n",
            "Epoch [8] Batch[2080] - loss: 1.484853  acc: 31.2500%(10/32)\n",
            "Epoch [8] Batch[2100] - loss: 1.387645  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.450707  acc: 38.2380%(421/1101) \n",
            "\n",
            "save best_model.pt, metric: 38.23796548592189\n",
            "Epoch [8] Batch[2120] - loss: 1.486952  acc: 37.5000%(12/32)\n",
            "Epoch [9] Batch[2140] - loss: 1.320826  acc: 46.8750%(15/32)\n",
            "Epoch [9] Batch[2160] - loss: 1.480453  acc: 43.7500%(14/32)\n",
            "Epoch [9] Batch[2180] - loss: 1.444106  acc: 40.6250%(13/32)\n",
            "Epoch [9] Batch[2200] - loss: 1.311561  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.446099  acc: 38.3288%(422/1101) \n",
            "\n",
            "save best_model.pt, metric: 38.328792007266124\n",
            "Epoch [9] Batch[2220] - loss: 1.331897  acc: 46.8750%(15/32)\n",
            "Epoch [9] Batch[2240] - loss: 1.398313  acc: 43.7500%(14/32)\n",
            "Epoch [9] Batch[2260] - loss: 1.465662  acc: 37.5000%(12/32)\n",
            "Epoch [9] Batch[2280] - loss: 1.356992  acc: 50.0000%(16/32)\n",
            "Epoch [9] Batch[2300] - loss: 1.420059  acc: 37.5000%(12/32)\n",
            "\n",
            "Evaluation - loss: 1.440731  acc: 39.0554%(430/1101) \n",
            "\n",
            "save best_model.pt, metric: 39.055404178019984\n",
            "Epoch [9] Batch[2320] - loss: 1.487507  acc: 31.2500%(10/32)\n",
            "Epoch [9] Batch[2340] - loss: 1.419718  acc: 50.0000%(16/32)\n",
            "Epoch [9] Batch[2360] - loss: 1.429167  acc: 56.2500%(18/32)\n",
            "Epoch [9] Batch[2380] - loss: 1.295876  acc: 40.6250%(13/32)\n",
            "Epoch [9] Batch[2400] - loss: 1.520493  acc: 21.8750%(7/32)\n",
            "\n",
            "Evaluation - loss: 1.435283  acc: 38.6921%(426/1101) \n",
            "\n",
            "Epoch [10] Batch[2420] - loss: 1.259923  acc: 56.2500%(18/32)\n",
            "Epoch [10] Batch[2440] - loss: 1.351400  acc: 40.6250%(13/32)\n",
            "Epoch [10] Batch[2460] - loss: 1.363561  acc: 40.6250%(13/32)\n",
            "Epoch [10] Batch[2480] - loss: 1.365589  acc: 43.7500%(14/32)\n",
            "Epoch [10] Batch[2500] - loss: 1.476721  acc: 34.3750%(11/32)\n",
            "\n",
            "Evaluation - loss: 1.430782  acc: 38.5104%(424/1101) \n",
            "\n",
            "Epoch [10] Batch[2520] - loss: 1.466694  acc: 40.6250%(13/32)\n",
            "Epoch [10] Batch[2540] - loss: 1.372497  acc: 50.0000%(16/32)\n",
            "Epoch [10] Batch[2560] - loss: 1.430255  acc: 37.5000%(12/32)\n",
            "Epoch [10] Batch[2580] - loss: 1.429883  acc: 40.6250%(13/32)\n",
            "Epoch [10] Batch[2600] - loss: 1.225987  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.425467  acc: 39.2371%(432/1101) \n",
            "\n",
            "save best_model.pt, metric: 39.237057220708444\n",
            "Epoch [10] Batch[2620] - loss: 1.544090  acc: 34.3750%(11/32)\n",
            "Epoch [10] Batch[2640] - loss: 1.437841  acc: 34.3750%(11/32)\n",
            "Epoch [10] Batch[2660] - loss: 1.441400  acc: 31.2500%(10/32)\n",
            "Epoch [11] Batch[2680] - loss: 1.319635  acc: 43.7500%(14/32)\n",
            "Epoch [11] Batch[2700] - loss: 1.397251  acc: 37.5000%(12/32)\n",
            "\n",
            "Evaluation - loss: 1.421579  acc: 39.5095%(435/1101) \n",
            "\n",
            "save best_model.pt, metric: 39.509536784741144\n",
            "Epoch [11] Batch[2720] - loss: 1.337667  acc: 46.8750%(15/32)\n",
            "Epoch [11] Batch[2740] - loss: 1.401177  acc: 40.6250%(13/32)\n",
            "Epoch [11] Batch[2760] - loss: 1.370223  acc: 46.8750%(15/32)\n",
            "Epoch [11] Batch[2780] - loss: 1.446656  acc: 31.2500%(10/32)\n",
            "Epoch [11] Batch[2800] - loss: 1.378813  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.416496  acc: 39.2371%(432/1101) \n",
            "\n",
            "Epoch [11] Batch[2820] - loss: 1.351296  acc: 53.1250%(17/32)\n",
            "Epoch [11] Batch[2840] - loss: 1.466924  acc: 37.5000%(12/32)\n",
            "Epoch [11] Batch[2860] - loss: 1.387804  acc: 31.2500%(10/32)\n",
            "Epoch [11] Batch[2880] - loss: 1.291961  acc: 53.1250%(17/32)\n",
            "Epoch [11] Batch[2900] - loss: 1.403974  acc: 34.3750%(11/32)\n",
            "\n",
            "Evaluation - loss: 1.410663  acc: 39.8728%(439/1101) \n",
            "\n",
            "save best_model.pt, metric: 39.87284287011808\n",
            "Epoch [11] Batch[2920] - loss: 1.462540  acc: 28.1250%(9/32)\n",
            "Epoch [12] Batch[2940] - loss: 1.402539  acc: 34.3750%(11/32)\n",
            "Epoch [12] Batch[2960] - loss: 1.338305  acc: 43.7500%(14/32)\n",
            "Epoch [12] Batch[2980] - loss: 1.362303  acc: 50.0000%(16/32)\n",
            "Epoch [12] Batch[3000] - loss: 1.280270  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.407132  acc: 39.9637%(440/1101) \n",
            "\n",
            "save best_model.pt, metric: 39.963669391462304\n",
            "Epoch [12] Batch[3020] - loss: 1.427755  acc: 37.5000%(12/32)\n",
            "Epoch [12] Batch[3040] - loss: 1.409216  acc: 28.1250%(9/32)\n",
            "Epoch [12] Batch[3060] - loss: 1.460889  acc: 40.6250%(13/32)\n",
            "Epoch [12] Batch[3080] - loss: 1.313670  acc: 53.1250%(17/32)\n",
            "Epoch [12] Batch[3100] - loss: 1.422235  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.403325  acc: 39.6912%(437/1101) \n",
            "\n",
            "Epoch [12] Batch[3120] - loss: 1.410962  acc: 31.2500%(10/32)\n",
            "Epoch [12] Batch[3140] - loss: 1.332193  acc: 43.7500%(14/32)\n",
            "Epoch [12] Batch[3160] - loss: 1.220141  acc: 62.5000%(20/32)\n",
            "Epoch [12] Batch[3180] - loss: 1.337285  acc: 43.7500%(14/32)\n",
            "Epoch [12] Batch[3200] - loss: 1.330503  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.398724  acc: 39.9637%(440/1101) \n",
            "\n",
            "Epoch [13] Batch[3220] - loss: 1.354611  acc: 43.7500%(14/32)\n",
            "Epoch [13] Batch[3240] - loss: 1.222895  acc: 46.8750%(15/32)\n",
            "Epoch [13] Batch[3260] - loss: 1.431426  acc: 34.3750%(11/32)\n",
            "Epoch [13] Batch[3280] - loss: 1.337062  acc: 40.6250%(13/32)\n",
            "Epoch [13] Batch[3300] - loss: 1.314945  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.395155  acc: 39.8728%(439/1101) \n",
            "\n",
            "Epoch [13] Batch[3320] - loss: 1.292597  acc: 59.3750%(19/32)\n",
            "Epoch [13] Batch[3340] - loss: 1.322136  acc: 40.6250%(13/32)\n",
            "Epoch [13] Batch[3360] - loss: 1.374041  acc: 40.6250%(13/32)\n",
            "Epoch [13] Batch[3380] - loss: 1.262185  acc: 43.7500%(14/32)\n",
            "Epoch [13] Batch[3400] - loss: 1.273830  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.391800  acc: 40.3270%(444/1101) \n",
            "\n",
            "save best_model.pt, metric: 40.32697547683924\n",
            "Epoch [13] Batch[3420] - loss: 1.281788  acc: 50.0000%(16/32)\n",
            "Epoch [13] Batch[3440] - loss: 1.469614  acc: 28.1250%(9/32)\n",
            "Epoch [13] Batch[3460] - loss: 1.356421  acc: 34.3750%(11/32)\n",
            "Epoch [14] Batch[3480] - loss: 1.368416  acc: 37.5000%(12/32)\n",
            "Epoch [14] Batch[3500] - loss: 1.311975  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.386769  acc: 39.8728%(439/1101) \n",
            "\n",
            "Epoch [14] Batch[3520] - loss: 1.324128  acc: 50.0000%(16/32)\n",
            "Epoch [14] Batch[3540] - loss: 1.337171  acc: 46.8750%(15/32)\n",
            "Epoch [14] Batch[3560] - loss: 1.455707  acc: 25.0000%(8/32)\n",
            "Epoch [14] Batch[3580] - loss: 1.434056  acc: 40.6250%(13/32)\n",
            "Epoch [14] Batch[3600] - loss: 1.425392  acc: 34.3750%(11/32)\n",
            "\n",
            "Evaluation - loss: 1.384024  acc: 40.0545%(441/1101) \n",
            "\n",
            "Epoch [14] Batch[3620] - loss: 1.371071  acc: 43.7500%(14/32)\n",
            "Epoch [14] Batch[3640] - loss: 1.279153  acc: 56.2500%(18/32)\n",
            "Epoch [14] Batch[3660] - loss: 1.368926  acc: 37.5000%(12/32)\n",
            "Epoch [14] Batch[3680] - loss: 1.366627  acc: 50.0000%(16/32)\n",
            "Epoch [14] Batch[3700] - loss: 1.371390  acc: 31.2500%(10/32)\n",
            "\n",
            "Evaluation - loss: 1.380579  acc: 40.3270%(444/1101) \n",
            "\n",
            "Epoch [14] Batch[3720] - loss: 1.440847  acc: 34.3750%(11/32)\n",
            "Epoch [15] Batch[3740] - loss: 1.388803  acc: 43.7500%(14/32)\n",
            "Epoch [15] Batch[3760] - loss: 1.265681  acc: 59.3750%(19/32)\n",
            "Epoch [15] Batch[3780] - loss: 1.298026  acc: 40.6250%(13/32)\n",
            "Epoch [15] Batch[3800] - loss: 1.498787  acc: 25.0000%(8/32)\n",
            "\n",
            "Evaluation - loss: 1.376378  acc: 40.6903%(448/1101) \n",
            "\n",
            "save best_model.pt, metric: 40.690281562216164\n",
            "Epoch [15] Batch[3820] - loss: 1.371391  acc: 43.7500%(14/32)\n",
            "Epoch [15] Batch[3840] - loss: 1.254094  acc: 56.2500%(18/32)\n",
            "Epoch [15] Batch[3860] - loss: 1.297036  acc: 43.7500%(14/32)\n",
            "Epoch [15] Batch[3880] - loss: 1.263220  acc: 62.5000%(20/32)\n",
            "Epoch [15] Batch[3900] - loss: 1.269067  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.373150  acc: 40.4178%(445/1101) \n",
            "\n",
            "Epoch [15] Batch[3920] - loss: 1.426551  acc: 31.2500%(10/32)\n",
            "Epoch [15] Batch[3940] - loss: 1.351603  acc: 53.1250%(17/32)\n",
            "Epoch [15] Batch[3960] - loss: 1.307651  acc: 34.3750%(11/32)\n",
            "Epoch [15] Batch[3980] - loss: 1.298571  acc: 43.7500%(14/32)\n",
            "Epoch [15] Batch[4000] - loss: 1.230782  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.370020  acc: 40.6903%(448/1101) \n",
            "\n",
            "Epoch [16] Batch[4020] - loss: 1.255015  acc: 50.0000%(16/32)\n",
            "Epoch [16] Batch[4040] - loss: 1.331429  acc: 43.7500%(14/32)\n",
            "Epoch [16] Batch[4060] - loss: 1.334363  acc: 40.6250%(13/32)\n",
            "Epoch [16] Batch[4080] - loss: 1.299539  acc: 50.0000%(16/32)\n",
            "Epoch [16] Batch[4100] - loss: 1.282653  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.367492  acc: 40.8719%(450/1101) \n",
            "\n",
            "save best_model.pt, metric: 40.87193460490463\n",
            "Epoch [16] Batch[4120] - loss: 1.311354  acc: 43.7500%(14/32)\n",
            "Epoch [16] Batch[4140] - loss: 1.258019  acc: 50.0000%(16/32)\n",
            "Epoch [16] Batch[4160] - loss: 1.294842  acc: 43.7500%(14/32)\n",
            "Epoch [16] Batch[4180] - loss: 1.274558  acc: 50.0000%(16/32)\n",
            "Epoch [16] Batch[4200] - loss: 1.333650  acc: 37.5000%(12/32)\n",
            "\n",
            "Evaluation - loss: 1.363928  acc: 40.4178%(445/1101) \n",
            "\n",
            "Epoch [16] Batch[4220] - loss: 1.373564  acc: 43.7500%(14/32)\n",
            "Epoch [16] Batch[4240] - loss: 1.394076  acc: 34.3750%(11/32)\n",
            "Epoch [16] Batch[4260] - loss: 1.156714  acc: 65.6250%(21/32)\n",
            "Epoch [17] Batch[4280] - loss: 1.294559  acc: 43.7500%(14/32)\n",
            "Epoch [17] Batch[4300] - loss: 1.228761  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.361186  acc: 40.5995%(447/1101) \n",
            "\n",
            "Epoch [17] Batch[4320] - loss: 1.249514  acc: 50.0000%(16/32)\n",
            "Epoch [17] Batch[4340] - loss: 1.262145  acc: 40.6250%(13/32)\n",
            "Epoch [17] Batch[4360] - loss: 1.166279  acc: 50.0000%(16/32)\n",
            "Epoch [17] Batch[4380] - loss: 1.260864  acc: 62.5000%(20/32)\n",
            "Epoch [17] Batch[4400] - loss: 1.275137  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.358362  acc: 40.5086%(446/1101) \n",
            "\n",
            "Epoch [17] Batch[4420] - loss: 1.292031  acc: 40.6250%(13/32)\n",
            "Epoch [17] Batch[4440] - loss: 1.264958  acc: 53.1250%(17/32)\n",
            "Epoch [17] Batch[4460] - loss: 1.221794  acc: 46.8750%(15/32)\n",
            "Epoch [17] Batch[4480] - loss: 1.423309  acc: 43.7500%(14/32)\n",
            "Epoch [17] Batch[4500] - loss: 1.227051  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.355916  acc: 40.3270%(444/1101) \n",
            "\n",
            "Epoch [17] Batch[4520] - loss: 1.294867  acc: 46.8750%(15/32)\n",
            "Epoch [18] Batch[4540] - loss: 1.317505  acc: 43.7500%(14/32)\n",
            "Epoch [18] Batch[4560] - loss: 1.191847  acc: 56.2500%(18/32)\n",
            "Epoch [18] Batch[4580] - loss: 1.254525  acc: 53.1250%(17/32)\n",
            "Epoch [18] Batch[4600] - loss: 1.288195  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.354467  acc: 40.5086%(446/1101) \n",
            "\n",
            "Epoch [18] Batch[4620] - loss: 1.197066  acc: 50.0000%(16/32)\n",
            "Epoch [18] Batch[4640] - loss: 1.175354  acc: 56.2500%(18/32)\n",
            "Epoch [18] Batch[4660] - loss: 1.226385  acc: 46.8750%(15/32)\n",
            "Epoch [18] Batch[4680] - loss: 1.201175  acc: 53.1250%(17/32)\n",
            "Epoch [18] Batch[4700] - loss: 1.236395  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.351417  acc: 40.3270%(444/1101) \n",
            "\n",
            "Epoch [18] Batch[4720] - loss: 1.247360  acc: 53.1250%(17/32)\n",
            "Epoch [18] Batch[4740] - loss: 1.261601  acc: 56.2500%(18/32)\n",
            "Epoch [18] Batch[4760] - loss: 1.277246  acc: 43.7500%(14/32)\n",
            "Epoch [18] Batch[4780] - loss: 1.333660  acc: 56.2500%(18/32)\n",
            "Epoch [18] Batch[4800] - loss: 1.245684  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.348636  acc: 41.3261%(455/1101) \n",
            "\n",
            "save best_model.pt, metric: 41.3260672116258\n",
            "Epoch [19] Batch[4820] - loss: 1.263755  acc: 50.0000%(16/32)\n",
            "Epoch [19] Batch[4840] - loss: 1.371770  acc: 50.0000%(16/32)\n",
            "Epoch [19] Batch[4860] - loss: 1.313093  acc: 46.8750%(15/32)\n",
            "Epoch [19] Batch[4880] - loss: 1.269640  acc: 53.1250%(17/32)\n",
            "Epoch [19] Batch[4900] - loss: 1.341252  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.346988  acc: 41.0536%(452/1101) \n",
            "\n",
            "Epoch [19] Batch[4920] - loss: 1.212785  acc: 65.6250%(21/32)\n",
            "Epoch [19] Batch[4940] - loss: 1.225322  acc: 53.1250%(17/32)\n",
            "Epoch [19] Batch[4960] - loss: 1.349060  acc: 37.5000%(12/32)\n",
            "Epoch [19] Batch[4980] - loss: 1.305669  acc: 43.7500%(14/32)\n",
            "Epoch [19] Batch[5000] - loss: 1.315191  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.344299  acc: 41.1444%(453/1101) \n",
            "\n",
            "Epoch [19] Batch[5020] - loss: 1.274304  acc: 50.0000%(16/32)\n",
            "Epoch [19] Batch[5040] - loss: 1.319743  acc: 46.8750%(15/32)\n",
            "Epoch [19] Batch[5060] - loss: 1.395674  acc: 40.6250%(13/32)\n",
            "Epoch [20] Batch[5080] - loss: 1.220188  acc: 59.3750%(19/32)\n",
            "Epoch [20] Batch[5100] - loss: 1.252072  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.341773  acc: 41.5985%(458/1101) \n",
            "\n",
            "save best_model.pt, metric: 41.59854677565849\n",
            "Epoch [20] Batch[5120] - loss: 1.270349  acc: 43.7500%(14/32)\n",
            "Epoch [20] Batch[5140] - loss: 1.240265  acc: 56.2500%(18/32)\n",
            "Epoch [20] Batch[5160] - loss: 1.260249  acc: 53.1250%(17/32)\n",
            "Epoch [20] Batch[5180] - loss: 1.313951  acc: 37.5000%(12/32)\n",
            "Epoch [20] Batch[5200] - loss: 1.159119  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.339596  acc: 41.8710%(461/1101) \n",
            "\n",
            "save best_model.pt, metric: 41.87102633969119\n",
            "Epoch [20] Batch[5220] - loss: 1.314776  acc: 59.3750%(19/32)\n",
            "Epoch [20] Batch[5240] - loss: 1.286900  acc: 37.5000%(12/32)\n",
            "Epoch [20] Batch[5260] - loss: 1.173687  acc: 65.6250%(21/32)\n",
            "Epoch [20] Batch[5280] - loss: 1.329165  acc: 46.8750%(15/32)\n",
            "Epoch [20] Batch[5300] - loss: 1.320971  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.338180  acc: 40.9628%(451/1101) \n",
            "\n",
            "Epoch [20] Batch[5320] - loss: 1.222709  acc: 50.0000%(16/32)\n",
            "Epoch [20] Batch[5340] - loss: 1.224323  acc: 53.1250%(17/32)\n",
            "Epoch [21] Batch[5360] - loss: 1.328865  acc: 40.6250%(13/32)\n",
            "Epoch [21] Batch[5380] - loss: 1.401679  acc: 40.6250%(13/32)\n",
            "Epoch [21] Batch[5400] - loss: 1.377930  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.337117  acc: 41.1444%(453/1101) \n",
            "\n",
            "Epoch [21] Batch[5420] - loss: 1.316255  acc: 43.7500%(14/32)\n",
            "Epoch [21] Batch[5440] - loss: 1.248675  acc: 53.1250%(17/32)\n",
            "Epoch [21] Batch[5460] - loss: 1.221082  acc: 53.1250%(17/32)\n",
            "Epoch [21] Batch[5480] - loss: 1.129393  acc: 50.0000%(16/32)\n",
            "Epoch [21] Batch[5500] - loss: 1.337007  acc: 34.3750%(11/32)\n",
            "\n",
            "Evaluation - loss: 1.334496  acc: 42.0527%(463/1101) \n",
            "\n",
            "save best_model.pt, metric: 42.05267938237966\n",
            "Epoch [21] Batch[5520] - loss: 1.152656  acc: 59.3750%(19/32)\n",
            "Epoch [21] Batch[5540] - loss: 1.260000  acc: 43.7500%(14/32)\n",
            "Epoch [21] Batch[5560] - loss: 1.280864  acc: 37.5000%(12/32)\n",
            "Epoch [21] Batch[5580] - loss: 1.171427  acc: 65.6250%(21/32)\n",
            "Epoch [21] Batch[5600] - loss: 1.306698  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.331873  acc: 41.4169%(456/1101) \n",
            "\n",
            "Epoch [22] Batch[5620] - loss: 1.311193  acc: 46.8750%(15/32)\n",
            "Epoch [22] Batch[5640] - loss: 1.172794  acc: 56.2500%(18/32)\n",
            "Epoch [22] Batch[5660] - loss: 1.182991  acc: 50.0000%(16/32)\n",
            "Epoch [22] Batch[5680] - loss: 1.207261  acc: 62.5000%(20/32)\n",
            "Epoch [22] Batch[5700] - loss: 1.277734  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.332198  acc: 41.3261%(455/1101) \n",
            "\n",
            "Epoch [22] Batch[5720] - loss: 1.119096  acc: 59.3750%(19/32)\n",
            "Epoch [22] Batch[5740] - loss: 1.178825  acc: 56.2500%(18/32)\n",
            "Epoch [22] Batch[5760] - loss: 1.231484  acc: 56.2500%(18/32)\n",
            "Epoch [22] Batch[5780] - loss: 1.314208  acc: 46.8750%(15/32)\n",
            "Epoch [22] Batch[5800] - loss: 1.137357  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.328701  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [22] Batch[5820] - loss: 1.252885  acc: 46.8750%(15/32)\n",
            "Epoch [22] Batch[5840] - loss: 1.310347  acc: 40.6250%(13/32)\n",
            "Epoch [22] Batch[5860] - loss: 1.439354  acc: 40.6250%(13/32)\n",
            "Epoch [23] Batch[5880] - loss: 1.144929  acc: 53.1250%(17/32)\n",
            "Epoch [23] Batch[5900] - loss: 1.351786  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.327030  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [23] Batch[5920] - loss: 1.294424  acc: 59.3750%(19/32)\n",
            "Epoch [23] Batch[5940] - loss: 1.132067  acc: 56.2500%(18/32)\n",
            "Epoch [23] Batch[5960] - loss: 1.311740  acc: 43.7500%(14/32)\n",
            "Epoch [23] Batch[5980] - loss: 1.403132  acc: 46.8750%(15/32)\n",
            "Epoch [23] Batch[6000] - loss: 1.059204  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.326200  acc: 42.1435%(464/1101) \n",
            "\n",
            "save best_model.pt, metric: 42.143505903723884\n",
            "Epoch [23] Batch[6020] - loss: 1.373191  acc: 34.3750%(11/32)\n",
            "Epoch [23] Batch[6040] - loss: 1.240006  acc: 53.1250%(17/32)\n",
            "Epoch [23] Batch[6060] - loss: 1.201111  acc: 40.6250%(13/32)\n",
            "Epoch [23] Batch[6080] - loss: 1.260938  acc: 46.8750%(15/32)\n",
            "Epoch [23] Batch[6100] - loss: 1.226078  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.324254  acc: 42.2343%(465/1101) \n",
            "\n",
            "save best_model.pt, metric: 42.23433242506812\n",
            "Epoch [23] Batch[6120] - loss: 1.172464  acc: 50.0000%(16/32)\n",
            "Epoch [23] Batch[6140] - loss: 1.268109  acc: 50.0000%(16/32)\n",
            "Epoch [24] Batch[6160] - loss: 1.191370  acc: 50.0000%(16/32)\n",
            "Epoch [24] Batch[6180] - loss: 1.203335  acc: 50.0000%(16/32)\n",
            "Epoch [24] Batch[6200] - loss: 1.221859  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.322313  acc: 41.5077%(457/1101) \n",
            "\n",
            "Epoch [24] Batch[6220] - loss: 1.117066  acc: 59.3750%(19/32)\n",
            "Epoch [24] Batch[6240] - loss: 1.369445  acc: 46.8750%(15/32)\n",
            "Epoch [24] Batch[6260] - loss: 1.309095  acc: 46.8750%(15/32)\n",
            "Epoch [24] Batch[6280] - loss: 1.181307  acc: 53.1250%(17/32)\n",
            "Epoch [24] Batch[6300] - loss: 1.390926  acc: 31.2500%(10/32)\n",
            "\n",
            "Evaluation - loss: 1.321343  acc: 41.5077%(457/1101) \n",
            "\n",
            "Epoch [24] Batch[6320] - loss: 1.292108  acc: 46.8750%(15/32)\n",
            "Epoch [24] Batch[6340] - loss: 1.199862  acc: 59.3750%(19/32)\n",
            "Epoch [24] Batch[6360] - loss: 1.185593  acc: 62.5000%(20/32)\n",
            "Epoch [24] Batch[6380] - loss: 1.279800  acc: 43.7500%(14/32)\n",
            "Epoch [24] Batch[6400] - loss: 1.200455  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.319705  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [25] Batch[6420] - loss: 1.259127  acc: 40.6250%(13/32)\n",
            "Epoch [25] Batch[6440] - loss: 1.332467  acc: 43.7500%(14/32)\n",
            "Epoch [25] Batch[6460] - loss: 1.144979  acc: 53.1250%(17/32)\n",
            "Epoch [25] Batch[6480] - loss: 1.033174  acc: 68.7500%(22/32)\n",
            "Epoch [25] Batch[6500] - loss: 1.218592  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.318538  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [25] Batch[6520] - loss: 1.190796  acc: 50.0000%(16/32)\n",
            "Epoch [25] Batch[6540] - loss: 1.138478  acc: 50.0000%(16/32)\n",
            "Epoch [25] Batch[6560] - loss: 1.269068  acc: 50.0000%(16/32)\n",
            "Epoch [25] Batch[6580] - loss: 1.296319  acc: 40.6250%(13/32)\n",
            "Epoch [25] Batch[6600] - loss: 1.182872  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.317253  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [25] Batch[6620] - loss: 1.110527  acc: 62.5000%(20/32)\n",
            "Epoch [25] Batch[6640] - loss: 1.333940  acc: 43.7500%(14/32)\n",
            "Epoch [25] Batch[6660] - loss: 1.168045  acc: 56.2500%(18/32)\n",
            "Epoch [26] Batch[6680] - loss: 1.308712  acc: 46.8750%(15/32)\n",
            "Epoch [26] Batch[6700] - loss: 1.099716  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.315212  acc: 41.8710%(461/1101) \n",
            "\n",
            "Epoch [26] Batch[6720] - loss: 1.207918  acc: 46.8750%(15/32)\n",
            "Epoch [26] Batch[6740] - loss: 1.157907  acc: 56.2500%(18/32)\n",
            "Epoch [26] Batch[6760] - loss: 1.288057  acc: 50.0000%(16/32)\n",
            "Epoch [26] Batch[6780] - loss: 1.105014  acc: 50.0000%(16/32)\n",
            "Epoch [26] Batch[6800] - loss: 1.301128  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.314087  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [26] Batch[6820] - loss: 1.170362  acc: 68.7500%(22/32)\n",
            "Epoch [26] Batch[6840] - loss: 1.083347  acc: 65.6250%(21/32)\n",
            "Epoch [26] Batch[6860] - loss: 1.123133  acc: 68.7500%(22/32)\n",
            "Epoch [26] Batch[6880] - loss: 1.268579  acc: 34.3750%(11/32)\n",
            "Epoch [26] Batch[6900] - loss: 1.226579  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.313221  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [26] Batch[6920] - loss: 1.308153  acc: 34.3750%(11/32)\n",
            "Epoch [26] Batch[6940] - loss: 1.254311  acc: 62.5000%(20/32)\n",
            "Epoch [27] Batch[6960] - loss: 1.255716  acc: 40.6250%(13/32)\n",
            "Epoch [27] Batch[6980] - loss: 1.300391  acc: 46.8750%(15/32)\n",
            "Epoch [27] Batch[7000] - loss: 1.212383  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.311160  acc: 42.0527%(463/1101) \n",
            "\n",
            "Epoch [27] Batch[7020] - loss: 1.127337  acc: 62.5000%(20/32)\n",
            "Epoch [27] Batch[7040] - loss: 1.180305  acc: 43.7500%(14/32)\n",
            "Epoch [27] Batch[7060] - loss: 1.160980  acc: 56.2500%(18/32)\n",
            "Epoch [27] Batch[7080] - loss: 1.231454  acc: 53.1250%(17/32)\n",
            "Epoch [27] Batch[7100] - loss: 1.189556  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.310338  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [27] Batch[7120] - loss: 1.233873  acc: 37.5000%(12/32)\n",
            "Epoch [27] Batch[7140] - loss: 1.057393  acc: 68.7500%(22/32)\n",
            "Epoch [27] Batch[7160] - loss: 1.212430  acc: 50.0000%(16/32)\n",
            "Epoch [27] Batch[7180] - loss: 1.218864  acc: 46.8750%(15/32)\n",
            "Epoch [27] Batch[7200] - loss: 1.302023  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.310833  acc: 41.7802%(460/1101) \n",
            "\n",
            "Epoch [28] Batch[7220] - loss: 1.247208  acc: 46.8750%(15/32)\n",
            "Epoch [28] Batch[7240] - loss: 1.156651  acc: 59.3750%(19/32)\n",
            "Epoch [28] Batch[7260] - loss: 1.178506  acc: 56.2500%(18/32)\n",
            "Epoch [28] Batch[7280] - loss: 1.168897  acc: 50.0000%(16/32)\n",
            "Epoch [28] Batch[7300] - loss: 1.245073  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.309775  acc: 41.5985%(458/1101) \n",
            "\n",
            "Epoch [28] Batch[7320] - loss: 1.227549  acc: 53.1250%(17/32)\n",
            "Epoch [28] Batch[7340] - loss: 1.132478  acc: 53.1250%(17/32)\n",
            "Epoch [28] Batch[7360] - loss: 1.194357  acc: 53.1250%(17/32)\n",
            "Epoch [28] Batch[7380] - loss: 1.192212  acc: 53.1250%(17/32)\n",
            "Epoch [28] Batch[7400] - loss: 1.202776  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.307828  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [28] Batch[7420] - loss: 1.320716  acc: 28.1250%(9/32)\n",
            "Epoch [28] Batch[7440] - loss: 1.331735  acc: 43.7500%(14/32)\n",
            "Epoch [28] Batch[7460] - loss: 1.251226  acc: 56.2500%(18/32)\n",
            "Epoch [29] Batch[7480] - loss: 1.311076  acc: 43.7500%(14/32)\n",
            "Epoch [29] Batch[7500] - loss: 1.108534  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.307326  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [29] Batch[7520] - loss: 1.173240  acc: 62.5000%(20/32)\n",
            "Epoch [29] Batch[7540] - loss: 1.145872  acc: 53.1250%(17/32)\n",
            "Epoch [29] Batch[7560] - loss: 1.278643  acc: 43.7500%(14/32)\n",
            "Epoch [29] Batch[7580] - loss: 1.319256  acc: 34.3750%(11/32)\n",
            "Epoch [29] Batch[7600] - loss: 1.033727  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.305059  acc: 42.5976%(469/1101) \n",
            "\n",
            "save best_model.pt, metric: 42.59763851044505\n",
            "Epoch [29] Batch[7620] - loss: 1.103805  acc: 59.3750%(19/32)\n",
            "Epoch [29] Batch[7640] - loss: 1.147960  acc: 50.0000%(16/32)\n",
            "Epoch [29] Batch[7660] - loss: 1.161442  acc: 56.2500%(18/32)\n",
            "Epoch [29] Batch[7680] - loss: 1.210194  acc: 56.2500%(18/32)\n",
            "Epoch [29] Batch[7700] - loss: 1.190775  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.305656  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [29] Batch[7720] - loss: 1.037594  acc: 56.2500%(18/32)\n",
            "Epoch [29] Batch[7740] - loss: 1.151551  acc: 46.8750%(15/32)\n",
            "Epoch [30] Batch[7760] - loss: 1.282099  acc: 40.6250%(13/32)\n",
            "Epoch [30] Batch[7780] - loss: 1.148023  acc: 50.0000%(16/32)\n",
            "Epoch [30] Batch[7800] - loss: 1.070593  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.303103  acc: 42.1435%(464/1101) \n",
            "\n",
            "Epoch [30] Batch[7820] - loss: 1.179629  acc: 56.2500%(18/32)\n",
            "Epoch [30] Batch[7840] - loss: 1.204420  acc: 56.2500%(18/32)\n",
            "Epoch [30] Batch[7860] - loss: 1.133923  acc: 56.2500%(18/32)\n",
            "Epoch [30] Batch[7880] - loss: 1.196827  acc: 59.3750%(19/32)\n",
            "Epoch [30] Batch[7900] - loss: 1.098408  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.301905  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [30] Batch[7920] - loss: 1.182675  acc: 40.6250%(13/32)\n",
            "Epoch [30] Batch[7940] - loss: 1.213418  acc: 53.1250%(17/32)\n",
            "Epoch [30] Batch[7960] - loss: 1.263040  acc: 43.7500%(14/32)\n",
            "Epoch [30] Batch[7980] - loss: 1.371504  acc: 40.6250%(13/32)\n",
            "Epoch [30] Batch[8000] - loss: 1.119310  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.302375  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [31] Batch[8020] - loss: 1.063772  acc: 56.2500%(18/32)\n",
            "Epoch [31] Batch[8040] - loss: 1.172616  acc: 59.3750%(19/32)\n",
            "Epoch [31] Batch[8060] - loss: 1.155020  acc: 53.1250%(17/32)\n",
            "Epoch [31] Batch[8080] - loss: 1.054165  acc: 62.5000%(20/32)\n",
            "Epoch [31] Batch[8100] - loss: 1.085482  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.299324  acc: 42.8701%(472/1101) \n",
            "\n",
            "save best_model.pt, metric: 42.870118074477745\n",
            "Epoch [31] Batch[8120] - loss: 1.230625  acc: 56.2500%(18/32)\n",
            "Epoch [31] Batch[8140] - loss: 1.248346  acc: 46.8750%(15/32)\n",
            "Epoch [31] Batch[8160] - loss: 0.999327  acc: 59.3750%(19/32)\n",
            "Epoch [31] Batch[8180] - loss: 1.310972  acc: 43.7500%(14/32)\n",
            "Epoch [31] Batch[8200] - loss: 1.161974  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.299333  acc: 42.7793%(471/1101) \n",
            "\n",
            "Epoch [31] Batch[8220] - loss: 1.075188  acc: 68.7500%(22/32)\n",
            "Epoch [31] Batch[8240] - loss: 1.213495  acc: 59.3750%(19/32)\n",
            "Epoch [31] Batch[8260] - loss: 1.223913  acc: 46.8750%(15/32)\n",
            "Epoch [32] Batch[8280] - loss: 1.234208  acc: 50.0000%(16/32)\n",
            "Epoch [32] Batch[8300] - loss: 1.065923  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.300860  acc: 41.9619%(462/1101) \n",
            "\n",
            "Epoch [32] Batch[8320] - loss: 1.158028  acc: 56.2500%(18/32)\n",
            "Epoch [32] Batch[8340] - loss: 1.300253  acc: 53.1250%(17/32)\n",
            "Epoch [32] Batch[8360] - loss: 1.181584  acc: 46.8750%(15/32)\n",
            "Epoch [32] Batch[8380] - loss: 1.186176  acc: 46.8750%(15/32)\n",
            "Epoch [32] Batch[8400] - loss: 1.005984  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.299748  acc: 41.6894%(459/1101) \n",
            "\n",
            "Epoch [32] Batch[8420] - loss: 1.150154  acc: 50.0000%(16/32)\n",
            "Epoch [32] Batch[8440] - loss: 1.129172  acc: 53.1250%(17/32)\n",
            "Epoch [32] Batch[8460] - loss: 1.258392  acc: 43.7500%(14/32)\n",
            "Epoch [32] Batch[8480] - loss: 1.207636  acc: 40.6250%(13/32)\n",
            "Epoch [32] Batch[8500] - loss: 1.172832  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.296249  acc: 42.8701%(472/1101) \n",
            "\n",
            "Epoch [32] Batch[8520] - loss: 0.982675  acc: 75.0000%(24/32)\n",
            "Epoch [32] Batch[8540] - loss: 1.102962  acc: 50.0000%(16/32)\n",
            "Epoch [33] Batch[8560] - loss: 1.052989  acc: 59.3750%(19/32)\n",
            "Epoch [33] Batch[8580] - loss: 1.223440  acc: 46.8750%(15/32)\n",
            "Epoch [33] Batch[8600] - loss: 1.239199  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.295886  acc: 42.5976%(469/1101) \n",
            "\n",
            "Epoch [33] Batch[8620] - loss: 1.098893  acc: 59.3750%(19/32)\n",
            "Epoch [33] Batch[8640] - loss: 1.144255  acc: 50.0000%(16/32)\n",
            "Epoch [33] Batch[8660] - loss: 1.054476  acc: 62.5000%(20/32)\n",
            "Epoch [33] Batch[8680] - loss: 1.260795  acc: 56.2500%(18/32)\n",
            "Epoch [33] Batch[8700] - loss: 1.244009  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.295821  acc: 42.2343%(465/1101) \n",
            "\n",
            "Epoch [33] Batch[8720] - loss: 1.128494  acc: 50.0000%(16/32)\n",
            "Epoch [33] Batch[8740] - loss: 1.191130  acc: 43.7500%(14/32)\n",
            "Epoch [33] Batch[8760] - loss: 1.174637  acc: 46.8750%(15/32)\n",
            "Epoch [33] Batch[8780] - loss: 1.166134  acc: 59.3750%(19/32)\n",
            "Epoch [33] Batch[8800] - loss: 1.114126  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.294257  acc: 42.5976%(469/1101) \n",
            "\n",
            "Epoch [34] Batch[8820] - loss: 1.076669  acc: 59.3750%(19/32)\n",
            "Epoch [34] Batch[8840] - loss: 1.114138  acc: 59.3750%(19/32)\n",
            "Epoch [34] Batch[8860] - loss: 1.239954  acc: 62.5000%(20/32)\n",
            "Epoch [34] Batch[8880] - loss: 1.188425  acc: 53.1250%(17/32)\n",
            "Epoch [34] Batch[8900] - loss: 1.187117  acc: 37.5000%(12/32)\n",
            "\n",
            "Evaluation - loss: 1.294992  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [34] Batch[8920] - loss: 1.193350  acc: 43.7500%(14/32)\n",
            "Epoch [34] Batch[8940] - loss: 1.376908  acc: 50.0000%(16/32)\n",
            "Epoch [34] Batch[8960] - loss: 1.086186  acc: 65.6250%(21/32)\n",
            "Epoch [34] Batch[8980] - loss: 1.307102  acc: 40.6250%(13/32)\n",
            "Epoch [34] Batch[9000] - loss: 0.943569  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.293666  acc: 42.6885%(470/1101) \n",
            "\n",
            "Epoch [34] Batch[9020] - loss: 1.169278  acc: 40.6250%(13/32)\n",
            "Epoch [34] Batch[9040] - loss: 1.076446  acc: 56.2500%(18/32)\n",
            "Epoch [34] Batch[9060] - loss: 1.088418  acc: 56.2500%(18/32)\n",
            "Epoch [35] Batch[9080] - loss: 1.242722  acc: 56.2500%(18/32)\n",
            "Epoch [35] Batch[9100] - loss: 1.070536  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.291522  acc: 42.7793%(471/1101) \n",
            "\n",
            "Epoch [35] Batch[9120] - loss: 1.092599  acc: 59.3750%(19/32)\n",
            "Epoch [35] Batch[9140] - loss: 1.188568  acc: 40.6250%(13/32)\n",
            "Epoch [35] Batch[9160] - loss: 1.174183  acc: 46.8750%(15/32)\n",
            "Epoch [35] Batch[9180] - loss: 1.060258  acc: 56.2500%(18/32)\n",
            "Epoch [35] Batch[9200] - loss: 1.171921  acc: 37.5000%(12/32)\n",
            "\n",
            "Evaluation - loss: 1.292595  acc: 42.7793%(471/1101) \n",
            "\n",
            "Epoch [35] Batch[9220] - loss: 1.169446  acc: 56.2500%(18/32)\n",
            "Epoch [35] Batch[9240] - loss: 1.007502  acc: 50.0000%(16/32)\n",
            "Epoch [35] Batch[9260] - loss: 1.268753  acc: 56.2500%(18/32)\n",
            "Epoch [35] Batch[9280] - loss: 1.129306  acc: 62.5000%(20/32)\n",
            "Epoch [35] Batch[9300] - loss: 1.229553  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.289725  acc: 42.4160%(467/1101) \n",
            "\n",
            "Epoch [35] Batch[9320] - loss: 1.351771  acc: 43.7500%(14/32)\n",
            "Epoch [35] Batch[9340] - loss: 0.960142  acc: 65.6250%(21/32)\n",
            "Epoch [36] Batch[9360] - loss: 1.056003  acc: 62.5000%(20/32)\n",
            "Epoch [36] Batch[9380] - loss: 1.141328  acc: 62.5000%(20/32)\n",
            "Epoch [36] Batch[9400] - loss: 1.181435  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.289211  acc: 42.5068%(468/1101) \n",
            "\n",
            "Epoch [36] Batch[9420] - loss: 1.096864  acc: 53.1250%(17/32)\n",
            "Epoch [36] Batch[9440] - loss: 1.060374  acc: 59.3750%(19/32)\n",
            "Epoch [36] Batch[9460] - loss: 1.177177  acc: 59.3750%(19/32)\n",
            "Epoch [36] Batch[9480] - loss: 1.263754  acc: 50.0000%(16/32)\n",
            "Epoch [36] Batch[9500] - loss: 1.166246  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.289546  acc: 42.6885%(470/1101) \n",
            "\n",
            "Epoch [36] Batch[9520] - loss: 1.214864  acc: 46.8750%(15/32)\n",
            "Epoch [36] Batch[9540] - loss: 1.222258  acc: 46.8750%(15/32)\n",
            "Epoch [36] Batch[9560] - loss: 1.177611  acc: 50.0000%(16/32)\n",
            "Epoch [36] Batch[9580] - loss: 1.102141  acc: 59.3750%(19/32)\n",
            "Epoch [36] Batch[9600] - loss: 1.198774  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.289120  acc: 42.8701%(472/1101) \n",
            "\n",
            "Epoch [37] Batch[9620] - loss: 1.078168  acc: 50.0000%(16/32)\n",
            "Epoch [37] Batch[9640] - loss: 1.140941  acc: 53.1250%(17/32)\n",
            "Epoch [37] Batch[9660] - loss: 1.060377  acc: 53.1250%(17/32)\n",
            "Epoch [37] Batch[9680] - loss: 1.262254  acc: 37.5000%(12/32)\n",
            "Epoch [37] Batch[9700] - loss: 1.013824  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.287814  acc: 42.5976%(469/1101) \n",
            "\n",
            "Epoch [37] Batch[9720] - loss: 1.078493  acc: 50.0000%(16/32)\n",
            "Epoch [37] Batch[9740] - loss: 1.078459  acc: 65.6250%(21/32)\n",
            "Epoch [37] Batch[9760] - loss: 1.138288  acc: 46.8750%(15/32)\n",
            "Epoch [37] Batch[9780] - loss: 1.169819  acc: 59.3750%(19/32)\n",
            "Epoch [37] Batch[9800] - loss: 1.292075  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.288234  acc: 42.6885%(470/1101) \n",
            "\n",
            "Epoch [37] Batch[9820] - loss: 1.243497  acc: 46.8750%(15/32)\n",
            "Epoch [37] Batch[9840] - loss: 1.030603  acc: 71.8750%(23/32)\n",
            "Epoch [37] Batch[9860] - loss: 1.104894  acc: 59.3750%(19/32)\n",
            "Epoch [38] Batch[9880] - loss: 1.247932  acc: 50.0000%(16/32)\n",
            "Epoch [38] Batch[9900] - loss: 1.058723  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.286630  acc: 42.8701%(472/1101) \n",
            "\n",
            "Epoch [38] Batch[9920] - loss: 1.007641  acc: 68.7500%(22/32)\n",
            "Epoch [38] Batch[9940] - loss: 1.234773  acc: 50.0000%(16/32)\n",
            "Epoch [38] Batch[9960] - loss: 1.193878  acc: 56.2500%(18/32)\n",
            "Epoch [38] Batch[9980] - loss: 1.063044  acc: 59.3750%(19/32)\n",
            "Epoch [38] Batch[10000] - loss: 1.107446  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.286178  acc: 42.7793%(471/1101) \n",
            "\n",
            "Epoch [38] Batch[10020] - loss: 1.153073  acc: 62.5000%(20/32)\n",
            "Epoch [38] Batch[10040] - loss: 1.235134  acc: 56.2500%(18/32)\n",
            "Epoch [38] Batch[10060] - loss: 1.142560  acc: 50.0000%(16/32)\n",
            "Epoch [38] Batch[10080] - loss: 1.043285  acc: 56.2500%(18/32)\n",
            "Epoch [38] Batch[10100] - loss: 1.110337  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.286229  acc: 42.7793%(471/1101) \n",
            "\n",
            "Epoch [38] Batch[10120] - loss: 1.310197  acc: 46.8750%(15/32)\n",
            "Epoch [38] Batch[10140] - loss: 0.965067  acc: 75.0000%(24/32)\n",
            "Epoch [39] Batch[10160] - loss: 1.152993  acc: 62.5000%(20/32)\n",
            "Epoch [39] Batch[10180] - loss: 0.967920  acc: 65.6250%(21/32)\n",
            "Epoch [39] Batch[10200] - loss: 1.067900  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.285564  acc: 43.3243%(477/1101) \n",
            "\n",
            "save best_model.pt, metric: 43.32425068119891\n",
            "Epoch [39] Batch[10220] - loss: 1.171170  acc: 50.0000%(16/32)\n",
            "Epoch [39] Batch[10240] - loss: 1.073588  acc: 56.2500%(18/32)\n",
            "Epoch [39] Batch[10260] - loss: 1.083221  acc: 46.8750%(15/32)\n",
            "Epoch [39] Batch[10280] - loss: 1.104641  acc: 56.2500%(18/32)\n",
            "Epoch [39] Batch[10300] - loss: 0.968595  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.285260  acc: 42.8701%(472/1101) \n",
            "\n",
            "Epoch [39] Batch[10320] - loss: 1.125407  acc: 40.6250%(13/32)\n",
            "Epoch [39] Batch[10340] - loss: 1.142336  acc: 59.3750%(19/32)\n",
            "Epoch [39] Batch[10360] - loss: 1.193099  acc: 46.8750%(15/32)\n",
            "Epoch [39] Batch[10380] - loss: 1.301763  acc: 37.5000%(12/32)\n",
            "Epoch [39] Batch[10400] - loss: 0.961644  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.283794  acc: 43.1426%(475/1101) \n",
            "\n",
            "Epoch [40] Batch[10420] - loss: 0.978293  acc: 71.8750%(23/32)\n",
            "Epoch [40] Batch[10440] - loss: 0.992711  acc: 65.6250%(21/32)\n",
            "Epoch [40] Batch[10460] - loss: 1.098736  acc: 59.3750%(19/32)\n",
            "Epoch [40] Batch[10480] - loss: 1.093199  acc: 53.1250%(17/32)\n",
            "Epoch [40] Batch[10500] - loss: 1.025920  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.283470  acc: 43.0518%(474/1101) \n",
            "\n",
            "Epoch [40] Batch[10520] - loss: 1.224649  acc: 59.3750%(19/32)\n",
            "Epoch [40] Batch[10540] - loss: 1.076234  acc: 62.5000%(20/32)\n",
            "Epoch [40] Batch[10560] - loss: 1.006427  acc: 53.1250%(17/32)\n",
            "Epoch [40] Batch[10580] - loss: 1.175224  acc: 46.8750%(15/32)\n",
            "Epoch [40] Batch[10600] - loss: 1.242975  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.282652  acc: 43.3243%(477/1101) \n",
            "\n",
            "Epoch [40] Batch[10620] - loss: 1.108256  acc: 62.5000%(20/32)\n",
            "Epoch [40] Batch[10640] - loss: 1.169692  acc: 59.3750%(19/32)\n",
            "Epoch [40] Batch[10660] - loss: 1.030201  acc: 53.1250%(17/32)\n",
            "Epoch [40] Batch[10680] - loss: 1.311520  acc: 43.7500%(14/32)\n",
            "Epoch [41] Batch[10700] - loss: 1.279484  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.282070  acc: 42.5976%(469/1101) \n",
            "\n",
            "Epoch [41] Batch[10720] - loss: 1.179387  acc: 28.1250%(9/32)\n",
            "Epoch [41] Batch[10740] - loss: 1.119115  acc: 59.3750%(19/32)\n",
            "Epoch [41] Batch[10760] - loss: 1.299672  acc: 37.5000%(12/32)\n",
            "Epoch [41] Batch[10780] - loss: 1.006238  acc: 65.6250%(21/32)\n",
            "Epoch [41] Batch[10800] - loss: 1.056928  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.280407  acc: 42.6885%(470/1101) \n",
            "\n",
            "Epoch [41] Batch[10820] - loss: 1.003294  acc: 62.5000%(20/32)\n",
            "Epoch [41] Batch[10840] - loss: 1.165386  acc: 53.1250%(17/32)\n",
            "Epoch [41] Batch[10860] - loss: 1.244690  acc: 40.6250%(13/32)\n",
            "Epoch [41] Batch[10880] - loss: 1.192029  acc: 59.3750%(19/32)\n",
            "Epoch [41] Batch[10900] - loss: 1.109989  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.281241  acc: 43.7784%(482/1101) \n",
            "\n",
            "save best_model.pt, metric: 43.77838328792007\n",
            "Epoch [41] Batch[10920] - loss: 1.066016  acc: 56.2500%(18/32)\n",
            "Epoch [41] Batch[10940] - loss: 1.107981  acc: 62.5000%(20/32)\n",
            "Epoch [42] Batch[10960] - loss: 1.038664  acc: 50.0000%(16/32)\n",
            "Epoch [42] Batch[10980] - loss: 1.021753  acc: 59.3750%(19/32)\n",
            "Epoch [42] Batch[11000] - loss: 1.185337  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.281227  acc: 43.3243%(477/1101) \n",
            "\n",
            "Epoch [42] Batch[11020] - loss: 0.978961  acc: 62.5000%(20/32)\n",
            "Epoch [42] Batch[11040] - loss: 1.132946  acc: 65.6250%(21/32)\n",
            "Epoch [42] Batch[11060] - loss: 1.144104  acc: 50.0000%(16/32)\n",
            "Epoch [42] Batch[11080] - loss: 1.083575  acc: 59.3750%(19/32)\n",
            "Epoch [42] Batch[11100] - loss: 1.038490  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.279271  acc: 43.4151%(478/1101) \n",
            "\n",
            "Epoch [42] Batch[11120] - loss: 1.158467  acc: 56.2500%(18/32)\n",
            "Epoch [42] Batch[11140] - loss: 1.087594  acc: 56.2500%(18/32)\n",
            "Epoch [42] Batch[11160] - loss: 0.952006  acc: 78.1250%(25/32)\n",
            "Epoch [42] Batch[11180] - loss: 0.954563  acc: 68.7500%(22/32)\n",
            "Epoch [42] Batch[11200] - loss: 0.951696  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.280338  acc: 43.2334%(476/1101) \n",
            "\n",
            "Epoch [43] Batch[11220] - loss: 0.998101  acc: 68.7500%(22/32)\n",
            "Epoch [43] Batch[11240] - loss: 1.067025  acc: 53.1250%(17/32)\n",
            "Epoch [43] Batch[11260] - loss: 0.975233  acc: 71.8750%(23/32)\n",
            "Epoch [43] Batch[11280] - loss: 0.925952  acc: 71.8750%(23/32)\n",
            "Epoch [43] Batch[11300] - loss: 1.119499  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.280654  acc: 43.2334%(476/1101) \n",
            "\n",
            "Epoch [43] Batch[11320] - loss: 1.091241  acc: 46.8750%(15/32)\n",
            "Epoch [43] Batch[11340] - loss: 1.037366  acc: 65.6250%(21/32)\n",
            "Epoch [43] Batch[11360] - loss: 1.093971  acc: 50.0000%(16/32)\n",
            "Epoch [43] Batch[11380] - loss: 1.129194  acc: 53.1250%(17/32)\n",
            "Epoch [43] Batch[11400] - loss: 1.148290  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.278490  acc: 43.0518%(474/1101) \n",
            "\n",
            "Epoch [43] Batch[11420] - loss: 0.932957  acc: 71.8750%(23/32)\n",
            "Epoch [43] Batch[11440] - loss: 1.081979  acc: 62.5000%(20/32)\n",
            "Epoch [43] Batch[11460] - loss: 1.142578  acc: 65.6250%(21/32)\n",
            "Epoch [43] Batch[11480] - loss: 1.201709  acc: 56.2500%(18/32)\n",
            "Epoch [44] Batch[11500] - loss: 1.115207  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.277898  acc: 43.4151%(478/1101) \n",
            "\n",
            "Epoch [44] Batch[11520] - loss: 1.050378  acc: 71.8750%(23/32)\n",
            "Epoch [44] Batch[11540] - loss: 0.990739  acc: 62.5000%(20/32)\n",
            "Epoch [44] Batch[11560] - loss: 1.089971  acc: 71.8750%(23/32)\n",
            "Epoch [44] Batch[11580] - loss: 0.993162  acc: 65.6250%(21/32)\n",
            "Epoch [44] Batch[11600] - loss: 1.193203  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.277747  acc: 43.3243%(477/1101) \n",
            "\n",
            "Epoch [44] Batch[11620] - loss: 0.902019  acc: 71.8750%(23/32)\n",
            "Epoch [44] Batch[11640] - loss: 1.111837  acc: 62.5000%(20/32)\n",
            "Epoch [44] Batch[11660] - loss: 1.229789  acc: 50.0000%(16/32)\n",
            "Epoch [44] Batch[11680] - loss: 1.296177  acc: 53.1250%(17/32)\n",
            "Epoch [44] Batch[11700] - loss: 0.902475  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.276684  acc: 43.5059%(479/1101) \n",
            "\n",
            "Epoch [44] Batch[11720] - loss: 1.248919  acc: 46.8750%(15/32)\n",
            "Epoch [44] Batch[11740] - loss: 1.195397  acc: 43.7500%(14/32)\n",
            "Epoch [45] Batch[11760] - loss: 0.960115  acc: 56.2500%(18/32)\n",
            "Epoch [45] Batch[11780] - loss: 1.132596  acc: 59.3750%(19/32)\n",
            "Epoch [45] Batch[11800] - loss: 1.043280  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.277301  acc: 43.6876%(481/1101) \n",
            "\n",
            "Epoch [45] Batch[11820] - loss: 1.055532  acc: 59.3750%(19/32)\n",
            "Epoch [45] Batch[11840] - loss: 1.322869  acc: 43.7500%(14/32)\n",
            "Epoch [45] Batch[11860] - loss: 1.152982  acc: 46.8750%(15/32)\n",
            "Epoch [45] Batch[11880] - loss: 1.122766  acc: 56.2500%(18/32)\n",
            "Epoch [45] Batch[11900] - loss: 1.022583  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.276980  acc: 43.6876%(481/1101) \n",
            "\n",
            "Epoch [45] Batch[11920] - loss: 1.156121  acc: 62.5000%(20/32)\n",
            "Epoch [45] Batch[11940] - loss: 0.959227  acc: 78.1250%(25/32)\n",
            "Epoch [45] Batch[11960] - loss: 1.204146  acc: 46.8750%(15/32)\n",
            "Epoch [45] Batch[11980] - loss: 1.158509  acc: 62.5000%(20/32)\n",
            "Epoch [45] Batch[12000] - loss: 1.019503  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.275117  acc: 43.8692%(483/1101) \n",
            "\n",
            "save best_model.pt, metric: 43.869209809264305\n",
            "Epoch [46] Batch[12020] - loss: 0.993559  acc: 78.1250%(25/32)\n",
            "Epoch [46] Batch[12040] - loss: 1.128786  acc: 56.2500%(18/32)\n",
            "Epoch [46] Batch[12060] - loss: 1.025356  acc: 50.0000%(16/32)\n",
            "Epoch [46] Batch[12080] - loss: 1.126302  acc: 65.6250%(21/32)\n",
            "Epoch [46] Batch[12100] - loss: 1.204218  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.274293  acc: 43.9600%(484/1101) \n",
            "\n",
            "save best_model.pt, metric: 43.96003633060854\n",
            "Epoch [46] Batch[12120] - loss: 1.061497  acc: 53.1250%(17/32)\n",
            "Epoch [46] Batch[12140] - loss: 1.107307  acc: 59.3750%(19/32)\n",
            "Epoch [46] Batch[12160] - loss: 1.069363  acc: 59.3750%(19/32)\n",
            "Epoch [46] Batch[12180] - loss: 1.126347  acc: 43.7500%(14/32)\n",
            "Epoch [46] Batch[12200] - loss: 1.160044  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.274570  acc: 44.0509%(485/1101) \n",
            "\n",
            "save best_model.pt, metric: 44.05086285195277\n",
            "Epoch [46] Batch[12220] - loss: 0.944851  acc: 71.8750%(23/32)\n",
            "Epoch [46] Batch[12240] - loss: 1.032966  acc: 65.6250%(21/32)\n",
            "Epoch [46] Batch[12260] - loss: 0.966151  acc: 68.7500%(22/32)\n",
            "Epoch [46] Batch[12280] - loss: 1.088565  acc: 59.3750%(19/32)\n",
            "Epoch [47] Batch[12300] - loss: 1.198595  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.273584  acc: 43.4151%(478/1101) \n",
            "\n",
            "Epoch [47] Batch[12320] - loss: 1.125117  acc: 50.0000%(16/32)\n",
            "Epoch [47] Batch[12340] - loss: 0.883691  acc: 75.0000%(24/32)\n",
            "Epoch [47] Batch[12360] - loss: 0.968844  acc: 53.1250%(17/32)\n",
            "Epoch [47] Batch[12380] - loss: 1.185519  acc: 59.3750%(19/32)\n",
            "Epoch [47] Batch[12400] - loss: 1.241659  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.273690  acc: 43.5967%(480/1101) \n",
            "\n",
            "Epoch [47] Batch[12420] - loss: 0.963558  acc: 68.7500%(22/32)\n",
            "Epoch [47] Batch[12440] - loss: 1.022681  acc: 62.5000%(20/32)\n",
            "Epoch [47] Batch[12460] - loss: 1.119948  acc: 46.8750%(15/32)\n",
            "Epoch [47] Batch[12480] - loss: 1.021116  acc: 65.6250%(21/32)\n",
            "Epoch [47] Batch[12500] - loss: 0.984004  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.274572  acc: 43.7784%(482/1101) \n",
            "\n",
            "Epoch [47] Batch[12520] - loss: 1.179187  acc: 50.0000%(16/32)\n",
            "Epoch [47] Batch[12540] - loss: 1.022638  acc: 68.7500%(22/32)\n",
            "Epoch [48] Batch[12560] - loss: 1.094165  acc: 56.2500%(18/32)\n",
            "Epoch [48] Batch[12580] - loss: 1.085898  acc: 53.1250%(17/32)\n",
            "Epoch [48] Batch[12600] - loss: 1.128877  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.273177  acc: 43.3243%(477/1101) \n",
            "\n",
            "Epoch [48] Batch[12620] - loss: 1.044495  acc: 65.6250%(21/32)\n",
            "Epoch [48] Batch[12640] - loss: 1.173612  acc: 56.2500%(18/32)\n",
            "Epoch [48] Batch[12660] - loss: 1.001794  acc: 62.5000%(20/32)\n",
            "Epoch [48] Batch[12680] - loss: 0.965238  acc: 65.6250%(21/32)\n",
            "Epoch [48] Batch[12700] - loss: 0.990395  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.272632  acc: 44.2325%(487/1101) \n",
            "\n",
            "save best_model.pt, metric: 44.23251589464124\n",
            "Epoch [48] Batch[12720] - loss: 0.929863  acc: 71.8750%(23/32)\n",
            "Epoch [48] Batch[12740] - loss: 1.064000  acc: 62.5000%(20/32)\n",
            "Epoch [48] Batch[12760] - loss: 1.037739  acc: 68.7500%(22/32)\n",
            "Epoch [48] Batch[12780] - loss: 1.092289  acc: 53.1250%(17/32)\n",
            "Epoch [48] Batch[12800] - loss: 1.088461  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.272462  acc: 43.9600%(484/1101) \n",
            "\n",
            "Epoch [49] Batch[12820] - loss: 1.115861  acc: 56.2500%(18/32)\n",
            "Epoch [49] Batch[12840] - loss: 1.049905  acc: 43.7500%(14/32)\n",
            "Epoch [49] Batch[12860] - loss: 1.111435  acc: 59.3750%(19/32)\n",
            "Epoch [49] Batch[12880] - loss: 1.073410  acc: 56.2500%(18/32)\n",
            "Epoch [49] Batch[12900] - loss: 1.041940  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.272337  acc: 43.9600%(484/1101) \n",
            "\n",
            "Epoch [49] Batch[12920] - loss: 1.120747  acc: 53.1250%(17/32)\n",
            "Epoch [49] Batch[12940] - loss: 0.964014  acc: 65.6250%(21/32)\n",
            "Epoch [49] Batch[12960] - loss: 1.189847  acc: 53.1250%(17/32)\n",
            "Epoch [49] Batch[12980] - loss: 1.083368  acc: 50.0000%(16/32)\n",
            "Epoch [49] Batch[13000] - loss: 1.147212  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.271028  acc: 43.8692%(483/1101) \n",
            "\n",
            "Epoch [49] Batch[13020] - loss: 1.163571  acc: 50.0000%(16/32)\n",
            "Epoch [49] Batch[13040] - loss: 1.039798  acc: 56.2500%(18/32)\n",
            "Epoch [49] Batch[13060] - loss: 0.923057  acc: 71.8750%(23/32)\n",
            "Epoch [49] Batch[13080] - loss: 1.036553  acc: 59.3750%(19/32)\n",
            "Epoch [50] Batch[13100] - loss: 1.080720  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.270666  acc: 44.1417%(486/1101) \n",
            "\n",
            "Epoch [50] Batch[13120] - loss: 0.998436  acc: 68.7500%(22/32)\n",
            "Epoch [50] Batch[13140] - loss: 1.011606  acc: 62.5000%(20/32)\n",
            "Epoch [50] Batch[13160] - loss: 1.028449  acc: 68.7500%(22/32)\n",
            "Epoch [50] Batch[13180] - loss: 1.140886  acc: 53.1250%(17/32)\n",
            "Epoch [50] Batch[13200] - loss: 0.900410  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.269544  acc: 44.3233%(488/1101) \n",
            "\n",
            "save best_model.pt, metric: 44.323342415985465\n",
            "Epoch [50] Batch[13220] - loss: 0.983761  acc: 68.7500%(22/32)\n",
            "Epoch [50] Batch[13240] - loss: 1.171482  acc: 59.3750%(19/32)\n",
            "Epoch [50] Batch[13260] - loss: 1.166304  acc: 43.7500%(14/32)\n",
            "Epoch [50] Batch[13280] - loss: 1.093237  acc: 59.3750%(19/32)\n",
            "Epoch [50] Batch[13300] - loss: 1.063702  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.269430  acc: 44.4142%(489/1101) \n",
            "\n",
            "save best_model.pt, metric: 44.4141689373297\n",
            "Epoch [50] Batch[13320] - loss: 1.130410  acc: 53.1250%(17/32)\n",
            "Epoch [50] Batch[13340] - loss: 0.927864  acc: 68.7500%(22/32)\n",
            "Epoch [51] Batch[13360] - loss: 1.134066  acc: 53.1250%(17/32)\n",
            "Epoch [51] Batch[13380] - loss: 1.045769  acc: 62.5000%(20/32)\n",
            "Epoch [51] Batch[13400] - loss: 1.242538  acc: 37.5000%(12/32)\n",
            "\n",
            "Evaluation - loss: 1.270441  acc: 43.7784%(482/1101) \n",
            "\n",
            "Epoch [51] Batch[13420] - loss: 0.889445  acc: 78.1250%(25/32)\n",
            "Epoch [51] Batch[13440] - loss: 1.127991  acc: 53.1250%(17/32)\n",
            "Epoch [51] Batch[13460] - loss: 1.119828  acc: 56.2500%(18/32)\n",
            "Epoch [51] Batch[13480] - loss: 1.040351  acc: 65.6250%(21/32)\n",
            "Epoch [51] Batch[13500] - loss: 1.122396  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.269127  acc: 43.6876%(481/1101) \n",
            "\n",
            "Epoch [51] Batch[13520] - loss: 0.929621  acc: 62.5000%(20/32)\n",
            "Epoch [51] Batch[13540] - loss: 0.952896  acc: 68.7500%(22/32)\n",
            "Epoch [51] Batch[13560] - loss: 1.113434  acc: 53.1250%(17/32)\n",
            "Epoch [51] Batch[13580] - loss: 1.056367  acc: 62.5000%(20/32)\n",
            "Epoch [51] Batch[13600] - loss: 1.187037  acc: 43.7500%(14/32)\n",
            "\n",
            "Evaluation - loss: 1.270137  acc: 44.1417%(486/1101) \n",
            "\n",
            "Epoch [52] Batch[13620] - loss: 0.922106  acc: 75.0000%(24/32)\n",
            "Epoch [52] Batch[13640] - loss: 1.025329  acc: 62.5000%(20/32)\n",
            "Epoch [52] Batch[13660] - loss: 0.962131  acc: 71.8750%(23/32)\n",
            "Epoch [52] Batch[13680] - loss: 1.005666  acc: 65.6250%(21/32)\n",
            "Epoch [52] Batch[13700] - loss: 1.053280  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.268405  acc: 44.5958%(491/1101) \n",
            "\n",
            "save best_model.pt, metric: 44.595821980018165\n",
            "Epoch [52] Batch[13720] - loss: 1.043502  acc: 56.2500%(18/32)\n",
            "Epoch [52] Batch[13740] - loss: 0.957007  acc: 78.1250%(25/32)\n",
            "Epoch [52] Batch[13760] - loss: 1.046160  acc: 68.7500%(22/32)\n",
            "Epoch [52] Batch[13780] - loss: 1.132668  acc: 56.2500%(18/32)\n",
            "Epoch [52] Batch[13800] - loss: 1.007955  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.268928  acc: 43.5967%(480/1101) \n",
            "\n",
            "Epoch [52] Batch[13820] - loss: 0.993446  acc: 65.6250%(21/32)\n",
            "Epoch [52] Batch[13840] - loss: 0.916145  acc: 75.0000%(24/32)\n",
            "Epoch [52] Batch[13860] - loss: 0.958533  acc: 68.7500%(22/32)\n",
            "Epoch [52] Batch[13880] - loss: 0.937667  acc: 68.7500%(22/32)\n",
            "Epoch [53] Batch[13900] - loss: 1.176735  acc: 37.5000%(12/32)\n",
            "\n",
            "Evaluation - loss: 1.268577  acc: 43.9600%(484/1101) \n",
            "\n",
            "Epoch [53] Batch[13920] - loss: 1.017399  acc: 62.5000%(20/32)\n",
            "Epoch [53] Batch[13940] - loss: 1.202986  acc: 59.3750%(19/32)\n",
            "Epoch [53] Batch[13960] - loss: 1.046630  acc: 56.2500%(18/32)\n",
            "Epoch [53] Batch[13980] - loss: 0.963401  acc: 65.6250%(21/32)\n",
            "Epoch [53] Batch[14000] - loss: 0.969415  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.269281  acc: 44.3233%(488/1101) \n",
            "\n",
            "Epoch [53] Batch[14020] - loss: 1.078231  acc: 53.1250%(17/32)\n",
            "Epoch [53] Batch[14040] - loss: 1.142640  acc: 56.2500%(18/32)\n",
            "Epoch [53] Batch[14060] - loss: 1.028947  acc: 56.2500%(18/32)\n",
            "Epoch [53] Batch[14080] - loss: 1.175322  acc: 56.2500%(18/32)\n",
            "Epoch [53] Batch[14100] - loss: 1.266824  acc: 40.6250%(13/32)\n",
            "\n",
            "Evaluation - loss: 1.266542  acc: 43.9600%(484/1101) \n",
            "\n",
            "Epoch [53] Batch[14120] - loss: 1.006973  acc: 65.6250%(21/32)\n",
            "Epoch [53] Batch[14140] - loss: 1.044312  acc: 56.2500%(18/32)\n",
            "Epoch [54] Batch[14160] - loss: 1.092305  acc: 56.2500%(18/32)\n",
            "Epoch [54] Batch[14180] - loss: 1.276560  acc: 50.0000%(16/32)\n",
            "Epoch [54] Batch[14200] - loss: 1.033814  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.267305  acc: 43.6876%(481/1101) \n",
            "\n",
            "Epoch [54] Batch[14220] - loss: 1.137198  acc: 53.1250%(17/32)\n",
            "Epoch [54] Batch[14240] - loss: 1.100868  acc: 59.3750%(19/32)\n",
            "Epoch [54] Batch[14260] - loss: 0.937405  acc: 71.8750%(23/32)\n",
            "Epoch [54] Batch[14280] - loss: 0.945171  acc: 71.8750%(23/32)\n",
            "Epoch [54] Batch[14300] - loss: 1.136961  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.267084  acc: 44.0509%(485/1101) \n",
            "\n",
            "Epoch [54] Batch[14320] - loss: 1.176979  acc: 46.8750%(15/32)\n",
            "Epoch [54] Batch[14340] - loss: 1.041184  acc: 65.6250%(21/32)\n",
            "Epoch [54] Batch[14360] - loss: 1.024242  acc: 59.3750%(19/32)\n",
            "Epoch [54] Batch[14380] - loss: 1.019261  acc: 68.7500%(22/32)\n",
            "Epoch [54] Batch[14400] - loss: 0.901804  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.265372  acc: 44.9591%(495/1101) \n",
            "\n",
            "save best_model.pt, metric: 44.9591280653951\n",
            "Epoch [55] Batch[14420] - loss: 1.059223  acc: 62.5000%(20/32)\n",
            "Epoch [55] Batch[14440] - loss: 1.107582  acc: 53.1250%(17/32)\n",
            "Epoch [55] Batch[14460] - loss: 1.088309  acc: 50.0000%(16/32)\n",
            "Epoch [55] Batch[14480] - loss: 0.949274  acc: 68.7500%(22/32)\n",
            "Epoch [55] Batch[14500] - loss: 1.011406  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.265840  acc: 44.3233%(488/1101) \n",
            "\n",
            "Epoch [55] Batch[14520] - loss: 1.088999  acc: 62.5000%(20/32)\n",
            "Epoch [55] Batch[14540] - loss: 1.097837  acc: 56.2500%(18/32)\n",
            "Epoch [55] Batch[14560] - loss: 0.876257  acc: 78.1250%(25/32)\n",
            "Epoch [55] Batch[14580] - loss: 1.024095  acc: 75.0000%(24/32)\n",
            "Epoch [55] Batch[14600] - loss: 1.075525  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.266234  acc: 43.8692%(483/1101) \n",
            "\n",
            "Epoch [55] Batch[14620] - loss: 1.044774  acc: 53.1250%(17/32)\n",
            "Epoch [55] Batch[14640] - loss: 1.082086  acc: 62.5000%(20/32)\n",
            "Epoch [55] Batch[14660] - loss: 1.007484  acc: 59.3750%(19/32)\n",
            "Epoch [55] Batch[14680] - loss: 0.956546  acc: 75.0000%(24/32)\n",
            "Epoch [56] Batch[14700] - loss: 1.005540  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.266404  acc: 45.1408%(497/1101) \n",
            "\n",
            "save best_model.pt, metric: 45.14078110808356\n",
            "Epoch [56] Batch[14720] - loss: 0.988169  acc: 62.5000%(20/32)\n",
            "Epoch [56] Batch[14740] - loss: 1.106042  acc: 56.2500%(18/32)\n",
            "Epoch [56] Batch[14760] - loss: 1.119959  acc: 65.6250%(21/32)\n",
            "Epoch [56] Batch[14780] - loss: 0.907665  acc: 65.6250%(21/32)\n",
            "Epoch [56] Batch[14800] - loss: 0.975453  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.266335  acc: 44.9591%(495/1101) \n",
            "\n",
            "Epoch [56] Batch[14820] - loss: 1.079965  acc: 53.1250%(17/32)\n",
            "Epoch [56] Batch[14840] - loss: 1.001629  acc: 59.3750%(19/32)\n",
            "Epoch [56] Batch[14860] - loss: 1.097626  acc: 56.2500%(18/32)\n",
            "Epoch [56] Batch[14880] - loss: 0.997733  acc: 68.7500%(22/32)\n",
            "Epoch [56] Batch[14900] - loss: 1.069406  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.265635  acc: 44.2325%(487/1101) \n",
            "\n",
            "Epoch [56] Batch[14920] - loss: 1.168851  acc: 62.5000%(20/32)\n",
            "Epoch [56] Batch[14940] - loss: 0.972718  acc: 68.7500%(22/32)\n",
            "Epoch [57] Batch[14960] - loss: 1.018793  acc: 65.6250%(21/32)\n",
            "Epoch [57] Batch[14980] - loss: 1.044392  acc: 68.7500%(22/32)\n",
            "Epoch [57] Batch[15000] - loss: 1.016807  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.264309  acc: 44.4142%(489/1101) \n",
            "\n",
            "Epoch [57] Batch[15020] - loss: 1.070136  acc: 50.0000%(16/32)\n",
            "Epoch [57] Batch[15040] - loss: 0.916167  acc: 65.6250%(21/32)\n",
            "Epoch [57] Batch[15060] - loss: 1.029021  acc: 59.3750%(19/32)\n",
            "Epoch [57] Batch[15080] - loss: 0.966719  acc: 68.7500%(22/32)\n",
            "Epoch [57] Batch[15100] - loss: 0.863616  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.265387  acc: 45.0500%(496/1101) \n",
            "\n",
            "Epoch [57] Batch[15120] - loss: 1.192195  acc: 37.5000%(12/32)\n",
            "Epoch [57] Batch[15140] - loss: 1.072888  acc: 62.5000%(20/32)\n",
            "Epoch [57] Batch[15160] - loss: 1.058436  acc: 50.0000%(16/32)\n",
            "Epoch [57] Batch[15180] - loss: 0.985600  acc: 65.6250%(21/32)\n",
            "Epoch [57] Batch[15200] - loss: 1.158640  acc: 50.0000%(16/32)\n",
            "\n",
            "Evaluation - loss: 1.264769  acc: 44.5050%(490/1101) \n",
            "\n",
            "Epoch [58] Batch[15220] - loss: 0.992002  acc: 59.3750%(19/32)\n",
            "Epoch [58] Batch[15240] - loss: 1.021918  acc: 62.5000%(20/32)\n",
            "Epoch [58] Batch[15260] - loss: 1.054154  acc: 56.2500%(18/32)\n",
            "Epoch [58] Batch[15280] - loss: 1.155227  acc: 56.2500%(18/32)\n",
            "Epoch [58] Batch[15300] - loss: 1.084761  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.263469  acc: 44.9591%(495/1101) \n",
            "\n",
            "Epoch [58] Batch[15320] - loss: 1.042481  acc: 65.6250%(21/32)\n",
            "Epoch [58] Batch[15340] - loss: 1.072008  acc: 56.2500%(18/32)\n",
            "Epoch [58] Batch[15360] - loss: 1.283545  acc: 50.0000%(16/32)\n",
            "Epoch [58] Batch[15380] - loss: 0.948390  acc: 78.1250%(25/32)\n",
            "Epoch [58] Batch[15400] - loss: 1.016350  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.264292  acc: 44.8683%(494/1101) \n",
            "\n",
            "Epoch [58] Batch[15420] - loss: 0.973596  acc: 62.5000%(20/32)\n",
            "Epoch [58] Batch[15440] - loss: 0.911682  acc: 75.0000%(24/32)\n",
            "Epoch [58] Batch[15460] - loss: 0.966558  acc: 71.8750%(23/32)\n",
            "Epoch [58] Batch[15480] - loss: 1.006617  acc: 68.7500%(22/32)\n",
            "Epoch [59] Batch[15500] - loss: 0.969216  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.263199  acc: 45.0500%(496/1101) \n",
            "\n",
            "Epoch [59] Batch[15520] - loss: 1.077018  acc: 56.2500%(18/32)\n",
            "Epoch [59] Batch[15540] - loss: 0.950391  acc: 62.5000%(20/32)\n",
            "Epoch [59] Batch[15560] - loss: 0.942015  acc: 59.3750%(19/32)\n",
            "Epoch [59] Batch[15580] - loss: 1.064804  acc: 59.3750%(19/32)\n",
            "Epoch [59] Batch[15600] - loss: 0.814579  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.264130  acc: 44.8683%(494/1101) \n",
            "\n",
            "Epoch [59] Batch[15620] - loss: 1.079089  acc: 59.3750%(19/32)\n",
            "Epoch [59] Batch[15640] - loss: 0.978407  acc: 71.8750%(23/32)\n",
            "Epoch [59] Batch[15660] - loss: 1.010065  acc: 68.7500%(22/32)\n",
            "Epoch [59] Batch[15680] - loss: 1.003339  acc: 65.6250%(21/32)\n",
            "Epoch [59] Batch[15700] - loss: 1.329479  acc: 46.8750%(15/32)\n",
            "\n",
            "Evaluation - loss: 1.262468  acc: 45.2316%(498/1101) \n",
            "\n",
            "save best_model.pt, metric: 45.23160762942779\n",
            "Epoch [59] Batch[15720] - loss: 1.178096  acc: 40.6250%(13/32)\n",
            "Epoch [59] Batch[15740] - loss: 1.177196  acc: 53.1250%(17/32)\n",
            "Epoch [60] Batch[15760] - loss: 0.852556  acc: 78.1250%(25/32)\n",
            "Epoch [60] Batch[15780] - loss: 0.938571  acc: 59.3750%(19/32)\n",
            "Epoch [60] Batch[15800] - loss: 1.098324  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.264022  acc: 44.6866%(492/1101) \n",
            "\n",
            "Epoch [60] Batch[15820] - loss: 0.902237  acc: 62.5000%(20/32)\n",
            "Epoch [60] Batch[15840] - loss: 0.941327  acc: 68.7500%(22/32)\n",
            "Epoch [60] Batch[15860] - loss: 0.871151  acc: 81.2500%(26/32)\n",
            "Epoch [60] Batch[15880] - loss: 0.917131  acc: 68.7500%(22/32)\n",
            "Epoch [60] Batch[15900] - loss: 1.038460  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.264689  acc: 44.5958%(491/1101) \n",
            "\n",
            "Epoch [60] Batch[15920] - loss: 1.061325  acc: 65.6250%(21/32)\n",
            "Epoch [60] Batch[15940] - loss: 0.869158  acc: 75.0000%(24/32)\n",
            "Epoch [60] Batch[15960] - loss: 1.086801  acc: 56.2500%(18/32)\n",
            "Epoch [60] Batch[15980] - loss: 0.926217  acc: 68.7500%(22/32)\n",
            "Epoch [60] Batch[16000] - loss: 1.058177  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.262264  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [60] Batch[16020] - loss: 1.063127  acc: 71.8750%(23/32)\n",
            "Epoch [61] Batch[16040] - loss: 0.974476  acc: 68.7500%(22/32)\n",
            "Epoch [61] Batch[16060] - loss: 0.827097  acc: 68.7500%(22/32)\n",
            "Epoch [61] Batch[16080] - loss: 0.999203  acc: 56.2500%(18/32)\n",
            "Epoch [61] Batch[16100] - loss: 0.886646  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.261239  acc: 45.1408%(497/1101) \n",
            "\n",
            "Epoch [61] Batch[16120] - loss: 0.972665  acc: 65.6250%(21/32)\n",
            "Epoch [61] Batch[16140] - loss: 1.047283  acc: 56.2500%(18/32)\n",
            "Epoch [61] Batch[16160] - loss: 0.930884  acc: 71.8750%(23/32)\n",
            "Epoch [61] Batch[16180] - loss: 0.930705  acc: 62.5000%(20/32)\n",
            "Epoch [61] Batch[16200] - loss: 1.129656  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.261736  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [61] Batch[16220] - loss: 0.902743  acc: 65.6250%(21/32)\n",
            "Epoch [61] Batch[16240] - loss: 1.205123  acc: 50.0000%(16/32)\n",
            "Epoch [61] Batch[16260] - loss: 0.896052  acc: 71.8750%(23/32)\n",
            "Epoch [61] Batch[16280] - loss: 1.088888  acc: 56.2500%(18/32)\n",
            "Epoch [62] Batch[16300] - loss: 0.985050  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.262702  acc: 44.9591%(495/1101) \n",
            "\n",
            "Epoch [62] Batch[16320] - loss: 1.076473  acc: 50.0000%(16/32)\n",
            "Epoch [62] Batch[16340] - loss: 0.920078  acc: 71.8750%(23/32)\n",
            "Epoch [62] Batch[16360] - loss: 0.992743  acc: 68.7500%(22/32)\n",
            "Epoch [62] Batch[16380] - loss: 1.044993  acc: 59.3750%(19/32)\n",
            "Epoch [62] Batch[16400] - loss: 1.048306  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.263497  acc: 44.8683%(494/1101) \n",
            "\n",
            "Epoch [62] Batch[16420] - loss: 0.933089  acc: 75.0000%(24/32)\n",
            "Epoch [62] Batch[16440] - loss: 0.992873  acc: 68.7500%(22/32)\n",
            "Epoch [62] Batch[16460] - loss: 1.154177  acc: 56.2500%(18/32)\n",
            "Epoch [62] Batch[16480] - loss: 0.964980  acc: 59.3750%(19/32)\n",
            "Epoch [62] Batch[16500] - loss: 0.817056  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.259985  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [62] Batch[16520] - loss: 0.958399  acc: 75.0000%(24/32)\n",
            "Epoch [62] Batch[16540] - loss: 0.948721  acc: 75.0000%(24/32)\n",
            "Epoch [63] Batch[16560] - loss: 0.853215  acc: 71.8750%(23/32)\n",
            "Epoch [63] Batch[16580] - loss: 0.944641  acc: 68.7500%(22/32)\n",
            "Epoch [63] Batch[16600] - loss: 1.063680  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.263013  acc: 45.1408%(497/1101) \n",
            "\n",
            "Epoch [63] Batch[16620] - loss: 0.911866  acc: 68.7500%(22/32)\n",
            "Epoch [63] Batch[16640] - loss: 1.070642  acc: 62.5000%(20/32)\n",
            "Epoch [63] Batch[16660] - loss: 1.200261  acc: 50.0000%(16/32)\n",
            "Epoch [63] Batch[16680] - loss: 0.885276  acc: 65.6250%(21/32)\n",
            "Epoch [63] Batch[16700] - loss: 0.904008  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.262276  acc: 44.9591%(495/1101) \n",
            "\n",
            "Epoch [63] Batch[16720] - loss: 1.094803  acc: 50.0000%(16/32)\n",
            "Epoch [63] Batch[16740] - loss: 0.954409  acc: 56.2500%(18/32)\n",
            "Epoch [63] Batch[16760] - loss: 0.839966  acc: 75.0000%(24/32)\n",
            "Epoch [63] Batch[16780] - loss: 0.885956  acc: 75.0000%(24/32)\n",
            "Epoch [63] Batch[16800] - loss: 0.983584  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.260233  acc: 45.0500%(496/1101) \n",
            "\n",
            "Epoch [63] Batch[16820] - loss: 0.909906  acc: 78.1250%(25/32)\n",
            "Epoch [64] Batch[16840] - loss: 0.903062  acc: 71.8750%(23/32)\n",
            "Epoch [64] Batch[16860] - loss: 0.965750  acc: 68.7500%(22/32)\n",
            "Epoch [64] Batch[16880] - loss: 0.853833  acc: 71.8750%(23/32)\n",
            "Epoch [64] Batch[16900] - loss: 0.876753  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.258784  acc: 45.3224%(499/1101) \n",
            "\n",
            "save best_model.pt, metric: 45.322434150772025\n",
            "Epoch [64] Batch[16920] - loss: 0.945435  acc: 68.7500%(22/32)\n",
            "Epoch [64] Batch[16940] - loss: 1.075454  acc: 53.1250%(17/32)\n",
            "Epoch [64] Batch[16960] - loss: 0.942530  acc: 68.7500%(22/32)\n",
            "Epoch [64] Batch[16980] - loss: 1.010192  acc: 62.5000%(20/32)\n",
            "Epoch [64] Batch[17000] - loss: 1.102754  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.259809  acc: 45.4133%(500/1101) \n",
            "\n",
            "save best_model.pt, metric: 45.41326067211626\n",
            "Epoch [64] Batch[17020] - loss: 0.922778  acc: 75.0000%(24/32)\n",
            "Epoch [64] Batch[17040] - loss: 1.077487  acc: 50.0000%(16/32)\n",
            "Epoch [64] Batch[17060] - loss: 1.092389  acc: 53.1250%(17/32)\n",
            "Epoch [64] Batch[17080] - loss: 0.968451  acc: 59.3750%(19/32)\n",
            "Epoch [65] Batch[17100] - loss: 1.105598  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.262071  acc: 44.8683%(494/1101) \n",
            "\n",
            "Epoch [65] Batch[17120] - loss: 1.073148  acc: 46.8750%(15/32)\n",
            "Epoch [65] Batch[17140] - loss: 0.938485  acc: 68.7500%(22/32)\n",
            "Epoch [65] Batch[17160] - loss: 1.054355  acc: 56.2500%(18/32)\n",
            "Epoch [65] Batch[17180] - loss: 1.002768  acc: 56.2500%(18/32)\n",
            "Epoch [65] Batch[17200] - loss: 1.002068  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.261092  acc: 45.3224%(499/1101) \n",
            "\n",
            "Epoch [65] Batch[17220] - loss: 0.903427  acc: 75.0000%(24/32)\n",
            "Epoch [65] Batch[17240] - loss: 1.114249  acc: 56.2500%(18/32)\n",
            "Epoch [65] Batch[17260] - loss: 0.887063  acc: 71.8750%(23/32)\n",
            "Epoch [65] Batch[17280] - loss: 0.910589  acc: 65.6250%(21/32)\n",
            "Epoch [65] Batch[17300] - loss: 1.013270  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.259453  acc: 45.4133%(500/1101) \n",
            "\n",
            "Epoch [65] Batch[17320] - loss: 1.043941  acc: 53.1250%(17/32)\n",
            "Epoch [65] Batch[17340] - loss: 0.960400  acc: 62.5000%(20/32)\n",
            "Epoch [66] Batch[17360] - loss: 0.983166  acc: 59.3750%(19/32)\n",
            "Epoch [66] Batch[17380] - loss: 0.960308  acc: 68.7500%(22/32)\n",
            "Epoch [66] Batch[17400] - loss: 0.835081  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.261087  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [66] Batch[17420] - loss: 0.894353  acc: 78.1250%(25/32)\n",
            "Epoch [66] Batch[17440] - loss: 0.933576  acc: 71.8750%(23/32)\n",
            "Epoch [66] Batch[17460] - loss: 0.919029  acc: 68.7500%(22/32)\n",
            "Epoch [66] Batch[17480] - loss: 1.036537  acc: 62.5000%(20/32)\n",
            "Epoch [66] Batch[17500] - loss: 0.907943  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.258883  acc: 45.8674%(505/1101) \n",
            "\n",
            "save best_model.pt, metric: 45.86739327883742\n",
            "Epoch [66] Batch[17520] - loss: 0.949483  acc: 71.8750%(23/32)\n",
            "Epoch [66] Batch[17540] - loss: 0.951476  acc: 65.6250%(21/32)\n",
            "Epoch [66] Batch[17560] - loss: 0.975597  acc: 62.5000%(20/32)\n",
            "Epoch [66] Batch[17580] - loss: 0.991514  acc: 56.2500%(18/32)\n",
            "Epoch [66] Batch[17600] - loss: 1.105728  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.258922  acc: 45.2316%(498/1101) \n",
            "\n",
            "Epoch [66] Batch[17620] - loss: 1.030862  acc: 62.5000%(20/32)\n",
            "Epoch [67] Batch[17640] - loss: 0.959656  acc: 62.5000%(20/32)\n",
            "Epoch [67] Batch[17660] - loss: 0.912203  acc: 62.5000%(20/32)\n",
            "Epoch [67] Batch[17680] - loss: 0.963065  acc: 56.2500%(18/32)\n",
            "Epoch [67] Batch[17700] - loss: 0.899680  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.259095  acc: 45.2316%(498/1101) \n",
            "\n",
            "Epoch [67] Batch[17720] - loss: 0.928189  acc: 62.5000%(20/32)\n",
            "Epoch [67] Batch[17740] - loss: 1.018342  acc: 56.2500%(18/32)\n",
            "Epoch [67] Batch[17760] - loss: 0.873862  acc: 75.0000%(24/32)\n",
            "Epoch [67] Batch[17780] - loss: 1.092093  acc: 62.5000%(20/32)\n",
            "Epoch [67] Batch[17800] - loss: 0.919472  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.258784  acc: 45.6857%(503/1101) \n",
            "\n",
            "Epoch [67] Batch[17820] - loss: 1.036513  acc: 50.0000%(16/32)\n",
            "Epoch [67] Batch[17840] - loss: 0.873669  acc: 71.8750%(23/32)\n",
            "Epoch [67] Batch[17860] - loss: 0.931609  acc: 65.6250%(21/32)\n",
            "Epoch [67] Batch[17880] - loss: 0.967766  acc: 68.7500%(22/32)\n",
            "Epoch [68] Batch[17900] - loss: 1.049888  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.257995  acc: 45.5949%(502/1101) \n",
            "\n",
            "Epoch [68] Batch[17920] - loss: 0.885471  acc: 59.3750%(19/32)\n",
            "Epoch [68] Batch[17940] - loss: 1.104888  acc: 50.0000%(16/32)\n",
            "Epoch [68] Batch[17960] - loss: 0.966440  acc: 65.6250%(21/32)\n",
            "Epoch [68] Batch[17980] - loss: 1.060749  acc: 62.5000%(20/32)\n",
            "Epoch [68] Batch[18000] - loss: 1.004764  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.260738  acc: 44.8683%(494/1101) \n",
            "\n",
            "Epoch [68] Batch[18020] - loss: 1.004598  acc: 68.7500%(22/32)\n",
            "Epoch [68] Batch[18040] - loss: 0.981273  acc: 59.3750%(19/32)\n",
            "Epoch [68] Batch[18060] - loss: 0.886007  acc: 65.6250%(21/32)\n",
            "Epoch [68] Batch[18080] - loss: 0.852158  acc: 65.6250%(21/32)\n",
            "Epoch [68] Batch[18100] - loss: 1.042878  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.258541  acc: 45.3224%(499/1101) \n",
            "\n",
            "Epoch [68] Batch[18120] - loss: 0.798013  acc: 75.0000%(24/32)\n",
            "Epoch [68] Batch[18140] - loss: 0.946776  acc: 65.6250%(21/32)\n",
            "Epoch [69] Batch[18160] - loss: 0.885519  acc: 75.0000%(24/32)\n",
            "Epoch [69] Batch[18180] - loss: 0.853833  acc: 78.1250%(25/32)\n",
            "Epoch [69] Batch[18200] - loss: 0.797702  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.256151  acc: 45.3224%(499/1101) \n",
            "\n",
            "Epoch [69] Batch[18220] - loss: 1.024446  acc: 71.8750%(23/32)\n",
            "Epoch [69] Batch[18240] - loss: 0.863097  acc: 65.6250%(21/32)\n",
            "Epoch [69] Batch[18260] - loss: 0.983013  acc: 68.7500%(22/32)\n",
            "Epoch [69] Batch[18280] - loss: 0.816328  acc: 78.1250%(25/32)\n",
            "Epoch [69] Batch[18300] - loss: 0.817433  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.258574  acc: 45.1408%(497/1101) \n",
            "\n",
            "Epoch [69] Batch[18320] - loss: 1.085395  acc: 53.1250%(17/32)\n",
            "Epoch [69] Batch[18340] - loss: 0.885062  acc: 71.8750%(23/32)\n",
            "Epoch [69] Batch[18360] - loss: 0.859686  acc: 71.8750%(23/32)\n",
            "Epoch [69] Batch[18380] - loss: 0.868644  acc: 65.6250%(21/32)\n",
            "Epoch [69] Batch[18400] - loss: 0.890502  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.259012  acc: 45.0500%(496/1101) \n",
            "\n",
            "Epoch [69] Batch[18420] - loss: 1.015816  acc: 56.2500%(18/32)\n",
            "Epoch [70] Batch[18440] - loss: 0.982751  acc: 71.8750%(23/32)\n",
            "Epoch [70] Batch[18460] - loss: 1.007699  acc: 75.0000%(24/32)\n",
            "Epoch [70] Batch[18480] - loss: 0.996367  acc: 50.0000%(16/32)\n",
            "Epoch [70] Batch[18500] - loss: 0.967211  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.258251  acc: 45.6857%(503/1101) \n",
            "\n",
            "Epoch [70] Batch[18520] - loss: 1.131025  acc: 34.3750%(11/32)\n",
            "Epoch [70] Batch[18540] - loss: 0.967353  acc: 68.7500%(22/32)\n",
            "Epoch [70] Batch[18560] - loss: 0.980748  acc: 56.2500%(18/32)\n",
            "Epoch [70] Batch[18580] - loss: 0.984359  acc: 59.3750%(19/32)\n",
            "Epoch [70] Batch[18600] - loss: 0.874564  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.256687  acc: 45.5041%(501/1101) \n",
            "\n",
            "Epoch [70] Batch[18620] - loss: 0.957428  acc: 62.5000%(20/32)\n",
            "Epoch [70] Batch[18640] - loss: 0.854251  acc: 65.6250%(21/32)\n",
            "Epoch [70] Batch[18660] - loss: 0.939494  acc: 62.5000%(20/32)\n",
            "Epoch [70] Batch[18680] - loss: 0.922979  acc: 71.8750%(23/32)\n",
            "Epoch [71] Batch[18700] - loss: 0.977211  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.257686  acc: 45.5041%(501/1101) \n",
            "\n",
            "Epoch [71] Batch[18720] - loss: 0.896968  acc: 62.5000%(20/32)\n",
            "Epoch [71] Batch[18740] - loss: 0.842309  acc: 81.2500%(26/32)\n",
            "Epoch [71] Batch[18760] - loss: 0.775283  acc: 78.1250%(25/32)\n",
            "Epoch [71] Batch[18780] - loss: 0.951323  acc: 62.5000%(20/32)\n",
            "Epoch [71] Batch[18800] - loss: 0.906434  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.256760  acc: 45.5041%(501/1101) \n",
            "\n",
            "Epoch [71] Batch[18820] - loss: 1.029704  acc: 62.5000%(20/32)\n",
            "Epoch [71] Batch[18840] - loss: 0.855498  acc: 71.8750%(23/32)\n",
            "Epoch [71] Batch[18860] - loss: 0.944214  acc: 53.1250%(17/32)\n",
            "Epoch [71] Batch[18880] - loss: 0.896301  acc: 65.6250%(21/32)\n",
            "Epoch [71] Batch[18900] - loss: 0.959652  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.257157  acc: 45.2316%(498/1101) \n",
            "\n",
            "Epoch [71] Batch[18920] - loss: 0.978510  acc: 68.7500%(22/32)\n",
            "Epoch [71] Batch[18940] - loss: 0.871084  acc: 71.8750%(23/32)\n",
            "Epoch [72] Batch[18960] - loss: 0.880334  acc: 68.7500%(22/32)\n",
            "Epoch [72] Batch[18980] - loss: 0.892636  acc: 71.8750%(23/32)\n",
            "Epoch [72] Batch[19000] - loss: 0.821729  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.258434  acc: 44.9591%(495/1101) \n",
            "\n",
            "Epoch [72] Batch[19020] - loss: 1.100006  acc: 59.3750%(19/32)\n",
            "Epoch [72] Batch[19040] - loss: 0.896658  acc: 68.7500%(22/32)\n",
            "Epoch [72] Batch[19060] - loss: 0.822116  acc: 75.0000%(24/32)\n",
            "Epoch [72] Batch[19080] - loss: 1.083507  acc: 59.3750%(19/32)\n",
            "Epoch [72] Batch[19100] - loss: 0.835033  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.255412  acc: 46.1399%(508/1101) \n",
            "\n",
            "save best_model.pt, metric: 46.13987284287012\n",
            "Epoch [72] Batch[19120] - loss: 1.008602  acc: 62.5000%(20/32)\n",
            "Epoch [72] Batch[19140] - loss: 0.962238  acc: 71.8750%(23/32)\n",
            "Epoch [72] Batch[19160] - loss: 0.876380  acc: 62.5000%(20/32)\n",
            "Epoch [72] Batch[19180] - loss: 0.766354  acc: 81.2500%(26/32)\n",
            "Epoch [72] Batch[19200] - loss: 0.980256  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.256485  acc: 45.5041%(501/1101) \n",
            "\n",
            "Epoch [72] Batch[19220] - loss: 0.935414  acc: 68.7500%(22/32)\n",
            "Epoch [73] Batch[19240] - loss: 1.072766  acc: 53.1250%(17/32)\n",
            "Epoch [73] Batch[19260] - loss: 0.903205  acc: 78.1250%(25/32)\n",
            "Epoch [73] Batch[19280] - loss: 0.931512  acc: 65.6250%(21/32)\n",
            "Epoch [73] Batch[19300] - loss: 1.121903  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.255867  acc: 45.5949%(502/1101) \n",
            "\n",
            "Epoch [73] Batch[19320] - loss: 0.855563  acc: 75.0000%(24/32)\n",
            "Epoch [73] Batch[19340] - loss: 0.873168  acc: 78.1250%(25/32)\n",
            "Epoch [73] Batch[19360] - loss: 0.911503  acc: 71.8750%(23/32)\n",
            "Epoch [73] Batch[19380] - loss: 0.770956  acc: 71.8750%(23/32)\n",
            "Epoch [73] Batch[19400] - loss: 0.907495  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.256865  acc: 45.0500%(496/1101) \n",
            "\n",
            "Epoch [73] Batch[19420] - loss: 0.867628  acc: 78.1250%(25/32)\n",
            "Epoch [73] Batch[19440] - loss: 0.808813  acc: 68.7500%(22/32)\n",
            "Epoch [73] Batch[19460] - loss: 0.828056  acc: 71.8750%(23/32)\n",
            "Epoch [73] Batch[19480] - loss: 0.925483  acc: 59.3750%(19/32)\n",
            "Epoch [74] Batch[19500] - loss: 1.049527  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.255181  acc: 45.8674%(505/1101) \n",
            "\n",
            "Epoch [74] Batch[19520] - loss: 0.809931  acc: 78.1250%(25/32)\n",
            "Epoch [74] Batch[19540] - loss: 0.871285  acc: 71.8750%(23/32)\n",
            "Epoch [74] Batch[19560] - loss: 1.064358  acc: 50.0000%(16/32)\n",
            "Epoch [74] Batch[19580] - loss: 0.922998  acc: 75.0000%(24/32)\n",
            "Epoch [74] Batch[19600] - loss: 0.917696  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.255821  acc: 45.2316%(498/1101) \n",
            "\n",
            "Epoch [74] Batch[19620] - loss: 0.959373  acc: 65.6250%(21/32)\n",
            "Epoch [74] Batch[19640] - loss: 0.855081  acc: 75.0000%(24/32)\n",
            "Epoch [74] Batch[19660] - loss: 0.776987  acc: 75.0000%(24/32)\n",
            "Epoch [74] Batch[19680] - loss: 0.859238  acc: 68.7500%(22/32)\n",
            "Epoch [74] Batch[19700] - loss: 0.864456  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.256609  acc: 45.5041%(501/1101) \n",
            "\n",
            "Epoch [74] Batch[19720] - loss: 0.941853  acc: 68.7500%(22/32)\n",
            "Epoch [74] Batch[19740] - loss: 0.857688  acc: 68.7500%(22/32)\n",
            "Epoch [75] Batch[19760] - loss: 0.940707  acc: 65.6250%(21/32)\n",
            "Epoch [75] Batch[19780] - loss: 0.789716  acc: 59.3750%(19/32)\n",
            "Epoch [75] Batch[19800] - loss: 1.011901  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.254189  acc: 45.5949%(502/1101) \n",
            "\n",
            "Epoch [75] Batch[19820] - loss: 0.858626  acc: 75.0000%(24/32)\n",
            "Epoch [75] Batch[19840] - loss: 0.931582  acc: 65.6250%(21/32)\n",
            "Epoch [75] Batch[19860] - loss: 1.019835  acc: 62.5000%(20/32)\n",
            "Epoch [75] Batch[19880] - loss: 0.977146  acc: 62.5000%(20/32)\n",
            "Epoch [75] Batch[19900] - loss: 0.896189  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.255720  acc: 45.2316%(498/1101) \n",
            "\n",
            "Epoch [75] Batch[19920] - loss: 0.910161  acc: 75.0000%(24/32)\n",
            "Epoch [75] Batch[19940] - loss: 0.865900  acc: 71.8750%(23/32)\n",
            "Epoch [75] Batch[19960] - loss: 0.908977  acc: 75.0000%(24/32)\n",
            "Epoch [75] Batch[19980] - loss: 0.990229  acc: 65.6250%(21/32)\n",
            "Epoch [75] Batch[20000] - loss: 1.044849  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.255461  acc: 45.5949%(502/1101) \n",
            "\n",
            "Epoch [75] Batch[20020] - loss: 0.986108  acc: 62.5000%(20/32)\n",
            "Epoch [76] Batch[20040] - loss: 0.758787  acc: 81.2500%(26/32)\n",
            "Epoch [76] Batch[20060] - loss: 0.939350  acc: 71.8750%(23/32)\n",
            "Epoch [76] Batch[20080] - loss: 0.753857  acc: 84.3750%(27/32)\n",
            "Epoch [76] Batch[20100] - loss: 0.863472  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.255482  acc: 45.6857%(503/1101) \n",
            "\n",
            "Epoch [76] Batch[20120] - loss: 0.895189  acc: 65.6250%(21/32)\n",
            "Epoch [76] Batch[20140] - loss: 0.964046  acc: 75.0000%(24/32)\n",
            "Epoch [76] Batch[20160] - loss: 1.016928  acc: 62.5000%(20/32)\n",
            "Epoch [76] Batch[20180] - loss: 0.790933  acc: 87.5000%(28/32)\n",
            "Epoch [76] Batch[20200] - loss: 0.921434  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.255939  acc: 45.6857%(503/1101) \n",
            "\n",
            "Epoch [76] Batch[20220] - loss: 0.780294  acc: 71.8750%(23/32)\n",
            "Epoch [76] Batch[20240] - loss: 0.936430  acc: 71.8750%(23/32)\n",
            "Epoch [76] Batch[20260] - loss: 0.721739  acc: 81.2500%(26/32)\n",
            "Epoch [76] Batch[20280] - loss: 0.966320  acc: 59.3750%(19/32)\n",
            "Epoch [77] Batch[20300] - loss: 0.973274  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.254153  acc: 45.3224%(499/1101) \n",
            "\n",
            "Epoch [77] Batch[20320] - loss: 0.789516  acc: 75.0000%(24/32)\n",
            "Epoch [77] Batch[20340] - loss: 0.969051  acc: 56.2500%(18/32)\n",
            "Epoch [77] Batch[20360] - loss: 0.928883  acc: 62.5000%(20/32)\n",
            "Epoch [77] Batch[20380] - loss: 0.933466  acc: 65.6250%(21/32)\n",
            "Epoch [77] Batch[20400] - loss: 1.000164  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.254820  acc: 45.5041%(501/1101) \n",
            "\n",
            "Epoch [77] Batch[20420] - loss: 1.062059  acc: 46.8750%(15/32)\n",
            "Epoch [77] Batch[20440] - loss: 0.985073  acc: 56.2500%(18/32)\n",
            "Epoch [77] Batch[20460] - loss: 0.904168  acc: 65.6250%(21/32)\n",
            "Epoch [77] Batch[20480] - loss: 0.855496  acc: 68.7500%(22/32)\n",
            "Epoch [77] Batch[20500] - loss: 0.861182  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.253536  acc: 45.8674%(505/1101) \n",
            "\n",
            "Epoch [77] Batch[20520] - loss: 0.898765  acc: 68.7500%(22/32)\n",
            "Epoch [77] Batch[20540] - loss: 1.029416  acc: 59.3750%(19/32)\n",
            "Epoch [78] Batch[20560] - loss: 0.886057  acc: 68.7500%(22/32)\n",
            "Epoch [78] Batch[20580] - loss: 0.817383  acc: 71.8750%(23/32)\n",
            "Epoch [78] Batch[20600] - loss: 0.858263  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.255563  acc: 45.5041%(501/1101) \n",
            "\n",
            "Epoch [78] Batch[20620] - loss: 0.851152  acc: 71.8750%(23/32)\n",
            "Epoch [78] Batch[20640] - loss: 0.953569  acc: 62.5000%(20/32)\n",
            "Epoch [78] Batch[20660] - loss: 0.883591  acc: 68.7500%(22/32)\n",
            "Epoch [78] Batch[20680] - loss: 0.847388  acc: 65.6250%(21/32)\n",
            "Epoch [78] Batch[20700] - loss: 0.946878  acc: 56.2500%(18/32)\n",
            "\n",
            "Evaluation - loss: 1.255449  acc: 45.2316%(498/1101) \n",
            "\n",
            "Epoch [78] Batch[20720] - loss: 0.850028  acc: 75.0000%(24/32)\n",
            "Epoch [78] Batch[20740] - loss: 0.898525  acc: 71.8750%(23/32)\n",
            "Epoch [78] Batch[20760] - loss: 0.917714  acc: 68.7500%(22/32)\n",
            "Epoch [78] Batch[20780] - loss: 0.839132  acc: 75.0000%(24/32)\n",
            "Epoch [78] Batch[20800] - loss: 0.975502  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.254869  acc: 45.6857%(503/1101) \n",
            "\n",
            "Epoch [78] Batch[20820] - loss: 0.722635  acc: 71.8750%(23/32)\n",
            "Epoch [79] Batch[20840] - loss: 0.776366  acc: 81.2500%(26/32)\n",
            "Epoch [79] Batch[20860] - loss: 0.844740  acc: 81.2500%(26/32)\n",
            "Epoch [79] Batch[20880] - loss: 0.798702  acc: 68.7500%(22/32)\n",
            "Epoch [79] Batch[20900] - loss: 0.970304  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.253473  acc: 46.0490%(507/1101) \n",
            "\n",
            "Epoch [79] Batch[20920] - loss: 0.864940  acc: 71.8750%(23/32)\n",
            "Epoch [79] Batch[20940] - loss: 1.021166  acc: 59.3750%(19/32)\n",
            "Epoch [79] Batch[20960] - loss: 0.812774  acc: 71.8750%(23/32)\n",
            "Epoch [79] Batch[20980] - loss: 0.908179  acc: 53.1250%(17/32)\n",
            "Epoch [79] Batch[21000] - loss: 0.959941  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.253580  acc: 45.8674%(505/1101) \n",
            "\n",
            "Epoch [79] Batch[21020] - loss: 1.068486  acc: 50.0000%(16/32)\n",
            "Epoch [79] Batch[21040] - loss: 0.858274  acc: 65.6250%(21/32)\n",
            "Epoch [79] Batch[21060] - loss: 0.887240  acc: 62.5000%(20/32)\n",
            "Epoch [79] Batch[21080] - loss: 1.051703  acc: 68.7500%(22/32)\n",
            "Epoch [80] Batch[21100] - loss: 0.937755  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.253541  acc: 45.6857%(503/1101) \n",
            "\n",
            "Epoch [80] Batch[21120] - loss: 0.980707  acc: 78.1250%(25/32)\n",
            "Epoch [80] Batch[21140] - loss: 0.765390  acc: 78.1250%(25/32)\n",
            "Epoch [80] Batch[21160] - loss: 0.852166  acc: 78.1250%(25/32)\n",
            "Epoch [80] Batch[21180] - loss: 1.022210  acc: 65.6250%(21/32)\n",
            "Epoch [80] Batch[21200] - loss: 0.888403  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.254518  acc: 45.5949%(502/1101) \n",
            "\n",
            "Epoch [80] Batch[21220] - loss: 0.862432  acc: 68.7500%(22/32)\n",
            "Epoch [80] Batch[21240] - loss: 0.711152  acc: 84.3750%(27/32)\n",
            "Epoch [80] Batch[21260] - loss: 0.846024  acc: 68.7500%(22/32)\n",
            "Epoch [80] Batch[21280] - loss: 0.881082  acc: 78.1250%(25/32)\n",
            "Epoch [80] Batch[21300] - loss: 0.786796  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.254268  acc: 45.6857%(503/1101) \n",
            "\n",
            "Epoch [80] Batch[21320] - loss: 0.832856  acc: 65.6250%(21/32)\n",
            "Epoch [80] Batch[21340] - loss: 0.848218  acc: 71.8750%(23/32)\n",
            "Epoch [80] Batch[21360] - loss: 0.935222  acc: 68.7500%(22/32)\n",
            "Epoch [81] Batch[21380] - loss: 1.101768  acc: 56.2500%(18/32)\n",
            "Epoch [81] Batch[21400] - loss: 0.733272  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.254410  acc: 45.7766%(504/1101) \n",
            "\n",
            "Epoch [81] Batch[21420] - loss: 0.959112  acc: 75.0000%(24/32)\n",
            "Epoch [81] Batch[21440] - loss: 0.919422  acc: 68.7500%(22/32)\n",
            "Epoch [81] Batch[21460] - loss: 0.745476  acc: 87.5000%(28/32)\n",
            "Epoch [81] Batch[21480] - loss: 1.116758  acc: 53.1250%(17/32)\n",
            "Epoch [81] Batch[21500] - loss: 0.908719  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.254599  acc: 45.7766%(504/1101) \n",
            "\n",
            "Epoch [81] Batch[21520] - loss: 0.812005  acc: 68.7500%(22/32)\n",
            "Epoch [81] Batch[21540] - loss: 0.736265  acc: 78.1250%(25/32)\n",
            "Epoch [81] Batch[21560] - loss: 0.785836  acc: 78.1250%(25/32)\n",
            "Epoch [81] Batch[21580] - loss: 0.897678  acc: 75.0000%(24/32)\n",
            "Epoch [81] Batch[21600] - loss: 0.818911  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.254920  acc: 45.5041%(501/1101) \n",
            "\n",
            "Epoch [81] Batch[21620] - loss: 0.815054  acc: 71.8750%(23/32)\n",
            "Epoch [82] Batch[21640] - loss: 0.904959  acc: 65.6250%(21/32)\n",
            "Epoch [82] Batch[21660] - loss: 0.831022  acc: 81.2500%(26/32)\n",
            "Epoch [82] Batch[21680] - loss: 0.832376  acc: 81.2500%(26/32)\n",
            "Epoch [82] Batch[21700] - loss: 1.090307  acc: 53.1250%(17/32)\n",
            "\n",
            "Evaluation - loss: 1.255390  acc: 45.3224%(499/1101) \n",
            "\n",
            "Epoch [82] Batch[21720] - loss: 0.819740  acc: 75.0000%(24/32)\n",
            "Epoch [82] Batch[21740] - loss: 0.921620  acc: 68.7500%(22/32)\n",
            "Epoch [82] Batch[21760] - loss: 0.953838  acc: 78.1250%(25/32)\n",
            "Epoch [82] Batch[21780] - loss: 0.682309  acc: 81.2500%(26/32)\n",
            "Epoch [82] Batch[21800] - loss: 0.845829  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.254520  acc: 46.2307%(509/1101) \n",
            "\n",
            "save best_model.pt, metric: 46.23069936421435\n",
            "Epoch [82] Batch[21820] - loss: 0.897038  acc: 68.7500%(22/32)\n",
            "Epoch [82] Batch[21840] - loss: 0.940028  acc: 56.2500%(18/32)\n",
            "Epoch [82] Batch[21860] - loss: 0.963722  acc: 65.6250%(21/32)\n",
            "Epoch [82] Batch[21880] - loss: 0.832146  acc: 81.2500%(26/32)\n",
            "Epoch [83] Batch[21900] - loss: 0.897629  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.253563  acc: 45.5949%(502/1101) \n",
            "\n",
            "Epoch [83] Batch[21920] - loss: 0.744579  acc: 81.2500%(26/32)\n",
            "Epoch [83] Batch[21940] - loss: 0.859567  acc: 71.8750%(23/32)\n",
            "Epoch [83] Batch[21960] - loss: 0.806872  acc: 65.6250%(21/32)\n",
            "Epoch [83] Batch[21980] - loss: 0.825803  acc: 68.7500%(22/32)\n",
            "Epoch [83] Batch[22000] - loss: 0.962604  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.256444  acc: 45.4133%(500/1101) \n",
            "\n",
            "Epoch [83] Batch[22020] - loss: 0.873554  acc: 71.8750%(23/32)\n",
            "Epoch [83] Batch[22040] - loss: 0.877150  acc: 71.8750%(23/32)\n",
            "Epoch [83] Batch[22060] - loss: 1.020625  acc: 56.2500%(18/32)\n",
            "Epoch [83] Batch[22080] - loss: 0.805331  acc: 71.8750%(23/32)\n",
            "Epoch [83] Batch[22100] - loss: 0.903278  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.253319  acc: 45.3224%(499/1101) \n",
            "\n",
            "Epoch [83] Batch[22120] - loss: 0.838703  acc: 78.1250%(25/32)\n",
            "Epoch [83] Batch[22140] - loss: 0.882378  acc: 71.8750%(23/32)\n",
            "Epoch [83] Batch[22160] - loss: 0.922307  acc: 59.3750%(19/32)\n",
            "Epoch [84] Batch[22180] - loss: 0.965290  acc: 59.3750%(19/32)\n",
            "Epoch [84] Batch[22200] - loss: 0.847449  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.252355  acc: 45.1408%(497/1101) \n",
            "\n",
            "Epoch [84] Batch[22220] - loss: 0.825286  acc: 68.7500%(22/32)\n",
            "Epoch [84] Batch[22240] - loss: 0.796557  acc: 78.1250%(25/32)\n",
            "Epoch [84] Batch[22260] - loss: 0.810827  acc: 87.5000%(28/32)\n",
            "Epoch [84] Batch[22280] - loss: 0.973540  acc: 56.2500%(18/32)\n",
            "Epoch [84] Batch[22300] - loss: 0.859990  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.252148  acc: 45.9582%(506/1101) \n",
            "\n",
            "Epoch [84] Batch[22320] - loss: 0.946736  acc: 62.5000%(20/32)\n",
            "Epoch [84] Batch[22340] - loss: 0.849504  acc: 75.0000%(24/32)\n",
            "Epoch [84] Batch[22360] - loss: 0.752206  acc: 81.2500%(26/32)\n",
            "Epoch [84] Batch[22380] - loss: 1.105797  acc: 56.2500%(18/32)\n",
            "Epoch [84] Batch[22400] - loss: 0.899070  acc: 62.5000%(20/32)\n",
            "\n",
            "Evaluation - loss: 1.253429  acc: 45.5041%(501/1101) \n",
            "\n",
            "Epoch [84] Batch[22420] - loss: 0.830281  acc: 75.0000%(24/32)\n",
            "Epoch [85] Batch[22440] - loss: 0.848229  acc: 75.0000%(24/32)\n",
            "Epoch [85] Batch[22460] - loss: 0.942086  acc: 59.3750%(19/32)\n",
            "Epoch [85] Batch[22480] - loss: 0.989449  acc: 75.0000%(24/32)\n",
            "Epoch [85] Batch[22500] - loss: 0.752083  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.254203  acc: 45.5949%(502/1101) \n",
            "\n",
            "Epoch [85] Batch[22520] - loss: 0.765454  acc: 78.1250%(25/32)\n",
            "Epoch [85] Batch[22540] - loss: 0.851887  acc: 75.0000%(24/32)\n",
            "Epoch [85] Batch[22560] - loss: 0.899834  acc: 68.7500%(22/32)\n",
            "Epoch [85] Batch[22580] - loss: 0.852676  acc: 81.2500%(26/32)\n",
            "Epoch [85] Batch[22600] - loss: 1.001724  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.252188  acc: 45.7766%(504/1101) \n",
            "\n",
            "Epoch [85] Batch[22620] - loss: 0.867933  acc: 75.0000%(24/32)\n",
            "Epoch [85] Batch[22640] - loss: 0.813030  acc: 71.8750%(23/32)\n",
            "Epoch [85] Batch[22660] - loss: 0.725365  acc: 78.1250%(25/32)\n",
            "Epoch [85] Batch[22680] - loss: 0.711323  acc: 87.5000%(28/32)\n",
            "Epoch [86] Batch[22700] - loss: 0.843080  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.252423  acc: 45.5949%(502/1101) \n",
            "\n",
            "Epoch [86] Batch[22720] - loss: 0.841435  acc: 71.8750%(23/32)\n",
            "Epoch [86] Batch[22740] - loss: 0.853940  acc: 81.2500%(26/32)\n",
            "Epoch [86] Batch[22760] - loss: 0.782864  acc: 84.3750%(27/32)\n",
            "Epoch [86] Batch[22780] - loss: 0.966743  acc: 56.2500%(18/32)\n",
            "Epoch [86] Batch[22800] - loss: 0.793616  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.251412  acc: 45.9582%(506/1101) \n",
            "\n",
            "Epoch [86] Batch[22820] - loss: 0.764097  acc: 81.2500%(26/32)\n",
            "Epoch [86] Batch[22840] - loss: 0.882405  acc: 78.1250%(25/32)\n",
            "Epoch [86] Batch[22860] - loss: 0.936229  acc: 62.5000%(20/32)\n",
            "Epoch [86] Batch[22880] - loss: 0.796141  acc: 75.0000%(24/32)\n",
            "Epoch [86] Batch[22900] - loss: 0.833738  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.254272  acc: 45.0500%(496/1101) \n",
            "\n",
            "Epoch [86] Batch[22920] - loss: 0.858642  acc: 68.7500%(22/32)\n",
            "Epoch [86] Batch[22940] - loss: 0.891131  acc: 65.6250%(21/32)\n",
            "Epoch [86] Batch[22960] - loss: 0.842694  acc: 75.0000%(24/32)\n",
            "Epoch [87] Batch[22980] - loss: 0.869735  acc: 78.1250%(25/32)\n",
            "Epoch [87] Batch[23000] - loss: 0.845736  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.252438  acc: 45.1408%(497/1101) \n",
            "\n",
            "Epoch [87] Batch[23020] - loss: 1.016485  acc: 71.8750%(23/32)\n",
            "Epoch [87] Batch[23040] - loss: 0.951347  acc: 59.3750%(19/32)\n",
            "Epoch [87] Batch[23060] - loss: 0.758629  acc: 75.0000%(24/32)\n",
            "Epoch [87] Batch[23080] - loss: 0.832919  acc: 68.7500%(22/32)\n",
            "Epoch [87] Batch[23100] - loss: 0.826737  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.252040  acc: 45.4133%(500/1101) \n",
            "\n",
            "Epoch [87] Batch[23120] - loss: 0.708730  acc: 87.5000%(28/32)\n",
            "Epoch [87] Batch[23140] - loss: 0.780776  acc: 84.3750%(27/32)\n",
            "Epoch [87] Batch[23160] - loss: 0.906680  acc: 62.5000%(20/32)\n",
            "Epoch [87] Batch[23180] - loss: 0.885790  acc: 68.7500%(22/32)\n",
            "Epoch [87] Batch[23200] - loss: 0.817879  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.254167  acc: 45.5041%(501/1101) \n",
            "\n",
            "Epoch [87] Batch[23220] - loss: 0.818631  acc: 68.7500%(22/32)\n",
            "Epoch [88] Batch[23240] - loss: 0.866737  acc: 71.8750%(23/32)\n",
            "Epoch [88] Batch[23260] - loss: 0.807034  acc: 75.0000%(24/32)\n",
            "Epoch [88] Batch[23280] - loss: 0.744990  acc: 71.8750%(23/32)\n",
            "Epoch [88] Batch[23300] - loss: 0.738770  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.255880  acc: 45.4133%(500/1101) \n",
            "\n",
            "Epoch [88] Batch[23320] - loss: 0.922876  acc: 68.7500%(22/32)\n",
            "Epoch [88] Batch[23340] - loss: 0.835277  acc: 81.2500%(26/32)\n",
            "Epoch [88] Batch[23360] - loss: 0.870528  acc: 71.8750%(23/32)\n",
            "Epoch [88] Batch[23380] - loss: 0.999116  acc: 71.8750%(23/32)\n",
            "Epoch [88] Batch[23400] - loss: 0.813376  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.254170  acc: 44.6866%(492/1101) \n",
            "\n",
            "Epoch [88] Batch[23420] - loss: 0.755256  acc: 71.8750%(23/32)\n",
            "Epoch [88] Batch[23440] - loss: 0.889536  acc: 75.0000%(24/32)\n",
            "Epoch [88] Batch[23460] - loss: 0.925588  acc: 62.5000%(20/32)\n",
            "Epoch [88] Batch[23480] - loss: 0.940632  acc: 53.1250%(17/32)\n",
            "Epoch [89] Batch[23500] - loss: 0.741726  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.251097  acc: 45.5041%(501/1101) \n",
            "\n",
            "Epoch [89] Batch[23520] - loss: 0.787985  acc: 78.1250%(25/32)\n",
            "Epoch [89] Batch[23540] - loss: 0.871599  acc: 71.8750%(23/32)\n",
            "Epoch [89] Batch[23560] - loss: 0.949941  acc: 68.7500%(22/32)\n",
            "Epoch [89] Batch[23580] - loss: 0.914944  acc: 62.5000%(20/32)\n",
            "Epoch [89] Batch[23600] - loss: 0.980989  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.252422  acc: 45.3224%(499/1101) \n",
            "\n",
            "Epoch [89] Batch[23620] - loss: 0.874967  acc: 68.7500%(22/32)\n",
            "Epoch [89] Batch[23640] - loss: 0.874489  acc: 68.7500%(22/32)\n",
            "Epoch [89] Batch[23660] - loss: 0.721124  acc: 71.8750%(23/32)\n",
            "Epoch [89] Batch[23680] - loss: 0.658671  acc: 81.2500%(26/32)\n",
            "Epoch [89] Batch[23700] - loss: 0.790507  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.253569  acc: 44.9591%(495/1101) \n",
            "\n",
            "Epoch [89] Batch[23720] - loss: 0.862813  acc: 75.0000%(24/32)\n",
            "Epoch [89] Batch[23740] - loss: 0.825617  acc: 75.0000%(24/32)\n",
            "Epoch [89] Batch[23760] - loss: 0.800596  acc: 75.0000%(24/32)\n",
            "Epoch [90] Batch[23780] - loss: 0.764180  acc: 81.2500%(26/32)\n",
            "Epoch [90] Batch[23800] - loss: 0.829251  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.252209  acc: 45.2316%(498/1101) \n",
            "\n",
            "Epoch [90] Batch[23820] - loss: 0.765926  acc: 81.2500%(26/32)\n",
            "Epoch [90] Batch[23840] - loss: 0.687536  acc: 81.2500%(26/32)\n",
            "Epoch [90] Batch[23860] - loss: 0.836810  acc: 75.0000%(24/32)\n",
            "Epoch [90] Batch[23880] - loss: 0.818761  acc: 78.1250%(25/32)\n",
            "Epoch [90] Batch[23900] - loss: 0.701339  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.253710  acc: 44.9591%(495/1101) \n",
            "\n",
            "Epoch [90] Batch[23920] - loss: 0.995629  acc: 65.6250%(21/32)\n",
            "Epoch [90] Batch[23940] - loss: 1.003982  acc: 68.7500%(22/32)\n",
            "Epoch [90] Batch[23960] - loss: 0.899890  acc: 75.0000%(24/32)\n",
            "Epoch [90] Batch[23980] - loss: 0.811307  acc: 78.1250%(25/32)\n",
            "Epoch [90] Batch[24000] - loss: 0.811342  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.253851  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [90] Batch[24020] - loss: 0.859795  acc: 75.0000%(24/32)\n",
            "Epoch [91] Batch[24040] - loss: 0.812996  acc: 75.0000%(24/32)\n",
            "Epoch [91] Batch[24060] - loss: 0.984939  acc: 59.3750%(19/32)\n",
            "Epoch [91] Batch[24080] - loss: 0.868475  acc: 87.5000%(28/32)\n",
            "Epoch [91] Batch[24100] - loss: 0.668622  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.254438  acc: 45.3224%(499/1101) \n",
            "\n",
            "Epoch [91] Batch[24120] - loss: 0.873448  acc: 71.8750%(23/32)\n",
            "Epoch [91] Batch[24140] - loss: 0.869095  acc: 62.5000%(20/32)\n",
            "Epoch [91] Batch[24160] - loss: 0.789303  acc: 75.0000%(24/32)\n",
            "Epoch [91] Batch[24180] - loss: 0.791667  acc: 75.0000%(24/32)\n",
            "Epoch [91] Batch[24200] - loss: 0.967417  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.254137  acc: 44.5050%(490/1101) \n",
            "\n",
            "Epoch [91] Batch[24220] - loss: 0.848614  acc: 71.8750%(23/32)\n",
            "Epoch [91] Batch[24240] - loss: 0.949641  acc: 65.6250%(21/32)\n",
            "Epoch [91] Batch[24260] - loss: 0.807501  acc: 75.0000%(24/32)\n",
            "Epoch [91] Batch[24280] - loss: 0.766239  acc: 62.5000%(20/32)\n",
            "Epoch [92] Batch[24300] - loss: 0.964076  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.252426  acc: 45.3224%(499/1101) \n",
            "\n",
            "Epoch [92] Batch[24320] - loss: 0.815460  acc: 75.0000%(24/32)\n",
            "Epoch [92] Batch[24340] - loss: 0.739018  acc: 71.8750%(23/32)\n",
            "Epoch [92] Batch[24360] - loss: 0.784099  acc: 71.8750%(23/32)\n",
            "Epoch [92] Batch[24380] - loss: 0.677846  acc: 84.3750%(27/32)\n",
            "Epoch [92] Batch[24400] - loss: 0.733357  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.251939  acc: 45.4133%(500/1101) \n",
            "\n",
            "Epoch [92] Batch[24420] - loss: 0.809523  acc: 62.5000%(20/32)\n",
            "Epoch [92] Batch[24440] - loss: 0.722014  acc: 71.8750%(23/32)\n",
            "Epoch [92] Batch[24460] - loss: 0.820967  acc: 71.8750%(23/32)\n",
            "Epoch [92] Batch[24480] - loss: 0.708595  acc: 78.1250%(25/32)\n",
            "Epoch [92] Batch[24500] - loss: 0.757965  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.254611  acc: 44.5050%(490/1101) \n",
            "\n",
            "Epoch [92] Batch[24520] - loss: 1.069692  acc: 68.7500%(22/32)\n",
            "Epoch [92] Batch[24540] - loss: 0.827624  acc: 68.7500%(22/32)\n",
            "Epoch [92] Batch[24560] - loss: 0.991700  acc: 53.1250%(17/32)\n",
            "Epoch [93] Batch[24580] - loss: 0.779197  acc: 68.7500%(22/32)\n",
            "Epoch [93] Batch[24600] - loss: 1.017043  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.257308  acc: 44.3233%(488/1101) \n",
            "\n",
            "Epoch [93] Batch[24620] - loss: 0.800855  acc: 71.8750%(23/32)\n",
            "Epoch [93] Batch[24640] - loss: 0.744156  acc: 75.0000%(24/32)\n",
            "Epoch [93] Batch[24660] - loss: 0.670064  acc: 93.7500%(30/32)\n",
            "Epoch [93] Batch[24680] - loss: 0.866632  acc: 75.0000%(24/32)\n",
            "Epoch [93] Batch[24700] - loss: 0.846505  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.254176  acc: 45.5041%(501/1101) \n",
            "\n",
            "Epoch [93] Batch[24720] - loss: 0.840654  acc: 68.7500%(22/32)\n",
            "Epoch [93] Batch[24740] - loss: 0.758033  acc: 75.0000%(24/32)\n",
            "Epoch [93] Batch[24760] - loss: 0.769957  acc: 78.1250%(25/32)\n",
            "Epoch [93] Batch[24780] - loss: 0.867844  acc: 68.7500%(22/32)\n",
            "Epoch [93] Batch[24800] - loss: 0.793239  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.252834  acc: 44.6866%(492/1101) \n",
            "\n",
            "Epoch [93] Batch[24820] - loss: 0.911576  acc: 68.7500%(22/32)\n",
            "Epoch [94] Batch[24840] - loss: 0.816334  acc: 75.0000%(24/32)\n",
            "Epoch [94] Batch[24860] - loss: 0.630709  acc: 75.0000%(24/32)\n",
            "Epoch [94] Batch[24880] - loss: 0.859345  acc: 71.8750%(23/32)\n",
            "Epoch [94] Batch[24900] - loss: 0.858892  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.252189  acc: 45.8674%(505/1101) \n",
            "\n",
            "Epoch [94] Batch[24920] - loss: 0.743690  acc: 75.0000%(24/32)\n",
            "Epoch [94] Batch[24940] - loss: 0.697860  acc: 84.3750%(27/32)\n",
            "Epoch [94] Batch[24960] - loss: 0.804844  acc: 71.8750%(23/32)\n",
            "Epoch [94] Batch[24980] - loss: 0.727655  acc: 78.1250%(25/32)\n",
            "Epoch [94] Batch[25000] - loss: 0.832855  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.250833  acc: 45.4133%(500/1101) \n",
            "\n",
            "Epoch [94] Batch[25020] - loss: 0.862992  acc: 71.8750%(23/32)\n",
            "Epoch [94] Batch[25040] - loss: 0.897513  acc: 78.1250%(25/32)\n",
            "Epoch [94] Batch[25060] - loss: 0.693624  acc: 78.1250%(25/32)\n",
            "Epoch [94] Batch[25080] - loss: 0.830250  acc: 71.8750%(23/32)\n",
            "Epoch [95] Batch[25100] - loss: 0.723871  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.254727  acc: 44.8683%(494/1101) \n",
            "\n",
            "Epoch [95] Batch[25120] - loss: 0.911220  acc: 75.0000%(24/32)\n",
            "Epoch [95] Batch[25140] - loss: 0.857844  acc: 68.7500%(22/32)\n",
            "Epoch [95] Batch[25160] - loss: 0.908196  acc: 65.6250%(21/32)\n",
            "Epoch [95] Batch[25180] - loss: 1.067878  acc: 56.2500%(18/32)\n",
            "Epoch [95] Batch[25200] - loss: 0.833580  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.252630  acc: 45.1408%(497/1101) \n",
            "\n",
            "Epoch [95] Batch[25220] - loss: 0.795051  acc: 75.0000%(24/32)\n",
            "Epoch [95] Batch[25240] - loss: 0.860748  acc: 62.5000%(20/32)\n",
            "Epoch [95] Batch[25260] - loss: 0.630789  acc: 84.3750%(27/32)\n",
            "Epoch [95] Batch[25280] - loss: 0.786205  acc: 81.2500%(26/32)\n",
            "Epoch [95] Batch[25300] - loss: 0.773987  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.252703  acc: 45.3224%(499/1101) \n",
            "\n",
            "Epoch [95] Batch[25320] - loss: 0.847880  acc: 68.7500%(22/32)\n",
            "Epoch [95] Batch[25340] - loss: 0.745514  acc: 75.0000%(24/32)\n",
            "Epoch [95] Batch[25360] - loss: 0.965260  acc: 71.8750%(23/32)\n",
            "Epoch [96] Batch[25380] - loss: 0.928073  acc: 71.8750%(23/32)\n",
            "Epoch [96] Batch[25400] - loss: 0.814907  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.253825  acc: 45.1408%(497/1101) \n",
            "\n",
            "Epoch [96] Batch[25420] - loss: 0.811683  acc: 62.5000%(20/32)\n",
            "Epoch [96] Batch[25440] - loss: 0.827514  acc: 84.3750%(27/32)\n",
            "Epoch [96] Batch[25460] - loss: 0.816073  acc: 75.0000%(24/32)\n",
            "Epoch [96] Batch[25480] - loss: 0.831436  acc: 81.2500%(26/32)\n",
            "Epoch [96] Batch[25500] - loss: 0.635327  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.251330  acc: 45.0500%(496/1101) \n",
            "\n",
            "Epoch [96] Batch[25520] - loss: 0.783860  acc: 81.2500%(26/32)\n",
            "Epoch [96] Batch[25540] - loss: 0.742413  acc: 87.5000%(28/32)\n",
            "Epoch [96] Batch[25560] - loss: 0.756016  acc: 78.1250%(25/32)\n",
            "Epoch [96] Batch[25580] - loss: 0.718993  acc: 78.1250%(25/32)\n",
            "Epoch [96] Batch[25600] - loss: 0.823922  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.255543  acc: 44.2325%(487/1101) \n",
            "\n",
            "Epoch [96] Batch[25620] - loss: 0.699697  acc: 87.5000%(28/32)\n",
            "Epoch [97] Batch[25640] - loss: 0.855308  acc: 68.7500%(22/32)\n",
            "Epoch [97] Batch[25660] - loss: 0.810269  acc: 78.1250%(25/32)\n",
            "Epoch [97] Batch[25680] - loss: 0.799216  acc: 78.1250%(25/32)\n",
            "Epoch [97] Batch[25700] - loss: 0.695990  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.252785  acc: 45.8674%(505/1101) \n",
            "\n",
            "Epoch [97] Batch[25720] - loss: 0.852097  acc: 65.6250%(21/32)\n",
            "Epoch [97] Batch[25740] - loss: 0.869517  acc: 68.7500%(22/32)\n",
            "Epoch [97] Batch[25760] - loss: 0.845668  acc: 65.6250%(21/32)\n",
            "Epoch [97] Batch[25780] - loss: 0.671583  acc: 87.5000%(28/32)\n",
            "Epoch [97] Batch[25800] - loss: 0.628575  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.253882  acc: 45.3224%(499/1101) \n",
            "\n",
            "Epoch [97] Batch[25820] - loss: 0.924451  acc: 68.7500%(22/32)\n",
            "Epoch [97] Batch[25840] - loss: 0.730682  acc: 78.1250%(25/32)\n",
            "Epoch [97] Batch[25860] - loss: 0.625892  acc: 90.6250%(29/32)\n",
            "Epoch [97] Batch[25880] - loss: 0.873955  acc: 81.2500%(26/32)\n",
            "Epoch [98] Batch[25900] - loss: 0.683981  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.253618  acc: 44.5958%(491/1101) \n",
            "\n",
            "Epoch [98] Batch[25920] - loss: 0.755807  acc: 75.0000%(24/32)\n",
            "Epoch [98] Batch[25940] - loss: 0.790278  acc: 81.2500%(26/32)\n",
            "Epoch [98] Batch[25960] - loss: 0.821939  acc: 71.8750%(23/32)\n",
            "Epoch [98] Batch[25980] - loss: 0.867494  acc: 62.5000%(20/32)\n",
            "Epoch [98] Batch[26000] - loss: 0.689835  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.255402  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [98] Batch[26020] - loss: 0.813299  acc: 75.0000%(24/32)\n",
            "Epoch [98] Batch[26040] - loss: 0.676392  acc: 78.1250%(25/32)\n",
            "Epoch [98] Batch[26060] - loss: 0.772083  acc: 81.2500%(26/32)\n",
            "Epoch [98] Batch[26080] - loss: 0.916418  acc: 59.3750%(19/32)\n",
            "Epoch [98] Batch[26100] - loss: 0.703109  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.254189  acc: 45.2316%(498/1101) \n",
            "\n",
            "Epoch [98] Batch[26120] - loss: 0.824149  acc: 81.2500%(26/32)\n",
            "Epoch [98] Batch[26140] - loss: 0.720770  acc: 75.0000%(24/32)\n",
            "Epoch [98] Batch[26160] - loss: 0.920765  acc: 68.7500%(22/32)\n",
            "Epoch [99] Batch[26180] - loss: 0.702179  acc: 75.0000%(24/32)\n",
            "Epoch [99] Batch[26200] - loss: 0.682342  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.252077  acc: 44.9591%(495/1101) \n",
            "\n",
            "Epoch [99] Batch[26220] - loss: 0.779041  acc: 81.2500%(26/32)\n",
            "Epoch [99] Batch[26240] - loss: 0.794790  acc: 78.1250%(25/32)\n",
            "Epoch [99] Batch[26260] - loss: 0.705031  acc: 84.3750%(27/32)\n",
            "Epoch [99] Batch[26280] - loss: 0.590658  acc: 90.6250%(29/32)\n",
            "Epoch [99] Batch[26300] - loss: 0.690663  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.255139  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [99] Batch[26320] - loss: 0.745355  acc: 75.0000%(24/32)\n",
            "Epoch [99] Batch[26340] - loss: 0.697801  acc: 84.3750%(27/32)\n",
            "Epoch [99] Batch[26360] - loss: 0.761014  acc: 78.1250%(25/32)\n",
            "Epoch [99] Batch[26380] - loss: 0.800144  acc: 75.0000%(24/32)\n",
            "Epoch [99] Batch[26400] - loss: 0.843913  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.252785  acc: 44.8683%(494/1101) \n",
            "\n",
            "Epoch [99] Batch[26420] - loss: 0.722884  acc: 81.2500%(26/32)\n",
            "Epoch [100] Batch[26440] - loss: 0.798913  acc: 84.3750%(27/32)\n",
            "Epoch [100] Batch[26460] - loss: 0.766186  acc: 75.0000%(24/32)\n",
            "Epoch [100] Batch[26480] - loss: 0.738072  acc: 78.1250%(25/32)\n",
            "Epoch [100] Batch[26500] - loss: 0.917115  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.252925  acc: 45.2316%(498/1101) \n",
            "\n",
            "Epoch [100] Batch[26520] - loss: 0.693843  acc: 78.1250%(25/32)\n",
            "Epoch [100] Batch[26540] - loss: 0.759008  acc: 75.0000%(24/32)\n",
            "Epoch [100] Batch[26560] - loss: 0.725042  acc: 81.2500%(26/32)\n",
            "Epoch [100] Batch[26580] - loss: 0.719631  acc: 75.0000%(24/32)\n",
            "Epoch [100] Batch[26600] - loss: 0.630154  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.253826  acc: 45.2316%(498/1101) \n",
            "\n",
            "Epoch [100] Batch[26620] - loss: 0.735683  acc: 78.1250%(25/32)\n",
            "Epoch [100] Batch[26640] - loss: 0.736284  acc: 75.0000%(24/32)\n",
            "Epoch [100] Batch[26660] - loss: 0.748640  acc: 71.8750%(23/32)\n",
            "Epoch [100] Batch[26680] - loss: 0.805638  acc: 84.3750%(27/32)\n",
            "Epoch [100] Batch[26700] - loss: 0.719143  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.254173  acc: 44.4142%(489/1101) \n",
            "\n",
            "Epoch [101] Batch[26720] - loss: 0.792843  acc: 87.5000%(28/32)\n",
            "Epoch [101] Batch[26740] - loss: 0.769514  acc: 78.1250%(25/32)\n",
            "Epoch [101] Batch[26760] - loss: 0.691687  acc: 78.1250%(25/32)\n",
            "Epoch [101] Batch[26780] - loss: 0.722596  acc: 78.1250%(25/32)\n",
            "Epoch [101] Batch[26800] - loss: 0.805878  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.255244  acc: 45.1408%(497/1101) \n",
            "\n",
            "Epoch [101] Batch[26820] - loss: 0.810774  acc: 71.8750%(23/32)\n",
            "Epoch [101] Batch[26840] - loss: 0.769589  acc: 75.0000%(24/32)\n",
            "Epoch [101] Batch[26860] - loss: 0.591645  acc: 87.5000%(28/32)\n",
            "Epoch [101] Batch[26880] - loss: 1.023096  acc: 62.5000%(20/32)\n",
            "Epoch [101] Batch[26900] - loss: 0.769611  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.251962  acc: 44.8683%(494/1101) \n",
            "\n",
            "Epoch [101] Batch[26920] - loss: 0.664433  acc: 75.0000%(24/32)\n",
            "Epoch [101] Batch[26940] - loss: 0.696268  acc: 71.8750%(23/32)\n",
            "Epoch [101] Batch[26960] - loss: 0.746534  acc: 75.0000%(24/32)\n",
            "Epoch [102] Batch[26980] - loss: 0.519948  acc: 90.6250%(29/32)\n",
            "Epoch [102] Batch[27000] - loss: 0.713293  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.256633  acc: 44.5050%(490/1101) \n",
            "\n",
            "Epoch [102] Batch[27020] - loss: 0.834431  acc: 68.7500%(22/32)\n",
            "Epoch [102] Batch[27040] - loss: 0.699996  acc: 81.2500%(26/32)\n",
            "Epoch [102] Batch[27060] - loss: 0.888594  acc: 81.2500%(26/32)\n",
            "Epoch [102] Batch[27080] - loss: 0.684179  acc: 78.1250%(25/32)\n",
            "Epoch [102] Batch[27100] - loss: 0.806848  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.252808  acc: 45.1408%(497/1101) \n",
            "\n",
            "Epoch [102] Batch[27120] - loss: 0.803260  acc: 81.2500%(26/32)\n",
            "Epoch [102] Batch[27140] - loss: 0.783464  acc: 75.0000%(24/32)\n",
            "Epoch [102] Batch[27160] - loss: 0.809492  acc: 78.1250%(25/32)\n",
            "Epoch [102] Batch[27180] - loss: 0.995283  acc: 59.3750%(19/32)\n",
            "Epoch [102] Batch[27200] - loss: 0.662943  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.252626  acc: 44.9591%(495/1101) \n",
            "\n",
            "Epoch [102] Batch[27220] - loss: 0.724866  acc: 75.0000%(24/32)\n",
            "Epoch [103] Batch[27240] - loss: 0.925748  acc: 75.0000%(24/32)\n",
            "Epoch [103] Batch[27260] - loss: 0.650683  acc: 78.1250%(25/32)\n",
            "Epoch [103] Batch[27280] - loss: 0.764831  acc: 75.0000%(24/32)\n",
            "Epoch [103] Batch[27300] - loss: 0.782767  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.252821  acc: 45.0500%(496/1101) \n",
            "\n",
            "Epoch [103] Batch[27320] - loss: 0.628836  acc: 90.6250%(29/32)\n",
            "Epoch [103] Batch[27340] - loss: 0.623068  acc: 90.6250%(29/32)\n",
            "Epoch [103] Batch[27360] - loss: 0.650249  acc: 84.3750%(27/32)\n",
            "Epoch [103] Batch[27380] - loss: 0.793371  acc: 75.0000%(24/32)\n",
            "Epoch [103] Batch[27400] - loss: 0.864126  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.253898  acc: 44.8683%(494/1101) \n",
            "\n",
            "Epoch [103] Batch[27420] - loss: 0.852531  acc: 71.8750%(23/32)\n",
            "Epoch [103] Batch[27440] - loss: 0.766547  acc: 78.1250%(25/32)\n",
            "Epoch [103] Batch[27460] - loss: 0.684113  acc: 81.2500%(26/32)\n",
            "Epoch [103] Batch[27480] - loss: 0.667997  acc: 81.2500%(26/32)\n",
            "Epoch [103] Batch[27500] - loss: 0.880309  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.255371  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [104] Batch[27520] - loss: 0.846838  acc: 65.6250%(21/32)\n",
            "Epoch [104] Batch[27540] - loss: 0.943870  acc: 62.5000%(20/32)\n",
            "Epoch [104] Batch[27560] - loss: 0.793154  acc: 78.1250%(25/32)\n",
            "Epoch [104] Batch[27580] - loss: 0.711896  acc: 78.1250%(25/32)\n",
            "Epoch [104] Batch[27600] - loss: 0.776541  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.255217  acc: 44.9591%(495/1101) \n",
            "\n",
            "Epoch [104] Batch[27620] - loss: 0.941114  acc: 50.0000%(16/32)\n",
            "Epoch [104] Batch[27640] - loss: 0.785445  acc: 87.5000%(28/32)\n",
            "Epoch [104] Batch[27660] - loss: 0.814269  acc: 68.7500%(22/32)\n",
            "Epoch [104] Batch[27680] - loss: 0.719271  acc: 75.0000%(24/32)\n",
            "Epoch [104] Batch[27700] - loss: 0.752159  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.256467  acc: 44.6866%(492/1101) \n",
            "\n",
            "Epoch [104] Batch[27720] - loss: 0.782985  acc: 75.0000%(24/32)\n",
            "Epoch [104] Batch[27740] - loss: 0.816728  acc: 65.6250%(21/32)\n",
            "Epoch [104] Batch[27760] - loss: 0.684277  acc: 87.5000%(28/32)\n",
            "Epoch [105] Batch[27780] - loss: 0.674615  acc: 81.2500%(26/32)\n",
            "Epoch [105] Batch[27800] - loss: 0.778611  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.253151  acc: 45.1408%(497/1101) \n",
            "\n",
            "Epoch [105] Batch[27820] - loss: 0.732584  acc: 78.1250%(25/32)\n",
            "Epoch [105] Batch[27840] - loss: 0.867944  acc: 59.3750%(19/32)\n",
            "Epoch [105] Batch[27860] - loss: 0.734614  acc: 81.2500%(26/32)\n",
            "Epoch [105] Batch[27880] - loss: 0.752253  acc: 78.1250%(25/32)\n",
            "Epoch [105] Batch[27900] - loss: 0.708384  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.254873  acc: 44.5958%(491/1101) \n",
            "\n",
            "Epoch [105] Batch[27920] - loss: 0.768993  acc: 81.2500%(26/32)\n",
            "Epoch [105] Batch[27940] - loss: 0.696873  acc: 78.1250%(25/32)\n",
            "Epoch [105] Batch[27960] - loss: 0.755949  acc: 75.0000%(24/32)\n",
            "Epoch [105] Batch[27980] - loss: 0.849021  acc: 65.6250%(21/32)\n",
            "Epoch [105] Batch[28000] - loss: 0.736611  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.254739  acc: 44.5050%(490/1101) \n",
            "\n",
            "Epoch [105] Batch[28020] - loss: 0.931184  acc: 62.5000%(20/32)\n",
            "Epoch [106] Batch[28040] - loss: 0.752454  acc: 62.5000%(20/32)\n",
            "Epoch [106] Batch[28060] - loss: 0.629564  acc: 87.5000%(28/32)\n",
            "Epoch [106] Batch[28080] - loss: 0.699683  acc: 81.2500%(26/32)\n",
            "Epoch [106] Batch[28100] - loss: 0.933992  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.253234  acc: 45.0500%(496/1101) \n",
            "\n",
            "Epoch [106] Batch[28120] - loss: 0.750657  acc: 71.8750%(23/32)\n",
            "Epoch [106] Batch[28140] - loss: 0.757441  acc: 81.2500%(26/32)\n",
            "Epoch [106] Batch[28160] - loss: 0.695658  acc: 78.1250%(25/32)\n",
            "Epoch [106] Batch[28180] - loss: 0.730056  acc: 71.8750%(23/32)\n",
            "Epoch [106] Batch[28200] - loss: 0.687981  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.256150  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [106] Batch[28220] - loss: 0.798502  acc: 81.2500%(26/32)\n",
            "Epoch [106] Batch[28240] - loss: 0.743404  acc: 71.8750%(23/32)\n",
            "Epoch [106] Batch[28260] - loss: 0.648695  acc: 81.2500%(26/32)\n",
            "Epoch [106] Batch[28280] - loss: 0.780292  acc: 78.1250%(25/32)\n",
            "Epoch [106] Batch[28300] - loss: 0.716039  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.255131  acc: 44.6866%(492/1101) \n",
            "\n",
            "Epoch [107] Batch[28320] - loss: 0.625584  acc: 84.3750%(27/32)\n",
            "Epoch [107] Batch[28340] - loss: 0.682108  acc: 90.6250%(29/32)\n",
            "Epoch [107] Batch[28360] - loss: 0.773055  acc: 68.7500%(22/32)\n",
            "Epoch [107] Batch[28380] - loss: 0.741290  acc: 75.0000%(24/32)\n",
            "Epoch [107] Batch[28400] - loss: 0.631802  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.254671  acc: 44.5958%(491/1101) \n",
            "\n",
            "Epoch [107] Batch[28420] - loss: 0.647019  acc: 81.2500%(26/32)\n",
            "Epoch [107] Batch[28440] - loss: 0.714891  acc: 84.3750%(27/32)\n",
            "Epoch [107] Batch[28460] - loss: 0.683662  acc: 78.1250%(25/32)\n",
            "Epoch [107] Batch[28480] - loss: 0.845017  acc: 71.8750%(23/32)\n",
            "Epoch [107] Batch[28500] - loss: 0.731333  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.255733  acc: 44.1417%(486/1101) \n",
            "\n",
            "Epoch [107] Batch[28520] - loss: 0.657992  acc: 93.7500%(30/32)\n",
            "Epoch [107] Batch[28540] - loss: 0.597379  acc: 84.3750%(27/32)\n",
            "Epoch [107] Batch[28560] - loss: 0.705942  acc: 84.3750%(27/32)\n",
            "Epoch [108] Batch[28580] - loss: 0.774176  acc: 78.1250%(25/32)\n",
            "Epoch [108] Batch[28600] - loss: 0.698616  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.255278  acc: 44.5958%(491/1101) \n",
            "\n",
            "Epoch [108] Batch[28620] - loss: 0.764159  acc: 78.1250%(25/32)\n",
            "Epoch [108] Batch[28640] - loss: 1.025382  acc: 56.2500%(18/32)\n",
            "Epoch [108] Batch[28660] - loss: 0.809928  acc: 75.0000%(24/32)\n",
            "Epoch [108] Batch[28680] - loss: 0.682958  acc: 87.5000%(28/32)\n",
            "Epoch [108] Batch[28700] - loss: 0.815532  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.256628  acc: 44.5958%(491/1101) \n",
            "\n",
            "Epoch [108] Batch[28720] - loss: 0.548450  acc: 93.7500%(30/32)\n",
            "Epoch [108] Batch[28740] - loss: 0.855653  acc: 65.6250%(21/32)\n",
            "Epoch [108] Batch[28760] - loss: 0.519217  acc: 90.6250%(29/32)\n",
            "Epoch [108] Batch[28780] - loss: 0.913879  acc: 62.5000%(20/32)\n",
            "Epoch [108] Batch[28800] - loss: 0.630759  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.254631  acc: 44.5050%(490/1101) \n",
            "\n",
            "Epoch [108] Batch[28820] - loss: 0.777282  acc: 71.8750%(23/32)\n",
            "Epoch [109] Batch[28840] - loss: 0.719634  acc: 71.8750%(23/32)\n",
            "Epoch [109] Batch[28860] - loss: 0.656250  acc: 78.1250%(25/32)\n",
            "Epoch [109] Batch[28880] - loss: 0.745595  acc: 87.5000%(28/32)\n",
            "Epoch [109] Batch[28900] - loss: 0.597534  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.255915  acc: 44.6866%(492/1101) \n",
            "\n",
            "Epoch [109] Batch[28920] - loss: 0.606375  acc: 90.6250%(29/32)\n",
            "Epoch [109] Batch[28940] - loss: 0.860286  acc: 59.3750%(19/32)\n",
            "Epoch [109] Batch[28960] - loss: 0.663619  acc: 78.1250%(25/32)\n",
            "Epoch [109] Batch[28980] - loss: 0.667798  acc: 68.7500%(22/32)\n",
            "Epoch [109] Batch[29000] - loss: 0.654572  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.254215  acc: 44.4142%(489/1101) \n",
            "\n",
            "Epoch [109] Batch[29020] - loss: 0.722021  acc: 75.0000%(24/32)\n",
            "Epoch [109] Batch[29040] - loss: 0.717314  acc: 78.1250%(25/32)\n",
            "Epoch [109] Batch[29060] - loss: 0.484939  acc: 93.7500%(30/32)\n",
            "Epoch [109] Batch[29080] - loss: 0.557198  acc: 84.3750%(27/32)\n",
            "Epoch [109] Batch[29100] - loss: 0.724621  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.254950  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [110] Batch[29120] - loss: 0.745999  acc: 81.2500%(26/32)\n",
            "Epoch [110] Batch[29140] - loss: 0.842806  acc: 71.8750%(23/32)\n",
            "Epoch [110] Batch[29160] - loss: 0.733952  acc: 78.1250%(25/32)\n",
            "Epoch [110] Batch[29180] - loss: 0.693987  acc: 84.3750%(27/32)\n",
            "Epoch [110] Batch[29200] - loss: 0.719424  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.257512  acc: 44.8683%(494/1101) \n",
            "\n",
            "Epoch [110] Batch[29220] - loss: 0.692716  acc: 78.1250%(25/32)\n",
            "Epoch [110] Batch[29240] - loss: 0.830193  acc: 68.7500%(22/32)\n",
            "Epoch [110] Batch[29260] - loss: 0.747286  acc: 84.3750%(27/32)\n",
            "Epoch [110] Batch[29280] - loss: 0.605830  acc: 81.2500%(26/32)\n",
            "Epoch [110] Batch[29300] - loss: 0.588174  acc: 90.6250%(29/32)\n",
            "\n",
            "Evaluation - loss: 1.254312  acc: 45.1408%(497/1101) \n",
            "\n",
            "Epoch [110] Batch[29320] - loss: 0.630799  acc: 84.3750%(27/32)\n",
            "Epoch [110] Batch[29340] - loss: 0.698477  acc: 81.2500%(26/32)\n",
            "Epoch [110] Batch[29360] - loss: 0.731055  acc: 71.8750%(23/32)\n",
            "Epoch [111] Batch[29380] - loss: 0.592488  acc: 87.5000%(28/32)\n",
            "Epoch [111] Batch[29400] - loss: 0.766949  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.254945  acc: 44.4142%(489/1101) \n",
            "\n",
            "Epoch [111] Batch[29420] - loss: 0.631956  acc: 87.5000%(28/32)\n",
            "Epoch [111] Batch[29440] - loss: 0.569993  acc: 81.2500%(26/32)\n",
            "Epoch [111] Batch[29460] - loss: 0.718845  acc: 90.6250%(29/32)\n",
            "Epoch [111] Batch[29480] - loss: 0.744418  acc: 78.1250%(25/32)\n",
            "Epoch [111] Batch[29500] - loss: 0.690060  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.257644  acc: 44.8683%(494/1101) \n",
            "\n",
            "Epoch [111] Batch[29520] - loss: 0.713935  acc: 84.3750%(27/32)\n",
            "Epoch [111] Batch[29540] - loss: 0.591269  acc: 84.3750%(27/32)\n",
            "Epoch [111] Batch[29560] - loss: 0.726513  acc: 84.3750%(27/32)\n",
            "Epoch [111] Batch[29580] - loss: 0.740875  acc: 78.1250%(25/32)\n",
            "Epoch [111] Batch[29600] - loss: 0.740415  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.258124  acc: 44.2325%(487/1101) \n",
            "\n",
            "Epoch [111] Batch[29620] - loss: 0.825147  acc: 71.8750%(23/32)\n",
            "Epoch [112] Batch[29640] - loss: 0.659830  acc: 78.1250%(25/32)\n",
            "Epoch [112] Batch[29660] - loss: 0.614871  acc: 81.2500%(26/32)\n",
            "Epoch [112] Batch[29680] - loss: 0.693456  acc: 75.0000%(24/32)\n",
            "Epoch [112] Batch[29700] - loss: 0.588722  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.259505  acc: 44.8683%(494/1101) \n",
            "\n",
            "Epoch [112] Batch[29720] - loss: 0.837010  acc: 68.7500%(22/32)\n",
            "Epoch [112] Batch[29740] - loss: 0.739140  acc: 78.1250%(25/32)\n",
            "Epoch [112] Batch[29760] - loss: 0.749204  acc: 84.3750%(27/32)\n",
            "Epoch [112] Batch[29780] - loss: 0.693321  acc: 87.5000%(28/32)\n",
            "Epoch [112] Batch[29800] - loss: 0.726920  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.258800  acc: 44.6866%(492/1101) \n",
            "\n",
            "Epoch [112] Batch[29820] - loss: 0.768318  acc: 75.0000%(24/32)\n",
            "Epoch [112] Batch[29840] - loss: 0.521456  acc: 87.5000%(28/32)\n",
            "Epoch [112] Batch[29860] - loss: 0.640086  acc: 84.3750%(27/32)\n",
            "Epoch [112] Batch[29880] - loss: 0.680760  acc: 81.2500%(26/32)\n",
            "Epoch [112] Batch[29900] - loss: 0.617798  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.254475  acc: 44.2325%(487/1101) \n",
            "\n",
            "Epoch [113] Batch[29920] - loss: 0.777536  acc: 68.7500%(22/32)\n",
            "Epoch [113] Batch[29940] - loss: 0.644838  acc: 81.2500%(26/32)\n",
            "Epoch [113] Batch[29960] - loss: 0.724255  acc: 78.1250%(25/32)\n",
            "Epoch [113] Batch[29980] - loss: 0.780690  acc: 71.8750%(23/32)\n",
            "Epoch [113] Batch[30000] - loss: 0.775902  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.256742  acc: 44.1417%(486/1101) \n",
            "\n",
            "Epoch [113] Batch[30020] - loss: 0.801743  acc: 87.5000%(28/32)\n",
            "Epoch [113] Batch[30040] - loss: 0.633169  acc: 78.1250%(25/32)\n",
            "Epoch [113] Batch[30060] - loss: 0.622467  acc: 81.2500%(26/32)\n",
            "Epoch [113] Batch[30080] - loss: 0.665118  acc: 81.2500%(26/32)\n",
            "Epoch [113] Batch[30100] - loss: 0.590143  acc: 93.7500%(30/32)\n",
            "\n",
            "Evaluation - loss: 1.257265  acc: 44.9591%(495/1101) \n",
            "\n",
            "Epoch [113] Batch[30120] - loss: 0.784906  acc: 78.1250%(25/32)\n",
            "Epoch [113] Batch[30140] - loss: 0.677904  acc: 75.0000%(24/32)\n",
            "Epoch [113] Batch[30160] - loss: 0.661510  acc: 87.5000%(28/32)\n",
            "Epoch [114] Batch[30180] - loss: 0.758283  acc: 84.3750%(27/32)\n",
            "Epoch [114] Batch[30200] - loss: 0.644582  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.256437  acc: 44.3233%(488/1101) \n",
            "\n",
            "Epoch [114] Batch[30220] - loss: 0.712843  acc: 78.1250%(25/32)\n",
            "Epoch [114] Batch[30240] - loss: 0.742759  acc: 71.8750%(23/32)\n",
            "Epoch [114] Batch[30260] - loss: 0.672180  acc: 84.3750%(27/32)\n",
            "Epoch [114] Batch[30280] - loss: 0.642286  acc: 84.3750%(27/32)\n",
            "Epoch [114] Batch[30300] - loss: 0.706304  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.256783  acc: 44.3233%(488/1101) \n",
            "\n",
            "Epoch [114] Batch[30320] - loss: 0.657287  acc: 84.3750%(27/32)\n",
            "Epoch [114] Batch[30340] - loss: 0.671203  acc: 78.1250%(25/32)\n",
            "Epoch [114] Batch[30360] - loss: 0.693672  acc: 68.7500%(22/32)\n",
            "Epoch [114] Batch[30380] - loss: 0.681042  acc: 81.2500%(26/32)\n",
            "Epoch [114] Batch[30400] - loss: 0.760440  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.256322  acc: 44.5050%(490/1101) \n",
            "\n",
            "Epoch [114] Batch[30420] - loss: 0.664405  acc: 84.3750%(27/32)\n",
            "Epoch [115] Batch[30440] - loss: 0.802891  acc: 75.0000%(24/32)\n",
            "Epoch [115] Batch[30460] - loss: 0.766620  acc: 71.8750%(23/32)\n",
            "Epoch [115] Batch[30480] - loss: 0.671024  acc: 81.2500%(26/32)\n",
            "Epoch [115] Batch[30500] - loss: 0.728502  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.259354  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [115] Batch[30520] - loss: 0.725423  acc: 81.2500%(26/32)\n",
            "Epoch [115] Batch[30540] - loss: 0.505404  acc: 93.7500%(30/32)\n",
            "Epoch [115] Batch[30560] - loss: 0.742756  acc: 84.3750%(27/32)\n",
            "Epoch [115] Batch[30580] - loss: 0.763936  acc: 81.2500%(26/32)\n",
            "Epoch [115] Batch[30600] - loss: 0.950953  acc: 59.3750%(19/32)\n",
            "\n",
            "Evaluation - loss: 1.258814  acc: 44.9591%(495/1101) \n",
            "\n",
            "Epoch [115] Batch[30620] - loss: 0.665481  acc: 81.2500%(26/32)\n",
            "Epoch [115] Batch[30640] - loss: 0.736710  acc: 65.6250%(21/32)\n",
            "Epoch [115] Batch[30660] - loss: 0.584125  acc: 78.1250%(25/32)\n",
            "Epoch [115] Batch[30680] - loss: 0.664881  acc: 90.6250%(29/32)\n",
            "Epoch [115] Batch[30700] - loss: 0.633163  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.256772  acc: 44.5958%(491/1101) \n",
            "\n",
            "Epoch [116] Batch[30720] - loss: 0.655426  acc: 78.1250%(25/32)\n",
            "Epoch [116] Batch[30740] - loss: 0.794277  acc: 65.6250%(21/32)\n",
            "Epoch [116] Batch[30760] - loss: 0.624146  acc: 81.2500%(26/32)\n",
            "Epoch [116] Batch[30780] - loss: 0.839679  acc: 68.7500%(22/32)\n",
            "Epoch [116] Batch[30800] - loss: 0.707695  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.257442  acc: 44.6866%(492/1101) \n",
            "\n",
            "Epoch [116] Batch[30820] - loss: 0.695678  acc: 78.1250%(25/32)\n",
            "Epoch [116] Batch[30840] - loss: 0.746034  acc: 78.1250%(25/32)\n",
            "Epoch [116] Batch[30860] - loss: 0.654976  acc: 81.2500%(26/32)\n",
            "Epoch [116] Batch[30880] - loss: 0.599480  acc: 90.6250%(29/32)\n",
            "Epoch [116] Batch[30900] - loss: 0.732733  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.259169  acc: 44.5958%(491/1101) \n",
            "\n",
            "Epoch [116] Batch[30920] - loss: 0.625596  acc: 87.5000%(28/32)\n",
            "Epoch [116] Batch[30940] - loss: 0.679054  acc: 84.3750%(27/32)\n",
            "Epoch [116] Batch[30960] - loss: 0.687492  acc: 84.3750%(27/32)\n",
            "Epoch [117] Batch[30980] - loss: 0.564080  acc: 81.2500%(26/32)\n",
            "Epoch [117] Batch[31000] - loss: 0.552958  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.257479  acc: 44.4142%(489/1101) \n",
            "\n",
            "Epoch [117] Batch[31020] - loss: 0.651715  acc: 81.2500%(26/32)\n",
            "Epoch [117] Batch[31040] - loss: 0.663791  acc: 90.6250%(29/32)\n",
            "Epoch [117] Batch[31060] - loss: 0.688831  acc: 81.2500%(26/32)\n",
            "Epoch [117] Batch[31080] - loss: 0.626182  acc: 81.2500%(26/32)\n",
            "Epoch [117] Batch[31100] - loss: 0.651252  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.256350  acc: 44.2325%(487/1101) \n",
            "\n",
            "Epoch [117] Batch[31120] - loss: 0.632523  acc: 78.1250%(25/32)\n",
            "Epoch [117] Batch[31140] - loss: 0.685956  acc: 75.0000%(24/32)\n",
            "Epoch [117] Batch[31160] - loss: 0.822307  acc: 68.7500%(22/32)\n",
            "Epoch [117] Batch[31180] - loss: 0.705259  acc: 78.1250%(25/32)\n",
            "Epoch [117] Batch[31200] - loss: 0.690570  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.259293  acc: 44.8683%(494/1101) \n",
            "\n",
            "Epoch [117] Batch[31220] - loss: 0.715397  acc: 81.2500%(26/32)\n",
            "Epoch [118] Batch[31240] - loss: 0.632255  acc: 78.1250%(25/32)\n",
            "Epoch [118] Batch[31260] - loss: 0.624061  acc: 90.6250%(29/32)\n",
            "Epoch [118] Batch[31280] - loss: 0.714590  acc: 71.8750%(23/32)\n",
            "Epoch [118] Batch[31300] - loss: 0.553241  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.260145  acc: 44.5050%(490/1101) \n",
            "\n",
            "Epoch [118] Batch[31320] - loss: 0.485236  acc: 93.7500%(30/32)\n",
            "Epoch [118] Batch[31340] - loss: 0.696428  acc: 75.0000%(24/32)\n",
            "Epoch [118] Batch[31360] - loss: 0.660123  acc: 75.0000%(24/32)\n",
            "Epoch [118] Batch[31380] - loss: 0.505635  acc: 87.5000%(28/32)\n",
            "Epoch [118] Batch[31400] - loss: 0.866023  acc: 65.6250%(21/32)\n",
            "\n",
            "Evaluation - loss: 1.258755  acc: 44.6866%(492/1101) \n",
            "\n",
            "Epoch [118] Batch[31420] - loss: 0.777545  acc: 62.5000%(20/32)\n",
            "Epoch [118] Batch[31440] - loss: 0.809141  acc: 78.1250%(25/32)\n",
            "Epoch [118] Batch[31460] - loss: 0.707582  acc: 75.0000%(24/32)\n",
            "Epoch [118] Batch[31480] - loss: 0.653921  acc: 78.1250%(25/32)\n",
            "Epoch [118] Batch[31500] - loss: 0.649731  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.259100  acc: 44.2325%(487/1101) \n",
            "\n",
            "Epoch [119] Batch[31520] - loss: 0.669121  acc: 78.1250%(25/32)\n",
            "Epoch [119] Batch[31540] - loss: 0.792196  acc: 68.7500%(22/32)\n",
            "Epoch [119] Batch[31560] - loss: 0.504697  acc: 93.7500%(30/32)\n",
            "Epoch [119] Batch[31580] - loss: 0.666966  acc: 81.2500%(26/32)\n",
            "Epoch [119] Batch[31600] - loss: 0.637839  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.259236  acc: 45.0500%(496/1101) \n",
            "\n",
            "Epoch [119] Batch[31620] - loss: 0.790518  acc: 81.2500%(26/32)\n",
            "Epoch [119] Batch[31640] - loss: 0.709612  acc: 71.8750%(23/32)\n",
            "Epoch [119] Batch[31660] - loss: 0.747589  acc: 81.2500%(26/32)\n",
            "Epoch [119] Batch[31680] - loss: 0.784624  acc: 68.7500%(22/32)\n",
            "Epoch [119] Batch[31700] - loss: 0.782773  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.261891  acc: 44.5958%(491/1101) \n",
            "\n",
            "Epoch [119] Batch[31720] - loss: 0.762626  acc: 68.7500%(22/32)\n",
            "Epoch [119] Batch[31740] - loss: 0.955562  acc: 71.8750%(23/32)\n",
            "Epoch [119] Batch[31760] - loss: 0.666816  acc: 75.0000%(24/32)\n",
            "Epoch [120] Batch[31780] - loss: 0.674889  acc: 71.8750%(23/32)\n",
            "Epoch [120] Batch[31800] - loss: 0.710052  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.258405  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [120] Batch[31820] - loss: 0.590761  acc: 84.3750%(27/32)\n",
            "Epoch [120] Batch[31840] - loss: 0.768983  acc: 71.8750%(23/32)\n",
            "Epoch [120] Batch[31860] - loss: 0.606237  acc: 87.5000%(28/32)\n",
            "Epoch [120] Batch[31880] - loss: 0.646421  acc: 81.2500%(26/32)\n",
            "Epoch [120] Batch[31900] - loss: 0.796804  acc: 68.7500%(22/32)\n",
            "\n",
            "Evaluation - loss: 1.261963  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [120] Batch[31920] - loss: 0.758861  acc: 84.3750%(27/32)\n",
            "Epoch [120] Batch[31940] - loss: 0.611649  acc: 84.3750%(27/32)\n",
            "Epoch [120] Batch[31960] - loss: 0.571916  acc: 84.3750%(27/32)\n",
            "Epoch [120] Batch[31980] - loss: 0.606503  acc: 78.1250%(25/32)\n",
            "Epoch [120] Batch[32000] - loss: 0.664543  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.258463  acc: 44.4142%(489/1101) \n",
            "\n",
            "Epoch [120] Batch[32020] - loss: 0.513116  acc: 93.7500%(30/32)\n",
            "Epoch [120] Batch[32040] - loss: 0.510086  acc: 90.6250%(29/32)\n",
            "Epoch [121] Batch[32060] - loss: 0.620174  acc: 84.3750%(27/32)\n",
            "Epoch [121] Batch[32080] - loss: 0.672195  acc: 87.5000%(28/32)\n",
            "Epoch [121] Batch[32100] - loss: 0.603684  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.259226  acc: 44.6866%(492/1101) \n",
            "\n",
            "Epoch [121] Batch[32120] - loss: 0.738470  acc: 84.3750%(27/32)\n",
            "Epoch [121] Batch[32140] - loss: 0.611647  acc: 75.0000%(24/32)\n",
            "Epoch [121] Batch[32160] - loss: 0.593746  acc: 87.5000%(28/32)\n",
            "Epoch [121] Batch[32180] - loss: 0.669156  acc: 75.0000%(24/32)\n",
            "Epoch [121] Batch[32200] - loss: 0.646137  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.259420  acc: 44.5050%(490/1101) \n",
            "\n",
            "Epoch [121] Batch[32220] - loss: 0.580267  acc: 84.3750%(27/32)\n",
            "Epoch [121] Batch[32240] - loss: 0.583427  acc: 81.2500%(26/32)\n",
            "Epoch [121] Batch[32260] - loss: 0.715221  acc: 75.0000%(24/32)\n",
            "Epoch [121] Batch[32280] - loss: 0.709180  acc: 71.8750%(23/32)\n",
            "Epoch [121] Batch[32300] - loss: 0.560544  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.260227  acc: 44.5958%(491/1101) \n",
            "\n",
            "Epoch [122] Batch[32320] - loss: 0.543588  acc: 87.5000%(28/32)\n",
            "Epoch [122] Batch[32340] - loss: 0.744391  acc: 81.2500%(26/32)\n",
            "Epoch [122] Batch[32360] - loss: 0.717311  acc: 78.1250%(25/32)\n",
            "Epoch [122] Batch[32380] - loss: 0.398071  acc: 96.8750%(31/32)\n",
            "Epoch [122] Batch[32400] - loss: 0.678330  acc: 90.6250%(29/32)\n",
            "\n",
            "Evaluation - loss: 1.262076  acc: 44.5050%(490/1101) \n",
            "\n",
            "Epoch [122] Batch[32420] - loss: 0.610585  acc: 87.5000%(28/32)\n",
            "Epoch [122] Batch[32440] - loss: 0.626179  acc: 84.3750%(27/32)\n",
            "Epoch [122] Batch[32460] - loss: 0.778947  acc: 75.0000%(24/32)\n",
            "Epoch [122] Batch[32480] - loss: 0.584722  acc: 87.5000%(28/32)\n",
            "Epoch [122] Batch[32500] - loss: 0.776037  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.261198  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [122] Batch[32520] - loss: 0.552691  acc: 90.6250%(29/32)\n",
            "Epoch [122] Batch[32540] - loss: 0.657445  acc: 81.2500%(26/32)\n",
            "Epoch [122] Batch[32560] - loss: 0.774332  acc: 71.8750%(23/32)\n",
            "Epoch [123] Batch[32580] - loss: 0.647837  acc: 78.1250%(25/32)\n",
            "Epoch [123] Batch[32600] - loss: 0.610471  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.261992  acc: 44.6866%(492/1101) \n",
            "\n",
            "Epoch [123] Batch[32620] - loss: 0.629384  acc: 84.3750%(27/32)\n",
            "Epoch [123] Batch[32640] - loss: 0.575632  acc: 90.6250%(29/32)\n",
            "Epoch [123] Batch[32660] - loss: 0.732950  acc: 71.8750%(23/32)\n",
            "Epoch [123] Batch[32680] - loss: 0.837730  acc: 59.3750%(19/32)\n",
            "Epoch [123] Batch[32700] - loss: 0.633847  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.261559  acc: 44.5050%(490/1101) \n",
            "\n",
            "Epoch [123] Batch[32720] - loss: 0.772550  acc: 81.2500%(26/32)\n",
            "Epoch [123] Batch[32740] - loss: 0.850002  acc: 78.1250%(25/32)\n",
            "Epoch [123] Batch[32760] - loss: 0.618326  acc: 84.3750%(27/32)\n",
            "Epoch [123] Batch[32780] - loss: 0.706782  acc: 75.0000%(24/32)\n",
            "Epoch [123] Batch[32800] - loss: 0.635005  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.260170  acc: 44.9591%(495/1101) \n",
            "\n",
            "Epoch [123] Batch[32820] - loss: 0.629351  acc: 75.0000%(24/32)\n",
            "Epoch [123] Batch[32840] - loss: 0.743707  acc: 84.3750%(27/32)\n",
            "Epoch [124] Batch[32860] - loss: 0.640753  acc: 90.6250%(29/32)\n",
            "Epoch [124] Batch[32880] - loss: 0.747330  acc: 84.3750%(27/32)\n",
            "Epoch [124] Batch[32900] - loss: 0.742732  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.261168  acc: 44.4142%(489/1101) \n",
            "\n",
            "Epoch [124] Batch[32920] - loss: 0.547976  acc: 87.5000%(28/32)\n",
            "Epoch [124] Batch[32940] - loss: 0.614687  acc: 90.6250%(29/32)\n",
            "Epoch [124] Batch[32960] - loss: 0.729662  acc: 71.8750%(23/32)\n",
            "Epoch [124] Batch[32980] - loss: 0.550013  acc: 87.5000%(28/32)\n",
            "Epoch [124] Batch[33000] - loss: 0.775126  acc: 71.8750%(23/32)\n",
            "\n",
            "Evaluation - loss: 1.260787  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [124] Batch[33020] - loss: 0.548884  acc: 87.5000%(28/32)\n",
            "Epoch [124] Batch[33040] - loss: 0.611165  acc: 87.5000%(28/32)\n",
            "Epoch [124] Batch[33060] - loss: 0.696569  acc: 81.2500%(26/32)\n",
            "Epoch [124] Batch[33080] - loss: 0.629191  acc: 78.1250%(25/32)\n",
            "Epoch [124] Batch[33100] - loss: 0.572901  acc: 93.7500%(30/32)\n",
            "\n",
            "Evaluation - loss: 1.261989  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [125] Batch[33120] - loss: 0.558657  acc: 90.6250%(29/32)\n",
            "Epoch [125] Batch[33140] - loss: 0.585148  acc: 87.5000%(28/32)\n",
            "Epoch [125] Batch[33160] - loss: 0.590538  acc: 84.3750%(27/32)\n",
            "Epoch [125] Batch[33180] - loss: 0.650674  acc: 71.8750%(23/32)\n",
            "Epoch [125] Batch[33200] - loss: 0.520676  acc: 90.6250%(29/32)\n",
            "\n",
            "Evaluation - loss: 1.260174  acc: 44.5050%(490/1101) \n",
            "\n",
            "Epoch [125] Batch[33220] - loss: 0.650744  acc: 84.3750%(27/32)\n",
            "Epoch [125] Batch[33240] - loss: 0.536203  acc: 84.3750%(27/32)\n",
            "Epoch [125] Batch[33260] - loss: 0.652477  acc: 75.0000%(24/32)\n",
            "Epoch [125] Batch[33280] - loss: 0.534294  acc: 84.3750%(27/32)\n",
            "Epoch [125] Batch[33300] - loss: 0.665226  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.262381  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [125] Batch[33320] - loss: 0.505375  acc: 90.6250%(29/32)\n",
            "Epoch [125] Batch[33340] - loss: 0.617329  acc: 84.3750%(27/32)\n",
            "Epoch [125] Batch[33360] - loss: 0.725908  acc: 78.1250%(25/32)\n",
            "Epoch [126] Batch[33380] - loss: 0.591449  acc: 87.5000%(28/32)\n",
            "Epoch [126] Batch[33400] - loss: 0.641950  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.262973  acc: 44.4142%(489/1101) \n",
            "\n",
            "Epoch [126] Batch[33420] - loss: 0.731157  acc: 78.1250%(25/32)\n",
            "Epoch [126] Batch[33440] - loss: 0.591449  acc: 84.3750%(27/32)\n",
            "Epoch [126] Batch[33460] - loss: 0.512126  acc: 93.7500%(30/32)\n",
            "Epoch [126] Batch[33480] - loss: 0.790331  acc: 71.8750%(23/32)\n",
            "Epoch [126] Batch[33500] - loss: 0.673792  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.262909  acc: 44.0509%(485/1101) \n",
            "\n",
            "Epoch [126] Batch[33520] - loss: 0.494129  acc: 90.6250%(29/32)\n",
            "Epoch [126] Batch[33540] - loss: 0.582433  acc: 81.2500%(26/32)\n",
            "Epoch [126] Batch[33560] - loss: 0.479911  acc: 93.7500%(30/32)\n",
            "Epoch [126] Batch[33580] - loss: 0.698074  acc: 84.3750%(27/32)\n",
            "Epoch [126] Batch[33600] - loss: 0.542262  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.264256  acc: 44.9591%(495/1101) \n",
            "\n",
            "Epoch [126] Batch[33620] - loss: 0.686959  acc: 81.2500%(26/32)\n",
            "Epoch [126] Batch[33640] - loss: 0.744672  acc: 68.7500%(22/32)\n",
            "Epoch [127] Batch[33660] - loss: 0.540518  acc: 87.5000%(28/32)\n",
            "Epoch [127] Batch[33680] - loss: 0.433144  acc: 96.8750%(31/32)\n",
            "Epoch [127] Batch[33700] - loss: 0.577267  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.264177  acc: 44.3233%(488/1101) \n",
            "\n",
            "Epoch [127] Batch[33720] - loss: 0.626643  acc: 81.2500%(26/32)\n",
            "Epoch [127] Batch[33740] - loss: 0.626574  acc: 84.3750%(27/32)\n",
            "Epoch [127] Batch[33760] - loss: 0.533835  acc: 90.6250%(29/32)\n",
            "Epoch [127] Batch[33780] - loss: 0.522603  acc: 84.3750%(27/32)\n",
            "Epoch [127] Batch[33800] - loss: 0.635223  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.263218  acc: 45.1408%(497/1101) \n",
            "\n",
            "Epoch [127] Batch[33820] - loss: 0.705784  acc: 84.3750%(27/32)\n",
            "Epoch [127] Batch[33840] - loss: 0.758373  acc: 71.8750%(23/32)\n",
            "Epoch [127] Batch[33860] - loss: 0.577498  acc: 81.2500%(26/32)\n",
            "Epoch [127] Batch[33880] - loss: 0.568158  acc: 84.3750%(27/32)\n",
            "Epoch [127] Batch[33900] - loss: 0.627497  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.265341  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [128] Batch[33920] - loss: 0.620736  acc: 84.3750%(27/32)\n",
            "Epoch [128] Batch[33940] - loss: 0.670498  acc: 81.2500%(26/32)\n",
            "Epoch [128] Batch[33960] - loss: 0.753782  acc: 84.3750%(27/32)\n",
            "Epoch [128] Batch[33980] - loss: 0.576361  acc: 87.5000%(28/32)\n",
            "Epoch [128] Batch[34000] - loss: 0.565846  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.262196  acc: 44.5050%(490/1101) \n",
            "\n",
            "Epoch [128] Batch[34020] - loss: 0.569737  acc: 90.6250%(29/32)\n",
            "Epoch [128] Batch[34040] - loss: 0.649537  acc: 87.5000%(28/32)\n",
            "Epoch [128] Batch[34060] - loss: 0.681147  acc: 71.8750%(23/32)\n",
            "Epoch [128] Batch[34080] - loss: 0.506204  acc: 93.7500%(30/32)\n",
            "Epoch [128] Batch[34100] - loss: 0.567785  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.262560  acc: 44.6866%(492/1101) \n",
            "\n",
            "Epoch [128] Batch[34120] - loss: 0.676696  acc: 81.2500%(26/32)\n",
            "Epoch [128] Batch[34140] - loss: 0.594574  acc: 81.2500%(26/32)\n",
            "Epoch [128] Batch[34160] - loss: 0.487070  acc: 93.7500%(30/32)\n",
            "Epoch [129] Batch[34180] - loss: 0.758654  acc: 71.8750%(23/32)\n",
            "Epoch [129] Batch[34200] - loss: 0.666785  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.264606  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [129] Batch[34220] - loss: 0.630075  acc: 81.2500%(26/32)\n",
            "Epoch [129] Batch[34240] - loss: 0.715944  acc: 78.1250%(25/32)\n",
            "Epoch [129] Batch[34260] - loss: 0.583065  acc: 84.3750%(27/32)\n",
            "Epoch [129] Batch[34280] - loss: 0.588592  acc: 81.2500%(26/32)\n",
            "Epoch [129] Batch[34300] - loss: 0.508885  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.263145  acc: 44.5958%(491/1101) \n",
            "\n",
            "Epoch [129] Batch[34320] - loss: 0.737711  acc: 81.2500%(26/32)\n",
            "Epoch [129] Batch[34340] - loss: 0.708511  acc: 81.2500%(26/32)\n",
            "Epoch [129] Batch[34360] - loss: 0.798355  acc: 71.8750%(23/32)\n",
            "Epoch [129] Batch[34380] - loss: 0.575928  acc: 78.1250%(25/32)\n",
            "Epoch [129] Batch[34400] - loss: 0.569641  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.265254  acc: 44.2325%(487/1101) \n",
            "\n",
            "Epoch [129] Batch[34420] - loss: 0.512945  acc: 84.3750%(27/32)\n",
            "Epoch [129] Batch[34440] - loss: 0.667176  acc: 87.5000%(28/32)\n",
            "Epoch [130] Batch[34460] - loss: 0.727130  acc: 68.7500%(22/32)\n",
            "Epoch [130] Batch[34480] - loss: 0.534607  acc: 84.3750%(27/32)\n",
            "Epoch [130] Batch[34500] - loss: 0.581896  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.267837  acc: 43.9600%(484/1101) \n",
            "\n",
            "Epoch [130] Batch[34520] - loss: 0.585476  acc: 87.5000%(28/32)\n",
            "Epoch [130] Batch[34540] - loss: 0.543140  acc: 93.7500%(30/32)\n",
            "Epoch [130] Batch[34560] - loss: 0.524725  acc: 93.7500%(30/32)\n",
            "Epoch [130] Batch[34580] - loss: 0.590139  acc: 87.5000%(28/32)\n",
            "Epoch [130] Batch[34600] - loss: 0.641674  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.263885  acc: 44.5050%(490/1101) \n",
            "\n",
            "Epoch [130] Batch[34620] - loss: 0.629558  acc: 75.0000%(24/32)\n",
            "Epoch [130] Batch[34640] - loss: 0.569718  acc: 84.3750%(27/32)\n",
            "Epoch [130] Batch[34660] - loss: 0.528845  acc: 87.5000%(28/32)\n",
            "Epoch [130] Batch[34680] - loss: 0.631320  acc: 84.3750%(27/32)\n",
            "Epoch [130] Batch[34700] - loss: 0.680297  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.265997  acc: 44.5958%(491/1101) \n",
            "\n",
            "Epoch [131] Batch[34720] - loss: 0.535250  acc: 93.7500%(30/32)\n",
            "Epoch [131] Batch[34740] - loss: 0.694381  acc: 81.2500%(26/32)\n",
            "Epoch [131] Batch[34760] - loss: 0.768567  acc: 78.1250%(25/32)\n",
            "Epoch [131] Batch[34780] - loss: 0.579924  acc: 84.3750%(27/32)\n",
            "Epoch [131] Batch[34800] - loss: 0.383034  acc: 96.8750%(31/32)\n",
            "\n",
            "Evaluation - loss: 1.265891  acc: 44.5958%(491/1101) \n",
            "\n",
            "Epoch [131] Batch[34820] - loss: 0.563750  acc: 78.1250%(25/32)\n",
            "Epoch [131] Batch[34840] - loss: 0.585997  acc: 81.2500%(26/32)\n",
            "Epoch [131] Batch[34860] - loss: 0.602190  acc: 87.5000%(28/32)\n",
            "Epoch [131] Batch[34880] - loss: 0.715735  acc: 81.2500%(26/32)\n",
            "Epoch [131] Batch[34900] - loss: 0.617486  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.266761  acc: 44.3233%(488/1101) \n",
            "\n",
            "Epoch [131] Batch[34920] - loss: 0.595398  acc: 81.2500%(26/32)\n",
            "Epoch [131] Batch[34940] - loss: 0.733834  acc: 75.0000%(24/32)\n",
            "Epoch [131] Batch[34960] - loss: 0.671565  acc: 87.5000%(28/32)\n",
            "Epoch [132] Batch[34980] - loss: 0.524460  acc: 90.6250%(29/32)\n",
            "Epoch [132] Batch[35000] - loss: 0.488384  acc: 96.8750%(31/32)\n",
            "\n",
            "Evaluation - loss: 1.265598  acc: 44.6866%(492/1101) \n",
            "\n",
            "Epoch [132] Batch[35020] - loss: 0.625728  acc: 78.1250%(25/32)\n",
            "Epoch [132] Batch[35040] - loss: 0.526010  acc: 93.7500%(30/32)\n",
            "Epoch [132] Batch[35060] - loss: 0.631095  acc: 78.1250%(25/32)\n",
            "Epoch [132] Batch[35080] - loss: 0.472077  acc: 93.7500%(30/32)\n",
            "Epoch [132] Batch[35100] - loss: 0.726707  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.267344  acc: 44.2325%(487/1101) \n",
            "\n",
            "Epoch [132] Batch[35120] - loss: 0.474498  acc: 96.8750%(31/32)\n",
            "Epoch [132] Batch[35140] - loss: 0.648790  acc: 75.0000%(24/32)\n",
            "Epoch [132] Batch[35160] - loss: 0.644671  acc: 75.0000%(24/32)\n",
            "Epoch [132] Batch[35180] - loss: 0.641769  acc: 84.3750%(27/32)\n",
            "Epoch [132] Batch[35200] - loss: 0.667442  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.266449  acc: 44.3233%(488/1101) \n",
            "\n",
            "Epoch [132] Batch[35220] - loss: 0.563013  acc: 90.6250%(29/32)\n",
            "Epoch [132] Batch[35240] - loss: 0.559600  acc: 93.7500%(30/32)\n",
            "Epoch [133] Batch[35260] - loss: 0.585616  acc: 87.5000%(28/32)\n",
            "Epoch [133] Batch[35280] - loss: 0.705536  acc: 81.2500%(26/32)\n",
            "Epoch [133] Batch[35300] - loss: 0.715692  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.266326  acc: 44.5050%(490/1101) \n",
            "\n",
            "Epoch [133] Batch[35320] - loss: 0.600682  acc: 81.2500%(26/32)\n",
            "Epoch [133] Batch[35340] - loss: 0.608395  acc: 78.1250%(25/32)\n",
            "Epoch [133] Batch[35360] - loss: 0.522002  acc: 90.6250%(29/32)\n",
            "Epoch [133] Batch[35380] - loss: 0.649935  acc: 87.5000%(28/32)\n",
            "Epoch [133] Batch[35400] - loss: 0.497049  acc: 93.7500%(30/32)\n",
            "\n",
            "Evaluation - loss: 1.267768  acc: 44.9591%(495/1101) \n",
            "\n",
            "Epoch [133] Batch[35420] - loss: 0.673650  acc: 78.1250%(25/32)\n",
            "Epoch [133] Batch[35440] - loss: 0.518567  acc: 87.5000%(28/32)\n",
            "Epoch [133] Batch[35460] - loss: 0.656128  acc: 81.2500%(26/32)\n",
            "Epoch [133] Batch[35480] - loss: 0.510777  acc: 87.5000%(28/32)\n",
            "Epoch [133] Batch[35500] - loss: 0.652301  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.268199  acc: 43.8692%(483/1101) \n",
            "\n",
            "Epoch [134] Batch[35520] - loss: 0.626666  acc: 81.2500%(26/32)\n",
            "Epoch [134] Batch[35540] - loss: 0.748537  acc: 68.7500%(22/32)\n",
            "Epoch [134] Batch[35560] - loss: 0.484091  acc: 93.7500%(30/32)\n",
            "Epoch [134] Batch[35580] - loss: 0.721684  acc: 81.2500%(26/32)\n",
            "Epoch [134] Batch[35600] - loss: 0.669664  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.266199  acc: 43.9600%(484/1101) \n",
            "\n",
            "Epoch [134] Batch[35620] - loss: 0.642100  acc: 78.1250%(25/32)\n",
            "Epoch [134] Batch[35640] - loss: 0.557627  acc: 90.6250%(29/32)\n",
            "Epoch [134] Batch[35660] - loss: 0.661847  acc: 78.1250%(25/32)\n",
            "Epoch [134] Batch[35680] - loss: 0.696470  acc: 68.7500%(22/32)\n",
            "Epoch [134] Batch[35700] - loss: 0.715610  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.265694  acc: 44.1417%(486/1101) \n",
            "\n",
            "Epoch [134] Batch[35720] - loss: 0.725741  acc: 75.0000%(24/32)\n",
            "Epoch [134] Batch[35740] - loss: 0.515931  acc: 84.3750%(27/32)\n",
            "Epoch [134] Batch[35760] - loss: 0.548195  acc: 87.5000%(28/32)\n",
            "Epoch [135] Batch[35780] - loss: 0.691403  acc: 78.1250%(25/32)\n",
            "Epoch [135] Batch[35800] - loss: 0.678020  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.269218  acc: 44.2325%(487/1101) \n",
            "\n",
            "Epoch [135] Batch[35820] - loss: 0.522780  acc: 90.6250%(29/32)\n",
            "Epoch [135] Batch[35840] - loss: 0.552550  acc: 93.7500%(30/32)\n",
            "Epoch [135] Batch[35860] - loss: 0.704652  acc: 84.3750%(27/32)\n",
            "Epoch [135] Batch[35880] - loss: 0.556216  acc: 84.3750%(27/32)\n",
            "Epoch [135] Batch[35900] - loss: 0.669439  acc: 75.0000%(24/32)\n",
            "\n",
            "Evaluation - loss: 1.267830  acc: 43.9600%(484/1101) \n",
            "\n",
            "Epoch [135] Batch[35920] - loss: 0.591290  acc: 84.3750%(27/32)\n",
            "Epoch [135] Batch[35940] - loss: 0.655902  acc: 84.3750%(27/32)\n",
            "Epoch [135] Batch[35960] - loss: 0.569687  acc: 84.3750%(27/32)\n",
            "Epoch [135] Batch[35980] - loss: 0.478397  acc: 84.3750%(27/32)\n",
            "Epoch [135] Batch[36000] - loss: 0.522735  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.268335  acc: 44.7775%(493/1101) \n",
            "\n",
            "Epoch [135] Batch[36020] - loss: 0.602244  acc: 84.3750%(27/32)\n",
            "Epoch [135] Batch[36040] - loss: 0.488157  acc: 84.3750%(27/32)\n",
            "Epoch [136] Batch[36060] - loss: 0.765858  acc: 78.1250%(25/32)\n",
            "Epoch [136] Batch[36080] - loss: 0.786746  acc: 81.2500%(26/32)\n",
            "Epoch [136] Batch[36100] - loss: 0.574515  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.267933  acc: 43.8692%(483/1101) \n",
            "\n",
            "Epoch [136] Batch[36120] - loss: 0.542966  acc: 87.5000%(28/32)\n",
            "Epoch [136] Batch[36140] - loss: 0.550553  acc: 90.6250%(29/32)\n",
            "Epoch [136] Batch[36160] - loss: 0.622968  acc: 87.5000%(28/32)\n",
            "Epoch [136] Batch[36180] - loss: 0.637343  acc: 81.2500%(26/32)\n",
            "Epoch [136] Batch[36200] - loss: 0.505163  acc: 93.7500%(30/32)\n",
            "\n",
            "Evaluation - loss: 1.267692  acc: 44.3233%(488/1101) \n",
            "\n",
            "Epoch [136] Batch[36220] - loss: 0.687682  acc: 78.1250%(25/32)\n",
            "Epoch [136] Batch[36240] - loss: 0.576810  acc: 87.5000%(28/32)\n",
            "Epoch [136] Batch[36260] - loss: 0.605213  acc: 87.5000%(28/32)\n",
            "Epoch [136] Batch[36280] - loss: 0.473893  acc: 87.5000%(28/32)\n",
            "Epoch [136] Batch[36300] - loss: 0.662770  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.271395  acc: 44.4142%(489/1101) \n",
            "\n",
            "Epoch [137] Batch[36320] - loss: 0.602605  acc: 84.3750%(27/32)\n",
            "Epoch [137] Batch[36340] - loss: 0.571531  acc: 87.5000%(28/32)\n",
            "Epoch [137] Batch[36360] - loss: 0.614997  acc: 81.2500%(26/32)\n",
            "Epoch [137] Batch[36380] - loss: 0.467372  acc: 93.7500%(30/32)\n",
            "Epoch [137] Batch[36400] - loss: 0.580304  acc: 87.5000%(28/32)\n",
            "\n",
            "Evaluation - loss: 1.269050  acc: 44.0509%(485/1101) \n",
            "\n",
            "Epoch [137] Batch[36420] - loss: 0.589317  acc: 87.5000%(28/32)\n",
            "Epoch [137] Batch[36440] - loss: 0.570296  acc: 81.2500%(26/32)\n",
            "Epoch [137] Batch[36460] - loss: 0.446233  acc: 96.8750%(31/32)\n",
            "Epoch [137] Batch[36480] - loss: 0.685900  acc: 75.0000%(24/32)\n",
            "Epoch [137] Batch[36500] - loss: 0.563651  acc: 84.3750%(27/32)\n",
            "\n",
            "Evaluation - loss: 1.268917  acc: 44.2325%(487/1101) \n",
            "\n",
            "Epoch [137] Batch[36520] - loss: 0.536290  acc: 93.7500%(30/32)\n",
            "Epoch [137] Batch[36540] - loss: 0.647779  acc: 87.5000%(28/32)\n",
            "Epoch [137] Batch[36560] - loss: 0.652350  acc: 84.3750%(27/32)\n",
            "Epoch [138] Batch[36580] - loss: 0.616992  acc: 81.2500%(26/32)\n",
            "Epoch [138] Batch[36600] - loss: 0.446176  acc: 93.7500%(30/32)\n",
            "\n",
            "Evaluation - loss: 1.269332  acc: 44.1417%(486/1101) \n",
            "\n",
            "Epoch [138] Batch[36620] - loss: 0.543300  acc: 96.8750%(31/32)\n",
            "Epoch [138] Batch[36640] - loss: 0.618271  acc: 78.1250%(25/32)\n",
            "Epoch [138] Batch[36660] - loss: 0.517590  acc: 81.2500%(26/32)\n",
            "Epoch [138] Batch[36680] - loss: 0.552662  acc: 87.5000%(28/32)\n",
            "Epoch [138] Batch[36700] - loss: 0.848845  acc: 78.1250%(25/32)\n",
            "\n",
            "Evaluation - loss: 1.268770  acc: 44.0509%(485/1101) \n",
            "\n",
            "Epoch [138] Batch[36720] - loss: 0.521187  acc: 87.5000%(28/32)\n",
            "Epoch [138] Batch[36740] - loss: 0.557516  acc: 90.6250%(29/32)\n",
            "Epoch [138] Batch[36760] - loss: 0.707276  acc: 81.2500%(26/32)\n",
            "Epoch [138] Batch[36780] - loss: 0.699356  acc: 78.1250%(25/32)\n",
            "Epoch [138] Batch[36800] - loss: 0.629359  acc: 81.2500%(26/32)\n",
            "\n",
            "Evaluation - loss: 1.269994  acc: 44.5050%(490/1101) \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-5053cd930c90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mis_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'early stop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: early stop"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HlXAfooAhd-X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "18d047af-85a0-4231-c4a1-82ae2fb63de9"
      },
      "cell_type": "code",
      "source": [
        "best_model=torch.load('best_model.pt')\n",
        "eval(test_iter, model, args)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation - loss: 1.214050  acc: 45.7014%(1010/2210) \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45.70135746606335"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    }
  ]
}